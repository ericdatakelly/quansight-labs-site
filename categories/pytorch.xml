<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Quansight Labs (Posts about PyTorch)</title><link>https://labs.quansight.org/</link><description></description><atom:link href="https://labs.quansight.org/categories/pytorch.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2021 &lt;a href="mailto:info@quansight.com"&gt;Quansight Labs Team&lt;/a&gt; </copyright><lastBuildDate>Sat, 10 Jul 2021 10:45:59 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Distributed Training Made Easy with PyTorch-Ignite</title><link>https://labs.quansight.org/blog/2021/06/distributed-made-easy-with-ignite/</link><dc:creator>Victor Fomin</dc:creator><description>&lt;div&gt;&lt;img alt="PyTorch-Ignite logo" src="https://labs.quansight.org/images/pytorch-ignite/ignite_logo_mixed.png"&gt;
&lt;p&gt;Authors: &lt;a class="reference external" href="https://github.com/fco-dv"&gt;François Cokelaer&lt;/a&gt;,
&lt;a class="reference external" href="https://github.com/Priyansi"&gt;Priyansi&lt;/a&gt;, &lt;a class="reference external" href="https://github.com/sdesrozis/"&gt;Sylvain
Desroziers&lt;/a&gt;, &lt;a class="reference external" href="https://github.com/vfdev-5"&gt;Victor
Fomin&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Writing &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Agnostic_(data)"&gt;agnostic&lt;/a&gt;
&lt;a class="reference external" href="https://pytorch.org/tutorials/beginner/dist_overview.html"&gt;distributed
code&lt;/a&gt; that
supports different platforms, hardware configurations (GPUs, TPUs) and
communication frameworks is tedious. In this blog, we will discuss how
&lt;a class="reference external" href="https://pytorch.org/ignite/"&gt;PyTorch-Ignite&lt;/a&gt; solves this problem
with minimal code change.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://labs.quansight.org/blog/2021/06/distributed-made-easy-with-ignite/"&gt;Read more…&lt;/a&gt; (8 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Deep Learning</category><category>Distributed</category><category>Horovod</category><category>Machine Learning</category><category>PyTorch</category><category>PyTorch DDP</category><category>PyTorch XLA</category><category>PyTorch-Ignite</category><category>SLURM</category><guid>https://labs.quansight.org/blog/2021/06/distributed-made-easy-with-ignite/</guid><pubDate>Mon, 28 Jun 2021 08:00:00 GMT</pubDate></item><item><title>PyTorch-Ignite: training and evaluating neural networks flexibly and transparently</title><link>https://labs.quansight.org/blog/2020/09/pytorch-ignite/</link><dc:creator>Victor Fomin</dc:creator><description>&lt;div class="cell border-box-sizing text_cell rendered"&gt;&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;!--# PyTorch-Ignite: training and evaluating neural networks flexibly and transparently--&gt;

&lt;!--div align="center"&gt;
&lt;img width=512 src="https://i.ibb.co/WtbmXJQ/ignite-blog.jpg"/&gt;
&lt;/div--&gt;

&lt;div align="center"&gt;
&lt;img width="512" src="https://i.ibb.co/x8Bhqhj/habr-pytorch-ignite-image.png"&gt;
&lt;/div&gt;&lt;div align="center"&gt;
Authors: Victor Fomin (Quansight), Sylvain Desroziers (IFPEN, France)
&lt;/div&gt;&lt;div&gt;
This post is a general introduction of PyTorch-Ignite. It intends to give a brief but illustrative overview of what PyTorch-Ignite can offer for Deep Learning enthusiasts, professionals and researchers. Following the same philosophy as PyTorch, PyTorch-Ignite aims to keep it simple, flexible and extensible but performant and scalable.
&lt;/div&gt;&lt;p&gt;&lt;a href="https://labs.quansight.org/blog/2020/09/pytorch-ignite/"&gt;Read more…&lt;/a&gt; (28 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>Deep Learning</category><category>Labs</category><category>Machine Learning</category><category>Neural Networks</category><category>Python</category><category>PyTorch</category><category>Tutorial</category><guid>https://labs.quansight.org/blog/2020/09/pytorch-ignite/</guid><pubDate>Thu, 10 Sep 2020 05:00:00 GMT</pubDate></item><item><title>Quansight Labs: what I learned in my first 3 months</title><link>https://labs.quansight.org/blog/2020/07/a-win-win-all-around/</link><dc:creator>Matti Picus</dc:creator><description>&lt;div&gt;&lt;p&gt;I joined Quansight at the beginning of April, splitting my time between
PyTorch (as part of a larger Quansight team) and contributing to Quansight Labs
supported community-driven projects in the Python scientific and data science
software stack, primarily to NumPy.  I have found my next home; the people, the
projects, and the atmosphere are an all around win-win for me and (I hope) for
the projects to which I contribute.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://labs.quansight.org/blog/2020/07/a-win-win-all-around/"&gt;Read more…&lt;/a&gt; (2 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>funding</category><category>Labs</category><category>NumPy</category><category>PyTorch</category><guid>https://labs.quansight.org/blog/2020/07/a-win-win-all-around/</guid><pubDate>Tue, 21 Jul 2020 06:00:00 GMT</pubDate></item><item><title>PyTorch TensorIterator Internals</title><link>https://labs.quansight.org/blog/2020/04/pytorch-tensoriterator-internals/</link><dc:creator>Sameer Deshmukh</dc:creator><description>&lt;div&gt;&lt;p class="alert alert-warning"&gt;The history section of this post is still relevant, but &lt;code&gt;TensorIterator&lt;/code&gt;'s
interface has changed significantly. For an update on the new API, please check
out &lt;a href="https://labs.quansight.org/blog/2021/04/pytorch-tensoriterator-internals-update/index.html"&gt;this new blog
post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;PyTorch is one of the leading frameworks for deep learning. Its core data
structure is &lt;code&gt;Tensor&lt;/code&gt;, a multi-dimensional array implementation with many
advanced features like auto-differentiation. PyTorch is a massive
codebase (approx. &lt;a href="https://www.openhub.net/p/pytorch"&gt;a million lines&lt;/a&gt; of
C++, Python and CUDA code), and having a method for iterating over tensors in a
very efficient manner that is independent of data type, dimension, striding and
hardware is a critical feature that can lead to a very massive simplification
of the codebase and make distributed development much faster and smoother. The
&lt;a href="https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/TensorIterator.cpp"&gt;&lt;code&gt;TensorIterator&lt;/code&gt;&lt;/a&gt;
C++ class within PyTorch is a complex yet useful class that is used for
iterating over the elements of a tensor over any dimension and implicitly
parallelizing various operations in a device independent manner.&lt;/p&gt;
&lt;p&gt;It does this through a C++ API that is independent of type and device of the
tensor, freeing the programmer of having to worry about the datatype or device
when writing iteration logic for PyTorch tensors. For those coming from the
NumPy universe, &lt;code&gt;NpyIter&lt;/code&gt; is a close cousin of &lt;code&gt;TensorIterator&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This post is a deep dive into how &lt;code&gt;TensorIterator&lt;/code&gt; works, and is an essential
part of learning to contribute to the PyTorch codebase since iterations over
tensors in the C++ codebase are extremely commonplace. This post is aimed at
someone who wants to contribute to PyTorch, and you should at least be familiar
with some of the basic terminologies of the PyTorch codebase that can be found
in Edward Yang's excellent &lt;a href="http://blog.ezyang.com/2019/05/pytorch-internals"&gt;blog post&lt;/a&gt;
on PyTorch internals.  Although &lt;code&gt;TensorIterator&lt;/code&gt; can be used for both CPUs and
accelerators, this post has been written keeping in mind usage on the CPU.
Although there can be some dissimilarities between the two, the overall
concepts are the same.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://labs.quansight.org/blog/2020/04/pytorch-tensoriterator-internals/"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>C++</category><category>PyTorch</category><guid>https://labs.quansight.org/blog/2020/04/pytorch-tensoriterator-internals/</guid><pubDate>Mon, 13 Apr 2020 15:39:56 GMT</pubDate></item></channel></rss>