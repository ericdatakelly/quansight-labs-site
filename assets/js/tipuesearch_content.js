var tipuesearch = {
  "pages": [
    {
      "title": "Pyflyby: Improving Efficiency of Jupyter Interactive Sessions",
      "text": "Interruption hinder productivity more than interruption. A notification, random\nrealization, or unrelated error can derail one's train of thought when deep in a\ncomplex analysis \u2013 a frustrating experience.\nIn the software development context, forgetting to import a statement in an\ninteractive Jupyter session is such an experience. This can be especially\nfrustrating when using typical abbreviations, like np, pd, plt, where the\nmeaning is obvious to the human reader, but not to the computer. The\ntime-to-first-plot, and ability to quickly cleanup one's notebook afterward\nare critical to an enjoyable and efficient workflow.\nIn this blogpost we present pyflyby, a\nproject and an extension to IPython and\nJupyterLab, that, among many\nthings, automatically inserts imports and tidies Python files and notebooks.\n\n\nWhat is pyflyby?\nPyflyby is a set of tools designed to improve interactive and non-interactive\nworkflows in Python. Pyflyby provides a number of utilities and extensions aimed\nat making day-to-day work in Python faster and simpler.\n\nAutoimport\nOne of pyflyby's key capabilities is automatic importing of commonly used\nmodules and objects, leading to simpler, faster, and less disruptive coding. In\na new session, for example, one can type:\nsin(arange(10))\n\n\n\nPyflyby will then hook into the execution mechanism of Python, execute the\ncorrect import, and confirm with a clear message:\n[PYFLYBY] from numpy import arange\n[PYFLYBY] from numpy import sin\n\n\n\nPyflyby will also do so when running a command line file via\nthe py executable replacing python.\nWith the jupyterlab-pyflyby extension, imports will be executed and inserted in\nthe first cell of one's notebook:\n\ntidy-import\nIn addition to the extension that can seamlessly import the right libraries\nwhile exploring, another pyflyby feature helps to maintain scripts and notebooks\nwith explicit and correct imports.\nPyflyby displays the tidy-import command line tool to gather, insert, and\nformat imports in Python files. This is similar to tools such as\nblack and\nisort, but with different styling options and\nwith the ability to infer missing imports.\ntidy-import includes the imports to pandas and matplotlib, as in the example\nbelow, and queries whether to update the file:\n$ tidy-imports example.py\n[PYFLYBY] example.py: added 'import pandas as pd'\n[PYFLYBY] example.py: added 'from matplotlib import pyplot'\n--- example.py  2021-03-08 10:33:04.000000000 -0800\n+++ example.py  2021-03-08 10:33:18.000000000 -0800\n@@ -1,2 +1,7 @@\n+from   matplotlib               import pyplot\n+import pandas as pd\n+\n data = pd.read_csv(\"./data/base-pop-2015.csv\")\n pyplot.plot(data.population)\n\nReplace example.py? [y/N]\n\n\n\nOther utilities\nPyflyby contains a number of other utilities to improve the efficiency of\nmanipulating and executing Python code. (Please refer to pyflyby's\nREADME for additional information.)\npy is one such example. It is a flexible tool that can be used either to start\nIPython or to execute commands quickly from the developer's shell without the\nneed for imports. It supports a range of syntax options, allowing for quick\ncalculation and graph plotting.\n\nWithout any parameters, py will start IPython with the pyflyby extension\n   activated.\nWith space-separated arguments, py will attempt to interpret each argument\n   as a Python function call with the right imports:\n\n$ py np.random.normal 0 1\n[PYFLYBY] import numpy as np\n[PYFLYBY] np.random.normal(0, 1)\n-0.027577422117386\n\n\n\n\nWhen more control over values is necessary, py will run a Python expression:\n\n$ py 'plot(scipy.stats.norm.pdf(linspace(-5, 5), 0, 1))'\n[PYFLYBY] from numpy import linspace\n[PYFLYBY] from matplotlib.pyplot import plot\n[PYFLYBY] import scipy.stats\n[PYFLYBY] plot(scipy.stats.norm.pdf(linspace(-5, 5), 0, 1))\n[<matplotlib.lines.Line2D object at 0x132981940>]\n\n\n\n\nfind-import , another utility available in pyflyby, can be deployed to find a\nparticular function across many libraries by returning the relevant import.  For\nexample:\n$ find-import norm\nfrom scipy.stats.distributions import norm\n\n\n\nNotes on pyflyby codebase\nThe Pyflyby codebase provides a window into\nadvanced data structures and programming paradigms. Its use of modules and\nprogramming concepts is unusual relative to those found in more classical data\nscience-focused libraries. For example:\n\n\nPyflyby will conduct non-trivial manipulation of the Python Abstract Syntax\n   Tree (AST), which represents code written in tree form. Pyflyby uses AST to\n   find and insert missing imports, and it does so even as AST's exact\n   representation changes with almost every minor release.\n\n\nPyflyby demonstrates the use of  Aspect-Oriented\n   programming,\n   which highlights the flexibility of the Python programming model.\n\n\nConclusion\nPyflyby's utilities are designed to improve developer efficiency and materially\nreduce the impact of interruption on productivity. Its value, however, is\nbroader than that. Pyflyby expands what one is capable of doing within the Python ecosystem\nand has helped identify a number of limitations and bugs in Python and IPython\nover time.\nHow to get pyflyby\nPyflyby is available on GitHub or, for\nterminal IPython users:\n$ pip install pyflyby\n$ py pyflyby.install_in_ipython_config_file\n\n\n\nJupyterLab users can also install the JupyterLab\nExtension, which is\nnotebook-aware and enables even more features.\nAcknowledgements\nPyflyby was created by Karl Chen and is supported by the D. E. Shaw\ngroup in collaboration with Quansight.",
      "tags": "Deshaw,Labs,Pyflyby",
      "url": "https://labs.quansight.org/blog/2021/07/pyflyby-improving-efficiency-of-jupyter-interactive-sessions/"
    },
    {
      "title": "Distributed Training Made Easy with PyTorch-Ignite",
      "text": "Authors: Fran\u00e7ois Cokelaer,\nPriyansi, Sylvain\nDesroziers, Victor\nFomin\nWriting agnostic\ndistributed\ncode that\nsupports different platforms, hardware configurations (GPUs, TPUs) and\ncommunication frameworks is tedious. In this blog, we will discuss how\nPyTorch-Ignite solves this problem\nwith minimal code change.\n\n\nContents\n\nPrerequisites\nIntroduction\n\ud83d\udd25 Pytorch-Ignite Unified Distributed API\n\n\ud83d\udd0d Focus on the helper auto_* methods:\n\n\nExamples\n\nPyTorch-Ignite - Torch native Distributed Data Parallel - Horovod - XLA/TPUs\n\n\nRunning Distributed Code\n\nWith torch.multiprocessing.spawn\nWith Distributed launchers\n\nWith torch.distributed.launch\nWith horovodrun\nWith slurm\n\n\n\n\nClosing Remarks\n\nReferences\nNext Steps\n\n\n\n\n\nPrerequisites\nThis blog assumes you have some knowledge about:\n\nPyTorch's distributed\npackage,\nthe\nbackends\nand collective\nfunctions\nit provides. In this blog, we will focus on distributed data\nparallel\ncode.\nPyTorch-Ignite. Refer to this\nblog\nfor a quick high-level overview.\n\n\n\nIntroduction\nPyTorch-Ignite's\nignite.distributed\n(idist) submodule introduced in version v0.4.0 (July\n2020)\nquickly turns single-process code into its data distributed version.\nThus, you will now be able to run the same version of the code across\nall supported backends seamlessly:\n\nbackends from native torch distributed configuration:\nnccl,\ngloo,\nmpi\nHorovod framework\nwith gloo or nccl communication backend\nXLA on TPUs via pytorch/xla\n\nIn this blog post we will compare PyTorch-Ignite's API with torch\nnative's distributed code and highlight the differences and ease of use\nof the former. We will also show how Ignite's auto_* methods\nautomatically make your code compatible with the aforementioned\ndistributed backends so that you only have to bring your own model,\noptimizer and data loader objects.\nCode snippets, as well as commands for running all the scripts, are\nprovided in a separate\nrepository.\nThen we will also cover several ways of spawning processes via torch\nnative torch.multiprocessing.spawn and also via multiple distributed\nlaunchers in order to highlight how Pytorch-Ignite's idist can\nhandle it without any changes to the code, in particular:\n\ntorch.multiprocessing.spawn\ntorch.distributed.launch\nhorovodrun\nslurm\n\nMore information on launchers experiments can be found\nhere.\n\n\n\ud83d\udd25 Pytorch-Ignite Unified Distributed API\nWe need to write different code for different distributed backends. This\ncan be tedious especially if you would like to run your code on\ndifferent hardware configurations. Pytorch-Ignite's idist will do\nall the work for you, owing to the high-level helper methods.\n\n\ud83d\udd0d Focus on the helper auto_* methods:\n\nauto_model()\n\nThis method adapts the logic for non-distributed and available\ndistributed configurations. Here are the equivalent code snippets for\ndistributed model instantiation:\n\n   \n      \n         \n            PyTorch-Ignite\n            PyTorch DDP\n         \n         \n              \n         \n         \n            \u00a0\n         \n         \n            Horovod\n            Torch XLA\n         \n         \n             \n         \n      \n   \n\n\n\nAdditionally, it is also compatible with NVIDIA/apex\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)\nmodel = idist.auto_model(model)\nand Torch native AMP\nmodel = idist.auto_model(model)\n\nwith autocast():\n    y_pred = model(x)\n\nauto_optim()\n\nThis method adapts the optimizer logic for non-distributed and available\ndistributed configurations seamlessly. Here are the equivalent code\nsnippets for distributed optimizer instantiation:\n\n   \n      \n         \n            PyTorch-Ignite\n            PyTorch DDP\n         \n         \n              \n         \n         \n            \u00a0\n         \n         \n            Horovod\n            Torch XLA\n         \n         \n             \n         \n      \n   \n\n\n\n\nauto_dataloader()\n\nThis method adapts the data loading logic for non-distributed and\navailable distributed configurations seamlessly on target devices.\nAdditionally, auto_dataloader() automatically scales the batch size\naccording to the distributed configuration context resulting in a\ngeneral way of loading sample batches on multiple devices.\nHere are the equivalent code snippets for the distributed data loading\nstep:\n\n   \n      \n         \n            PyTorch-Ignite\n            PyTorch DDP\n         \n         \n              \n         \n         \n            \u00a0\n         \n         \n            Horovod\n            Torch XLA\n         \n         \n             \n         \n      \n   \n\nNote\nAdditionally, idist provides collective operations like\nall_reduce, all_gather, and broadcast that can be used\nwith all supported distributed frameworks. Please, see our\ndocumentation\nfor more details.\n\n\n\n\nExamples\nThe code snippets below highlight the API's specificities of each of the\ndistributed backends on the same use case as compared to the idist\nAPI. PyTorch native code is available for DDP, Horovod, and for XLA/TPU\ndevices.\nPyTorch-Ignite's unified code snippet can be run with the standard PyTorch\nbackends like gloo and nccl and also with Horovod and XLA for\nTPU devices. Note that the code is less verbose, however, the user still\nhas full control of the training loop.\nThe following examples are introductory. For a more robust,\nproduction-grade example that uses PyTorch-Ignite, refer\nhere.\nThe complete source code of these experiments can be found\nhere.\n\nPyTorch-Ignite - Torch native Distributed Data Parallel - Horovod - XLA/TPUs\n\n   \n      \n         \n            \n               PyTorch-Ignite\n            \n               PyTorch DDP\n         \n         \n             Source Code \n             Source Code \n         \n         \n              \n         \n         \n            \n               Horovod\n            \n               Torch XLA\n         \n         \n             Source Code \n             Source Code \n         \n         \n              \n              \n         \n      \n   \n\nNote\nYou can also mix the usage of idist with other distributed APIs as below:\ndist.init_process_group(backend, store=..., world_size=world_size, rank=rank)\n\nrank = idist.get_rank()\nws = idist.get_world_size()\nmodel = idist.auto_model(model)\n\ndist.destroy_process_group()\n\n\n\n\nRunning Distributed Code\n\nPyTorch-Ignite's idist also unifies the distributed codes\nlaunching method and makes the distributed configuration setup easier\nwith the\nignite.distributed.launcher.Parallel (idist Parallel)\ncontext manager.\nThis context manager has the capability to either spawn\nnproc_per_node (passed as a script argument) child processes and\ninitialize a processing group according to the provided backend or use\ntools like torch.distributed.launch, slurm, horovodrun by\ninitializing the processing group given the backend argument only\nin a general way.\n\n\nWith torch.multiprocessing.spawn\nIn this case idist Parallel is using the native torch\ntorch.multiprocessing.spawn method under the hood in order to run\nthe distributed configuration. Here nproc_per_node is passed as a\nspawn argument.\n\nRunning multiple distributed configurations with one code. Source:\nignite_idist.py:\n\n# Running with gloo\npython -u ignite_idist.py --nproc_per_node 2 --backend gloo\n\n# Running with nccl\npython -u ignite_idist.py --nproc_per_node 2 --backend nccl\n\n# Running with horovod with gloo controller ( gloo or nccl support )\npython -u ignite_idist.py --backend horovod --nproc_per_node 2\n\n# Running on xla/tpu\npython -u ignite_idist.py --backend xla-tpu --nproc_per_node 8 --batch_size 32\n\n\nWith Distributed launchers\nPyTorch-Ignite's idist Parallel context manager is also compatible\nwith multiple distributed launchers.\n\nWith torch.distributed.launch\nHere we are using the torch.distributed.launch script in order to\nspawn the processes:\npython -m torch.distributed.launch --nproc_per_node 2 --use_env ignite_idist.py --backend gloo\n\n\nWith horovodrun\nhorovodrun -np 4 -H hostname1:2,hostname2:2 python ignite_idist.py --backend horovod\n\nNote\nIn order to run this example and to avoid the installation procedure, you can pull one of PyTorch-Ignite's docker image with pre-installed Horovod. It will include Horovod with gloo controller and nccl support.\ndocker run --gpus all -it -v $PWD:/project pytorchignite/hvd-vision:latest /bin/bash\ncd project\n\n\n\nWith slurm\nThe same result can be achieved by using slurm without any\nmodification to the code:\nsrun --nodes=2\n     --ntasks-per-node=2\n     --job-name=pytorch-ignite\n     --time=00:01:00\n     --partition=gpgpu\n     --gres=gpu:2\n     --mem=10G\n     python ignite_idist.py --backend nccl\nor using sbatch script.bash with the script file script.bash:\n#!/bin/bash\n#SBATCH --job-name=pytorch-ignite\n#SBATCH --output=slurm_%j.out\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=2\n#SBATCH --time=00:01:00\n#SBATCH --partition=gpgpu\n#SBATCH --gres=gpu:2\n#SBATCH --mem=10G\n\nsrun python ignite_idist.py --backend nccl\n\n\n\n\nClosing Remarks\nAs we saw through the above examples, managing multiple configurations\nand specifications for distributed computing has never been easier. In\njust a few lines we can parallelize and execute code wherever it is\nwhile maintaining control and simplicity.\n\nReferences\n\nidist-snippets:\ncomplete code used in this post.\nwhy-ignite: examples\nwith distributed data parallel: native pytorch, pytorch-ignite,\nslurm.\nCIFAR10\nexample\nof distributed training on CIFAR10 with muliple configurations: 1 or\nmultiple GPUs, multiple nodes and GPUs, TPUs.\n\n\n\nNext Steps\n\nIf you want to learn more about PyTorch-Ignite or have any further\nqueries, here is our GitHub,\ndocumentation and\nDiscord.\nPyTorch-Ignite is currently maintained by a team of volunteers and we\nare looking for more contributors.\nSee CONTRIBUTING.md\nfor how you can contribute.\nKeep updated with all PyTorch-Ignite news by following us on\nTwitter and\nFacebook.",
      "tags": "Deep Learning,Distributed,Horovod,Machine Learning,PyTorch,PyTorch DDP,PyTorch XLA,PyTorch-Ignite,SLURM",
      "url": "https://labs.quansight.org/blog/2021/06/distributed-made-easy-with-ignite/"
    },
    {
      "title": "Working with pytest on PyTorch",
      "text": "Prerequisites\n\n\n\n\nTo run the code in this post yourself, make sure you have torch, ipytest>0.9, and the plugin to be introduced pytest-pytorch installed.\n\n\n\n\n\n\n\n\npip install torch 'ipytest>0.9' pytest-pytorch\n\n\n\n\n\n\n\nBefore we start testing, we need to configure ipytest. We use the ipytest.autoconfig() as base and add some pytest CLI flags in order to get a concise output.\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \nimport ipytest\n\nipytest.autoconfig(defopts=False)\n\ndefault_flags = (\"--quiet\", \"--disable-warnings\")\n\ndef _configure_ipytest(*additional_flags, collect_only=False):\n    addopts = list(default_flags)\n    if collect_only:\n        addopts.append(\"--collect-only\")\n    addopts.extend(additional_flags)\n    \n    ipytest.config(addopts=addopts)\n\ndef enable_pytest_pytorch(collect_only=False):\n    _configure_ipytest(collect_only=collect_only)\n    \ndef disable_pytest_pytorch(collect_only=False):\n    _configure_ipytest(\"--disable-pytest-pytorch\", collect_only=collect_only)\n    \ndisable_pytest_pytorch()\n\n\n    \n\n\n\n\n\n\n\n\nIf you work on PyTorch and like pytest you may have noticed that you cannot run some tests in the test suite using the default pytest double colon syntax {MODULE}::TestFoo::test_bar.\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \n%%run_pytest[clean] {MODULE}::TestFoo::test_bar\n\nfrom torch.testing._internal.common_utils import TestCase\nfrom torch.testing._internal.common_device_type import instantiate_device_type_tests\n\n\nclass TestFoo(TestCase):\n    def test_bar(self, device):\n        assert False, \"Don't worry, this is supposed to happen!\"\n\n    \ninstantiate_device_type_tests(TestFoo, globals(), only_for=[\"cpu\"])\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n1 warning in 0.01s\n\n\n\n\n\n\n    \n\n\n\nERROR: not found: /home/user/tmp35zsok9u.py::TestFoo::test_bar\n(no name '/home/user/tmp35zsok9u.py::TestFoo::test_bar' in any of [<Module tmp35zsok9u.py>])\n\n\n\n\n\n\n\n\n\n\n\n\nIf the absence of this very basic pytest feature has ever been the source of frustration for you, you don't need to worry anymore. By installing the pytest-pytorch plugin with\n\n\n\n\n\n\n\n\npip install pytest-pytorch\n\n\n\n\n\n\n\nor\n\n\n\n\n\n\n\n\nconda install -c conda-forge pytest-pytorch\n\n\n\n\n\n\n\nyou get the default pytest experience back even if your workflow involves running tests from within your IDE!\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \nenable_pytest_pytorch()\n\n\n    \n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \n%%run_pytest {MODULE}::TestFoo::test_bar\n                \npass\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nF                                                                        [100%]\n=================================== FAILURES ===================================\n___________________________ TestFooCPU.test_bar_cpu ____________________________\n\nself = <__main__.TestFooCPU testMethod=test_bar_cpu>, device = 'cpu'\n\n    def test_bar(self, device):\n>       assert False, \"Don't worry, this is supposed to happen!\"\nE       AssertionError: Don't worry, this is supposed to happen!\nE       assert False\n\n<ipython-input-2-f22a5e9e7b30>:7: AssertionError\n=========================== short test summary info ============================\nFAILED tmpt8c1r46_.py::TestFooCPU::test_bar_cpu - AssertionError: Don't worry...\n1 failed, 1 warning in 0.17s\n\n\n\n\n\n\n\n\n\n\n\nAs you can see, with pytest-pytorch enabled, pytest ran the correct test but collected it under a different name. In this post we are going to find out why this is happening and what pytest-pytorch does to make your life easier.\n\n\n\n\n\n\n\nPyTorch test generation\u00b6PyTorch has an extensive test suite with a lot of configuration options and auto-generated tests. Internally, PyTorch uses a TestCase class that is derived from unittest.TestCase. In the first part of the post we are going to explore how the auto-generation of tests works in the PyTorch test suite and how they are collected by pytest.\nIn its default definition PyTorch's TestCase behaves exactly like its base class with regards to test collection.\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \ndisable_pytest_pytorch(collect_only=True)\n\n\n    \n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \n%%run_pytest[clean] {MODULE}\n\nfrom torch.testing._internal.common_utils import TestCase\n\n\nclass TestFoo(TestCase):\n    def test_bar(self):\n        pass\n\n    def test_baz(self):\n        pass\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\ntmpy90fpw2t.py::TestFoo::test_bar\ntmpy90fpw2t.py::TestFoo::test_baz\n\n2 tests collected in 0.04s\n\n\n\n\n\n\n\n\n\n\n\nDevice parametrization\u00b6Most TestCase's use additional configuration, though. In PyTorch, most operations can be performed on other device's than the CPU, for example a GPU. Thus, to not repeat yourself by writing the same test for multiple device's, the possible devices are used as parameters for the test. In PyTorch this is done with the instantiate_device_type_tests function.\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \n%%run_pytest[clean] {MODULE}\n\nfrom torch.testing._internal.common_utils import TestCase\nfrom torch.testing._internal.common_device_type import instantiate_device_type_tests\n\n\nclass TestFoo(TestCase):\n    def test_bar(self, device):\n        pass\n\n    def test_baz(self, device):\n        pass\n\n    \ninstantiate_device_type_tests(TestFoo, globals(), only_for=[\"cpu\", \"cuda\"])\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\ntmpyt_bxm9e.py::TestFooCPU::test_bar_cpu\ntmpyt_bxm9e.py::TestFooCPU::test_baz_cpu\ntmpyt_bxm9e.py::TestFooCUDA::test_bar_cuda\ntmpyt_bxm9e.py::TestFooCUDA::test_baz_cuda\n\n4 tests collected in 0.01s\n\n\n\n\n\n\n\n\n\n\n\nAs the name implies, instantiate_device_type_tests uses the passed test case as template to instantiate new test cases from it for the different device's. In this process the names of test cases as well as its test functions are changed:\n\nThe test case is namespaced by postfixing the device name in uppercase letters (TestFoo \u27f6 TestFooCPU)\nEach test function is namespaced by postfixing the device name in lower case letters and an additional underscore as separator (test_bar \u27f6 test_bar_cpu)\n\nAfter the instatiation, the current tested device is supplied to each test function.\nWe are glossing over many details here, two of which I should at least mention:\n\nAlthough it looks like only the test functions need to be parametrized, the parametrized test cases perform different setup and teardown on a per-device basis.\nThere are many decorators available that allow to adapt the device parametrization on a per-function basis.\n\n\n\n\n\n\n\n\nData type parametrization\u00b6In the same spirit as the device parametrizations, most PyTorch operators support a plethora of data types (dtype's in short). We can parametrize a test function with the @dtypes decorator, after which the dtype is available as parameter. Note that we can only use the @dtypes decorator if templating is enabled, which means that we have to use instantiate_device_type_tests.\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \n%%run_pytest[clean] {MODULE}\n\nimport torch\n\nfrom torch.testing._internal.common_utils import TestCase\nfrom torch.testing._internal.common_device_type import (\n    instantiate_device_type_tests,\n    dtypes,\n)\n\n\nclass TestFoo(TestCase):\n    @dtypes(torch.int32, torch.float32)\n    def test_bar(self, device, dtype):\n        pass\n\n\ninstantiate_device_type_tests(TestFoo, globals(), only_for=\"cpu\")\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\ntmpttnibud4.py::TestFooCPU::test_bar_cpu_float32\ntmpttnibud4.py::TestFooCPU::test_bar_cpu_int32\n\n2 tests collected in 0.01s\n\n\n\n\n\n\n\n\n\n\n\nSimilar to the device parametrization, the dtype name is postfixed to the name of the test function after the device (test_bar \u27f6 test_bar_cpu_float32). Since there is no need for special setup or teardown on a per-dtype basis, the test case is not instatiatied for different dtype's (TestFoo \u27f6 TestFooCPU).\nAgain, there are more decorators available for granular control, but they go beyond the scope of this post.\n\n\n\n\n\n\n\nOperator parametrization\u00b6A recent addition to the PyTorch test suite is the OpInfo class. It carries the meta data of an operator such as per-device supported dtype's or an optional reference function from another library. Going through all options would facilitate a blog post on its own, so we are going to stick to the basics here.\nOpInfo's enable even less duplicated code. For example, the test structure for checking an operator against a reference implementation is operator-agnostic. To parametrize a test function, we use the @ops decorator. We define our own op_db here, but in the PyTorch test suite there are pre-defined databases, for different operator types such as unary or binary operators. Again, note that we can only use the @ops decorator if templating is enabled, which means that we have to use instantiate_device_type_tests.\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \n%%run_pytest[clean] {MODULE}\n\nimport torch\n\nfrom torch.testing import _dispatch_dtypes\nfrom torch.testing._internal.common_device_type import (\n    instantiate_device_type_tests,\n    ops,\n)\nfrom torch.testing._internal.common_methods_invocations import OpInfo\nfrom torch.testing._internal.common_utils import TestCase\n\n\nop_db = [\n    OpInfo(\"add\", dtypesIfCPU=_dispatch_dtypes([torch.int32])), \n    OpInfo(\"sub\", dtypesIfCPU=_dispatch_dtypes([torch.float32])),\n]\n\n\nclass TestFoo(TestCase):\n    @ops(op_db)\n    def test_bar(self, device, dtype, op):\n        pass\n\n\ninstantiate_device_type_tests(TestFoo, globals(), only_for=\"cpu\")\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\ntmpe119_vdl.py::TestFooCPU::test_bar_add_cpu_int32\ntmpe119_vdl.py::TestFooCPU::test_bar_sub_cpu_float32\n\n2 tests collected in 0.04s\n\n\n\n\n\n\n\n\n\n\n\nIn contrast to the dtype, the op name is placed before the device identifier in the name of a test function (test_bar \u27f6 test_bar_add_cpu_int32). Still, no special setup or teardown is needed on a per-op basis, so the test case is only instantiated for the device (TestFoo \u27f6 TestFooCPU).\n\n\n\n\n\n\n\npytest \"equivalent\"\u00b6From a pytest perspective, the PyTorch test generation is \"equivalent\" to using the @pytest.mark.parametrize decorator. Of course this ignores all the gory details, which makes it seem easier than it is. Still, it might be a good mental analogy for someone coming from a pytest background.\n\n\n\n\n\n\nIn\u00a0[10]:\n\n    \n%%run_pytest[clean] {MODULE}\n\nimport pytest\n\nimport torch\n\n\n@pytest.mark.parametrize(\"device\", [\"cpu\"])\nclass TestFoo:\n    @pytest.mark.parametrize(\"dtype\", [pytest.param(torch.float32, id=\"float32\")])\n    @pytest.mark.parametrize(\"op\", [\"add\", \"sub\"])\n    def test_bar(self, device, dtype, op):\n        pass\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\ntmp8_w7dn68.py::TestFoo::test_bar[add-float32-cpu]\ntmp8_w7dn68.py::TestFoo::test_bar[sub-float32-cpu]\n\n2 tests collected in 0.00s\n\n\n\n\n\n\n\n\n\n\n\nSo far we looked at the test generation in the PyTorch test suite and how the tests are collected by pytest. Although PyTorch uses a different parametrization scheme than pytest, it is 100% compatible. The problems only materialize if you want select a specific test case or test function rather than say a whole module or the complete test suite.\n\n\n\n\n\n\n\nPyTorch test selection with pytest\u00b6As we observed above, pytest's default double colon :: notation, does not work on tests instantiated by PyTorch's test suite.\n\n\n\n\n\n\nIn\u00a0[11]:\n\n    \n%%run_pytest[clean] {MODULE}::TestFoo::test_bar\n\nfrom torch.testing._internal.common_utils import TestCase\nfrom torch.testing._internal.common_device_type import instantiate_device_type_tests\n\n\nclass TestFoo(TestCase):\n    def test_bar(self, device):\n        pass\n    \n    def test_baz(self, device):\n        pass\n\n    \ninstantiate_device_type_tests(TestFoo, globals(), only_for=[\"cpu\"])\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\nno tests collected in 0.01s\n\n\n\n\n\n\n    \n\n\n\nERROR: not found: /home/user/tmpg1b9f42o.py::TestFoo::test_bar\n(no name '/home/user/tmpg1b9f42o.py::TestFoo::test_bar' in any of [<Module tmpg1b9f42o.py>])\n\n\n\n\n\n\n\n\n\n\n\n\nEquipped with the knowledge we gathered about the instantiation, it is easy to see why this is happening.\n\n\n\n\n\n\nIn\u00a0[12]:\n\n    \n%%run_pytest {MODULE}\n\npass\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\ntmpo9kwbe9q.py::TestFooCPU::test_bar_cpu\ntmpo9kwbe9q.py::TestFooCPU::test_baz_cpu\n\n2 tests collected in 0.01s\n\n\n\n\n\n\n\n\n\n\n\npytest searches for the test case TestFoo with the test function test_bar, but can't find them, because instantiate_device_type_tests renamed them to TestFooCPU and test_bar_cpu. However, the test is selectable by its new, instantiated name {MODULE}::TestFooCPU::test_bar_cpu\n\n\n\n\n\n\nIn\u00a0[13]:\n\n    \n%%run_pytest {MODULE}::TestFooCPU::test_bar_cpu\n                \npass\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\ntmp_i0_ha5r.py::TestFooCPU::test_bar_cpu\n\n1 test collected in 0.04s\n\n\n\n\n\n\n\n\n\n\n\nFrom a convenience standpoint this is not optimal, because we need to remember the naming scheme. Furthermore, we can only select a specific parametrization rather than running a test case or a test function against all available parametrizations. The usual way around this is to rely on the pytest -k flag to do a pattern matching.\n\n\n\n\n\n\nIn\u00a0[14]:\n\n    \n%%run_pytest {MODULE} -k \"TestFoo and test_bar\"\n                \npass\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\ntmp633i_bea.py::TestFooCPU::test_bar_cpu\n\n1/2 tests collected (1 deselected) in 0.01s\n\n\n\n\n\n\n\n\n\n\n\nIn contrast to the default pytest practice we need to include the names of the test case and test function in the pattern rather than only the parametrization we want to select. This brings its own set of problems with it, for example if test cases or test functions use names that build on top of each other. Since the selection pattern does not support regular expression matching, it can get verbose and confusing.\n\n\n\n\n\n\nIn\u00a0[15]:\n\n    \n%%run_pytest[clean] {MODULE} -k \"TestFoo and not TestFooBar and test_spam and not test_spam_ham\"\n\nfrom torch.testing._internal.common_utils import TestCase\nfrom torch.testing._internal.common_device_type import instantiate_device_type_tests\n\nclass TestFoo(TestCase):\n    def test_spam(self, device):\n        pass\n    \n    def test_spam_ham(self, device):\n        pass\n\n    \ninstantiate_device_type_tests(TestFoo, globals(), only_for=\"cpu\")\n    \n    \nclass TestFooBar(TestCase):\n    def test_spam(self, device):\n        pass\n\n\ninstantiate_device_type_tests(TestFooBar, globals(), only_for=\"cpu\")\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\ntmpzoevi0fu.py::TestFooCPU::test_spam_cpu\n\n1/3 tests collected (2 deselected) in 0.01s\n\n\n\n\n\n\n\n\n\n\n\npytest-pytorch\u00b6Introducing: pytest-pytorch. After its installation and without any configuration, we get the default pytest experience back. Thus, even in complicated naming situations we can simply select a test with the double colon notation {MODULE}::TestFoo::test_spam\n\n\n\n\n\n\nIn\u00a0[16]:\n\n    \nenable_pytest_pytorch(collect_only=True)\n\n\n    \n\n\n\n\n\n\nIn\u00a0[17]:\n\n    \n%%run_pytest {MODULE}::TestFoo::test_spam\n                \npass\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\ntmpalkh4ddp.py::TestFooCPU::test_spam_cpu\n\n1 test collected in 0.05s\n\n\n\n\n\n\n\n\n\n\n\nOf course we can still use the pytest -k flag to select a specific parametrization.\n\n\n\n\n\n\nIn\u00a0[18]:\n\n    \n%%run_pytest[clean] {MODULE}::TestFoo::test_bar -k \"cuda\"\n\nfrom torch.testing._internal.common_utils import TestCase\nfrom torch.testing._internal.common_device_type import instantiate_device_type_tests\n\n\nclass TestFoo(TestCase):\n    def test_bar(self, device):\n        pass\n\n    \ninstantiate_device_type_tests(TestFoo, globals(), only_for=[\"cpu\", \"cuda\"])\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\ntmp6av_zpy3.py::TestFooCUDA::test_bar_cuda\n\n1/2 tests collected (1 deselected) in 0.02s\n\n\n\n\n\n\n\n\n\n\n\nHandwaving over details pytest-pytorch achieves this by hooking into pytest's test collection and performing the matching of the instantiated to the template name for you. If that sounds intruiging, have a look at the GitHub repository.\n\n\n\n\n\n\n\nIDEs with pytest support\u00b6Another use case for pytest-pytorch are modern Python IDEs such as PyCharm or VSCode with built-in pytest support. Within such an IDE you can run a test by clicking a button next to its definition without dropping into a terminal. Doing this, you also get the comfort of the IDE including the debugger.\nThis feature relies on the test selection with the default pytest syntax, which, as we have seen above, does not work well with the PyTorch test suite. You could fiddle with your IDEs default test runner config or you could simply install pytest-pytorch to make it work out of the box.\n\n\n\n\n\n\n\nConclusion\u00b6In this post we took a look at how the PyTorch test suite auto-generates device-, dtype-, and even operator-agnostic tests. Although it uses a different scheme than pytest, the tests can still be collected and run by it. The compatibility breaks down, if one tries to use the default pytest selection notation. To overcome this and in turn enhance the developer experience we introduced the pytest-pytorch plugin. It can for example be used to regain out-of-the-box pytest support in modern IDEs when working on PyTorch.",
      "tags": "",
      "url": "https://labs.quansight.org/blog/2021/06/pytest-pytorch/"
    },
    {
      "title": "Putting out the fire: Where do we start with accessibility in JupyterLab?",
      "text": "JupyterLab Accessibility Journey Part 2\nI want to be honest with you, I started asking accessibility questions\nin JupyterLab spaces while filled with anxiety. Anxiety that I was shouting\ninto the void and no one else would work on accessibility with me. Anxiety\nthat I didn\u2019t have the skills or energy or knowledge to back up what I\nwanted to do. Anxiety that I was going to do it wrong and make JupyterLab\neven more inaccessible. Sometimes I still feel that way.\n\n\nHere\u2019s the thing. That anxiety, while real and worth acknowledging, doesn\u2019t\nhelp the disabled people we constantly fail and exclude when we keep building\nthings inaccessibly. So yes, I want you to know I felt that way and that you\nmight too, but I also want you to remember who we are here for, especially\nif you are working to support a group you aren\u2019t a part of (as is the\ncase with me). Plus, many of these concerns didn\u2019t end up happening. First,\nI didn\u2019t end up being alone at all! Each of the people that have joined in\nhave different skills that have helped us tackle issues that I don\u2019t think\nany of us would\u2019ve been able to on our own. Knowing we are working together\nalso helps keep me accountable because I\u2019d like to be able to show up to our\nmeetings with something to share. As for worrying that I\u2019m doing it all\nwrong, I suppose that\u2019s still a possibility. Speaking for myself, I\u2019d rather\nbe making mistakes, learning, and iterating than continue to let JupyterLab\nstay inaccessible indefinitely.\nIn a space where considering the needs of disabled people isn\u2019t the standard,\naccessibility might feel like an insurmountable challenge. For example, when\nI showed up to our first devoted accessibility meeting, JupyterLab\u2019s\naccessibility status was like a hazy shape in the distance. I was pretty sure\nit wasn\u2019t good, but I didn\u2019t know for sure how or why. A few meetings later\nand a closer look made me realize that haze was actually smoke and I'd walked\nmyself and others directly into a (metaphorical) burning building. But just\nbecause it felt like everything was chaos without a good place to start didn't\nmean that was the truth. In fact, it wasn't. Building software is more about\npeople more than any tool, so let\u2019s consider what our regular team of people\non the user-contributor-maintainer spectrum said are the basics of what they\ncare about in JupyterLab.\nUsers want to:\n\nUse JupyterLab to read or navigate documents.\nUse JupyterLab to edit and run documents. To edit a document, users need to\nbe able to navigate where they want to edit, so the read-only experience is a\nprerequisite.\nKnow what things they can do in JupyterLab and get help on how to do it.\n\nContributors want to:\n\nGain enough understanding of a JupyterLab in order to work with it.\nUnderstand the expectations of their contributions and how to meet them. In\nthis case, they would want to know that they need to think about accessibility\nand how to consider that.\n\nMaintainers want to:\n\nEnsure that JupyterLab is both progressing and relatively stable.\nPromote sustainable growth for a project that doesn\u2019t overwrite past efforts.\nAutomation can be helpful because maintainers are usually strapped for time.\n\nWith the support of a team member with prior experience auditing for accessibility,\nwe pinpointed specific ways\nin which JupyterLab lacked support for accessibility broken up by\nWCAG 2.1 standards.\nFrom conversations with these more experienced community members, we found that\nissues generally broke up into four categories of work needed (not necessarily\nin this order):\n1. Make JupyterLab accessible for a read-only type experience\nThis is something users need. For our purposes, we\u2019re using read-only to\ndescribe what you need to navigate and consume all the content in JupyterLab\nfrom the interface to the documents and processes that live in it. Most\nof this also falls under WCAG standards, and are the first things users\nneed to start working with JupyterLab since it\u2019s difficult to interact\nwith a space if you can\u2019t get where you want to go.\n2. Make JupyterLab accessible for an interacting/editing experience\nThis is something users need and is the other half of WCAG standards. Once\nyou can navigate the space, people need to interact by writing, editing,\nrunning process, and so on. While WCAG standards do cover interactive web\nexperiences and they are written generally enough to apply to many interface\ntypes, their roots in a more standard website experience means that we\nalso have some grey areas to account for since JupyterLab can easily include\ncomplex and layered interactions than even other web apps. We are supporting\nthis by looking into how other tools with similar uses (like coding) approach\nthese types of accessibility and hope to test it in the future.\n3. Accessibility documentation\nThis is something users and contributors need and has two parts. One part\nis making the documentation itself accessible through WCAG compliance in\nthe docs theme, labeling relevant content, and providing content in different\nforms. Second is adding documentation specifically for accessibility such\nas how to use accessibility features and how accessibility fits in to our\ncontribution process.\nAccessibility and documentation both have reputations for falling to the\nwayside, and we almost got so caught up in applying WCAG standards to the\nsoftware itself that we continued the pattern. But making an accessible\nexperience is, like any UX problem, not limited to the time spent within\nthat product. Think of it this way, if there is no accessible documentation\non how to get started with JupyterLab and use relevant accessibility\nsupport, then all the work we\u2019ve done in the software itself won\u2019t be able\nto serve the very people it is there for.\n4. Adding relevant accessibility tests to the JupyterLab contributing workflow\nThis is something contributors and maintainers need, though the results\nalso benefit users. As grateful as I am to have a group of people who are\ntaking action to make JupyterLab accessible, it isn\u2019t enough on its own.\nWe aren\u2019t a group that can review every single PR and we may not all be\nable to devote time to this forever; tests ensure that accessibility\nremains a priority in the contributing workflow regardless of individual\ninvolvement. It also will help prevent current efforts from being\noverwritten by new contributions.\nAutomated accessibility testing has its limits\nbecause you are trying to quantify an experience without getting users\ninvolved, but I think a first pass and a reminder to the community\u2014especially\nthe contributing community\u2014that accessibility is something we are all\nresponsible for is critical. Since accessibility isn\u2019t yet a regular\nstandard for contributions in many projects, feedback from tests might\nalso be an opportunity for people who haven\u2019t worked with accessibility\nbefore to start learning more.\nWhere we are now\nAs I\u2019m writing this post, our team is mostly focused on JupyterLab\naccessibility for WCAG compliance starting with the read-only type\nexperience. Among many things, JupyterLab is currently missing of\nlandmarks and\nlabels that block manual\naccessibility testing to a degree since they prevent further navigation\nand interaction. Starting here means that we are a step closer to\nusers being able to accessibly read content in the interface.\nIf you are going to take away one thing from my journey so far, I\u2019d\ntell you to be consistently brave. Feeling anxious in the face of\nchallenges and accepting areas where you don\u2019t yet have knowledge is\nnormal, but it isn\u2019t reason to back down. Find the people that will\ncollaborate with you and dive in. And when I get lost and don\u2019t know\nwhat to do, I find it most helpful to put people first and remember\nwho I am doing this for. Breaking the work into pieces by what users\nneed can help you strategically start putting out fires.\nFocusing on people just for strategy isn\u2019t all though. Be on the look\nout for my next blog where I\u2019ll talk about what the disconnect of\nwhat accessibility meant to different people in our community and how\nthat impacted the time and way we\u2019ve solved issues in JupyterLab so far.\n\nThis is part of a series of blogs around making JupyterLab more accessible. You can read the\nwhole series here.\nInterested in getting involved? Join our community via the JupyterLab accessibility meetings\nlisted every other week on the Jupyter community calendar.",
      "tags": "Accessibility,JLabA11y,JupyterLab",
      "url": "https://labs.quansight.org/blog/2021/05/putting-out-the-fire/"
    },
    {
      "title": "Rethinking Jupyter Interactive Documentation",
      "text": "Jupyter Notebook first release was 8 years ago \u2013 under the IPython Notebook\nname at the time. Even if notebooks were not invented by Jupyter; they were\ndefinitely democratized by it. Being Web powered allowed development of many\nchanges in the Datascience world. Objects now often expose rich representation; from\nPandas dataframes with as html tables, to more recent Scikit-learn model.\nToday I want to look into a topic that has not evolved much since, and I believe\ncould use an upgrade. Accessing interactive Documentation when in a Jupyter\nsession, and what it could become. At the end I'll link to my current prototype\nif you are adventurous.\n\n\nThe current limitation for users\nThe current documentation of IPython and Jupyter come in a few forms, but mostly\nhave the same limitation.\nThe typical way to reach for help is to use the ? operator. Depending on\nthe frontend you are using it will bring a pager, or a panel that will display\nsome information about the current object.\nIt can show some information about the current object (signature, file,\nsub/super classes) and the raw DocString of the object.\nYou can scroll around but that's about it whether in terminal or Notebooks.\nCompare it to the same documentation on the NumPy website.\n\nOn the left is the documentation for NumPy when visiting the NumPy website. Let's\ncall that \"rendered documentation\". On the right what you get in Jupyter Lab or\nin the IPython or regular Python REPL, let's cal that \"help documentation\" since\nit is typically reached via identifier? or help(identifier)\nCompared to rendered documentation, the help documentation is:\n\nHard to read,\nHas no navigation,\nRST Directives have not been interpreted,\nNo inline graphs, no rendered math.\n\nThere is also no access to non-docstring based documentation, no narrative,\nno tutorials, no image gallery or examples, no search, no syntax\nhighlighting, no way to interact or modify documentation to test effects of\nparameters.\nLimitation for authors\nDue to Jupyter and IPython limitations to display documentation I believe\nauthors are often contained to document functions.\nSyntax in docstrings is often kept simple for readability, this first version is\noften preferred:\nYou can use ``np.einsum('i->', a)`` ...\n\n\n\nIn the longer form, which makes the reference into a link when viewing rendered\ndocumentation, it is difficult to read when shown as help documentation:\nYou can use :py:func:`np.einsum('i->', a) <numpy.einsum>` ...\n\n\n\nThis also leads to long discussions about which syntax to use in advanced areas,\nlike formulas in Sympy's docstrings.\nMany projects have to implement dynamic docstrings; for example to include all\nthe parameters a function or class would pass down using **kwargs (search\nthe matplotlib source code for _kwdoc for example, or look at the pandas.DataFrame implementation).\nThis can make it relatively difficult for authors and contributors to properly\nmaintain and provide comprehensive docs.\nI'm not sure I can completely predict all the side effects this has on how library\nmaintainers write docs; but I believe there is also a strong opportunity for a\ntool to help there. See for example v\u00e9lin\nwhich attempts to auto reformat and fix common NumPyDoc's format mistakes and\ntypos \u2013 but that's a subject of a future post.\nStuck between a Rock and a Hard place\nWhile Sphinx and related projects are great at offering hosted HTML\ndocumentation, extensive usage of those makes interactive documentation harder\nto consume.\nWhile it is possible to run Sphinx on the fly when rendering\ndocstrings, most Sphinx features\nonly work when building a full project, with the proper configuration and\nextension, and can be computationally intensive. This makes running Sphinx locally\nimpractical.\nHosted websites often may not reflect the locally installed version of the\nlibraries and require careful linking, deprecation and narrative around\nplatform or version specific features.\nThis is fixable\nFor the past few months I've been working on rewriting how IPython (and hence\nJupyter) can display documentation. It works both in terminal (IPython) and\nbrowser context (notebook, JupyterLab, Spyder) with proper rendering, and currently\nunderstands most directives; it could be customized to understand any new ones:\n\nAbove is the (terminal) documentation of scipy.polynomial.lagfit, see how the\nsingle backticks are properly understood and refer to known parameters, it\ndetected that  `n` is incorrect as it should have double backticks; notice\nthe rendering of the math even in terminal.\nFor that matter technically this does not care as to whether the DocString is\nwritten in RST or Markdown; though I need to implement the latter part. I believe\nthough that some maintainers would be quite happy to use Markdown, the syntax\nof which more users are familiar with.\n\nIt supports navigation \u2013 here in a terminal \u2013 where clicking or pressing enter on a\nlink would bring you to the target page. In the above gif you can see that many\ntokens of the code example are also automatically type-inferred (thanks Jedi), and\ncan also be clicked on to navigate to their corresponding page.\n\nImages are included, even in the terminal when they are not inline but replaced by\na button to open them in your preferred viewer (see the Open with quicklook in\nthe above screenshot).\nThe future\nI'm working on a number of other features, in particular:\n\nrendering of narrative docs \u2013 for which I have a prototype,\nautomatic indexing of all the figures and plots \u2013\u00a0 working but slow right now,\nproper cross-library referencing and indexing without the need for intersphinx.\n   For example, it is possible from the numpy.linspace page to see all pages that\n   reference it, or use numpy.linspace in their example section\n   (see previous image).\n\nAnd many others, like showing a graph of the local references between functions,\nsearch, and preference configurability. I think this could also support many\nother desirable features, like user preferences (hide/show type annotation,\ndeprecated directives, and custom color/syntax highlighting) - though I haven't started\nworking on these. I do have some ideas on how this could be used to provide\ntranslations as well.\nRight now, is it not as fast and efficient as I would like to\u00a0\u2013 though it's faster\nthan running Sphinx on the fly \u2013 but requires some ahead of time processing. And it\ncrashes in many places; it can render most of the documentation of SciPy, NumPy,\nxarray, IPython and scikit-image.\nI encourage you to think about what features you are missing when using\ndocumentation from within Jupyter and let me know. I hope this could become a\nnice addition to Sphinx when consulting documentation from within Jupyter.\nFor now I've submitted a Letter of intent to CZI EOSS\n4\nin an attempt to get some of that work funded to land in IPython, and if you\nhave any interest in contributing or want something like that for your library,\nfeel free to reach out.\nYou can find the repository on my GitHub account,\nit's still in pre-alpha stage. It is still quite unstable with too many hard\ncoded values to my taste, and needs some polish to be considered usable for production.\nI've focused my effort for now mostly on terminal rendering \u2013 a Jupyter notebook\nor JupyterLab extension would be welcome. So if you are adventurous and like to work\nfrom the cutting (or even bleeding) edge, please feel free to try it out and\nopen issues/pull request.\nIt also needs to be better documented (pun intended), I'm hoping to use papyri itself to\ndocument papyri; but it needs to be a bit more mature for that.\nStay tuned for more news, I'll try to explain how it works in more detail in a\nfollow-up post, and discuss some of the advantages (and drawbacks) this project\nhas.",
      "tags": "documentation,Open-Source,Python",
      "url": "https://labs.quansight.org/blog/2021/05/rethinking-jupyter-documentation/"
    },
    {
      "title": "Spot the differences: what is new in Spyder 5?",
      "text": "In case you missed it, Spyder 5 was released at the beginning of April! This \nblog post is a conversation attempting to document the long and complex \nprocess of improving Spyder's UI with this release. Portions lead by Juanita \nGomez are marked as Juanita, and those lead by Isabela Presedo-Floyd are \nmarked as Isabela.\nWhat did we do?\n[Juanita] Spyder was created more than 10 \nyears ago and it has had the contributions of a great number of developers \nwho have written code, proposed ideas, opened issues and tested PRs in order \nto build a piece of Spyder on their own. We (the Spyder team) have been lucky \nto have such a great community of people contributing throughout the years, \nbut this is the first time that we decided to ask for help from an UX/UI \nexpert! Why? You might wonder. Having the contributions of this great amount \nof people has resulted in inconsistencies around Spyder\u2019s interface which we \ndidn\u2019t stop to analyze until now. \nWhen Isabela joined Quansight, we realized that we had an opportunity of \nimproving Spyder\u2019s interface with her help. We thought her skill set was \neverything we needed to make Spyder\u2019s UI better.  So we started by reviewing \nthe results of a community survey from a few months ago and realized that \nsome of the most common feedback from users is related to its interface \n(very crowded, not consistent, many colors). This is why we decided to start \na joint project with Isabela, (who we consider now part of the Spyder team) \ncalled Spyder 5!!!\n\n\nThis version was in development for over a year and was finally released on \nApril 2nd. It has some nice new features that we hope will benefit our users \ngreatly. Most of these are focused on improving Spyder\u2019s interface and \nusability, which we did thanks to Isabela\u2019s help. The 3 main UX features \nimplemented in this release were:\n\nA brand new color palette designed to bring greater consistency to the UI \nand to make it easier to use.\nThe redesign of our toolbars by adjusting the margins and sizes of all the \nbuttons to meet accessibility recommendations.\nA new set of icons to ensure a consistent style.\n\nHow did we do it?\n1. First impressions\n[Isabela] I find collaboration usually starts with three things: \ndiscovering and stating a problem, asking why, and figuring out the best ways \nto communicate with each other. For me, this is a design problem on it\u2019s own, \nespecially when starting to work with a new team like I was with Spyder. For \nthis project, I was asked to audit Spyder for any UX/UI issues and report \nback. Because I have a habit of pushing every button in an interface, I ended \nup having a lot (maybe too much) feedback to pass on. One of the things I \nremember most about opening Spyder for the first time was having three dialogs \npop up immediately. That\u2019s really not the first impression you want to give, \nand I remember talking to Juanita about that right away. Figuring out how to \nstate problems as simply and clearly to a group of people I didn\u2019t know yet \nwas intimidating and went through several phases.\n2. From the \u201cnightmare document\u201d to the issue tracker\n[Juanita] The first phase was discussing all the problems that Isabela \nfound in weekly meetings with Carlos, the Spyder maintainer, and Stephanie, \nanother Spyder core developer. I created a Google drive document (which we \nended up calling \u201cThe Nightmare document\u201d) in which I collected most of the \nfeedback that Isabela gave us. Then, I grouped this information into \ncategories depending on whether the comments were about the interface, in \ngeneral, or if they were about a specific pane. Once we agreed on a relevant \nproblem that we wanted to address, I opened an issue on a new repo that we \ncreated in the Spyder\u2019s organization called \u201cux-improvements.\u201d\n[Isabela] In fact, that wasn\u2019t even the first set of documents we were \nworking with; I had a whole table, numbering system, and document I was \ntrying to handle before. But it was Juanita that turned them into Github \nissues.\n3. Sorting out the nightmare\n[Juanita] Since we ended up with more than 30 issues, we had to start a \n\u201ctriaging phase.\u201d We had to label, triage, organize, and prioritize issues \naccording to \u201curgency\u201d and importance. This issue tracker became our main \ntool to keep up with all the plans for the future!\n[Isabela] Juanita did wonderful work tracking our progress through issues \nand keeping us all accountable, but we were still left with a long list of \nissues to triage\u2014long enough that it wasn\u2019t all getting in Spyder 5. To have \nthe greatest impact on Spyder, we started with the issues that had \ninfluence on Spyder as a whole. Toolbars, icons, and colors are something\nyou will always encounter from the first impression to the most recent, so it \nmade sense to start thinking about those big picture issues first.\n4. Digging deeper into the dark hole\n[Isabela] When prioritizing the audit feedback for Spyder 5, each pass \nseemed to get to a deeper layer of the problem. For example, what started as \nissues to make tooltips more legible and improve the variable explorer\u2019s \ncolor coding soon became the realization that we weren\u2019t sure exactly what \nblue was already being used for much of Spyder\u2019s interface. It got more \ncomplicated when we found out how many colors were hard coded across multiple \nfiles or defined by an external project. Eventually, the problem changed from \nthe color contrast of tool tips to an unsustainable approach for managing \ncolor across the two default Spyder themes rooted in a non-Spyder repo. Work \nat each step did build up into a larger solution, but it\u2019s worth noting that \nit isn\u2019t what we set out to do in the first place. \n5. What witchcraft does Isabela do in the background?\n[Juanita] One of the most important parts of the process was designing \nthe mock ups for the new ideas that we came up with for the interface which \nis definitely not our expertise. So... how did the designs magically appear \non our Github issues?\n[Isabela] First things first, it isn\u2019t actually witchcraft even if it \nlooks magical from the outside. How I work depends somewhat on what problem \nwe are trying to solve, so let\u2019s use the design of custom icons for Spyder 5 \nas an example. Once I had a defined list of icons to work on, I needed to \nspend time making progress on my own. Research on best practices for the \nrelevant area of design is where I started; in this case, I knew we were \ngoing to be working with Material Design Icons\u2019 specifications. After that, \nI did a combination of pen-and-paper sketching and working digitally based on \nthe existing icons in Spyder and Material Design Icons while I kept note of \nthe pros and cons for different directions. I also collected design elements \nas I built them so that I could make more consistent, accurate designs faster \nas I worked. For the icons, this included things like letters, rounded \ncorners, and templates for the size and spacing of elements. Finally, I \ncompared options side by side and tried them out in the interface to evaluate \nwhat designs were strong enough to bring them to the rest of the team. Then we \ndiscussed the options together.\n\n6. Mock ups Vs Reality\n[Juanita] After many discussions, mock ups, and meetings, decisions were\nmade and we were ready to move onto the implementation phase. A big part of \nthe improvements were made in QDarkStyleSheet \nwhere we did the new palette and color system for both the \ndark and light themes of Spyder. In my opinion, this was the hardest part of \nthe process since it involved getting familiar with the code first and then, \ntrying and trying again changing lines of code to change the color or style \nof buttons, tabs, toolbars, borders, etc\u2026 \nThe other problem that I ran into, was trying to meet the designs\u2019 \nspecifications. Specially, when working with the toolbars, figuring the right \nnumber for the pixels of margins and sizes was a challenge. I tried several \nvalues before finding one that closely matched the proposed mock up only to \nrealize later that \u201cpixels\u201d was not the best unit for the specifications. I \nended up using \u201cem\u201d since it was more consistent across operating systems.\nIsabela, Stephanie and Carlos were part of this process as well. Between the \nfour of us we managed to implement all the changes that we had planned for \nSpyder 5, the new color palette, the complete redesign of toolbars and the \nnew set of icons. It was an arduous task, more than we all expected, but at \nthe end we were all very happy with the results and thankful to Isabela for \nhelping us to give a new face to Spyder. \nWhat's the final result?\n[Isabela] Individually, the colors, toolbars, and icons may feel like \nsmall adjustments, but those are some of the elements that make up most of \nSpyder. When they are together, those small adjustments set the mood in the \ninterface; they are more noticeable, and rooted in the Spyder UI many people \nare already familiar with. While the changes may feel obvious when they are \nnew, they are also chosen to create consistent patterns across interactions \nthat can become more comfortable over time. Spyder\u2019s default dark and light \nmodes, for example, used to use a different set of UI elements between modes. \nNow they both use the same elements and it is only the colors that change. \nThis makes it easier for users to jump into a familiar interface and take \nwhat they know from working in one space to another. For contributors, it \ngives a more clear UI pattern for them to follow in their own work.\nBefore and after (Dark theme)\n\nBefore and after (Light theme)\n\nWhat did we learn :)?\n[Isabela] From developing new skills to working as a team for the first \ntime, I think we both took a lot from this process. Here are some lessons that \nstood out to us.\n[Juanita] \n\nSometimes it is better to try some of the ideas during the process, than \nhaving long discussions about an idea and implementing at the end. In some \ncases you end up realizing that things don\u2019t look as good as you thought they \nwould, or that some are not even possible.\nOne of the most important parts of the design process is to get yourself in \nthe users\u2019 shoes. At the end, they are the reason why we work to improve \nthings constantly.\nOccasionally, less is more. Simple and consistent is better than crowded \nand complicated. \n\n[Isabela] \n\nDon\u2019t be afraid of asking questions even when you think you understand the \nproblem because every bit of information can be useful to better grasping what \nhurts or helps users.\nAlways take the time to review what you might think is obvious with the \nrest of the team. It\u2019s easy to forget about what you know when you are \nworking with people who have different skills than you.",
      "tags": "release,Spyder,UX/UI",
      "url": "https://labs.quansight.org/blog/2021/04/spot-the-diffenrences/"
    },
    {
      "title": "A step towards educating with Spyder",
      "text": "As a community manager in the Spyder team, I have been looking for ways of\ninvolving more users in the community and making Spyder useful for a larger\nnumber of people. With this, a new idea came: Education.\nFor the past months, we have been wondering with the team whether Spyder\ncould also serve as a teaching-learning platform, especially in this era\nwhere remote instruction has become necessary. We submitted a proposal to the\nEssential Open Source Software for Science (EOSS) program of the Chan\nZuckerberg Initiative, during its third cycle, with the idea of providing a\nsimple way inside Spyder to create and share interactive tutorials on topics\nrelevant to scientific research. Unfortunately, we didn\u2019t get this funding,\nbut we didn\u2019t let this great idea die.\nWe submitted a second proposal to the Python Software Foundation\nfrom which we were awarded $4000. For me, this is the perfect opportunity for\nus to take the first step towards using Spyder for education.\n\n\nWhat the project is about\nThe goal of this project is to create specialized Python online training\ncontent that uses Spyder as the main platform to deliver it. The grant will\ncover the development of three practical workshops:\n\nPython for Financial Data Analysis with Spyder\nPython for Scientific Computing and Visualization with Spyder\nSpyder 5 Plugin Development\n\nThey will be included as part of Spyder\u2019s documentation\nfor remote learning, but they will also be used as hands-on materials for talks and workshops.\nThese materials are meant for users to learn how Spyder can accelerate their\nworkflow when working with Python in scientific research and data analysis.\nThe idea is for us to provide a way in which we can help people get the most\nout of Spyder by applying it in their day-to-day jobs.\nThe first two workshops will cover aspects such as data exploration and\nvisualization with Spyder\u2019s variable explorer and plots panes, getting\ndocumentation through Spyder\u2019s help pane, writing good quality and efficient\ncode using Spyder\u2019s code analysis and profiler, etc.\nOur last workshop will demonstrate how to create a plugin for Spyder, which,\nthanks to our new API in Spyder 5,\nreleased in April 2021, will allow users to easily customize and\nextend Spyder\u2019s interface with new menus, toolbars, widgets or panes in order\nto adapt it to their own needs...\nWhy it is important\nThis project will benefit the international community of Spyder users\n(around 500,000, we estimate) to discover new capabilities of Spyder in order\nto take advantage of all its resources. It will also provide testing\nmaterials for potential users who will be able to adopt Spyder as a tool for\ntheir work in Financial Data Analysis, Scientific research and Spyder plugin\ndevelopment.\nFor the past months, our documentation tutorials\nhave had a great impact in our community, with more than 20,000 views in our\nYouTube channel. We expect these workshops to be a great input to our\ndocumentation and help us continue building a community around Spyder.\nWhat is next?\nThis project is just the first step towards making Spyder an educational\ntool. In the future, we hope that we can develop the infrastructure necessary\nto support in-IDE tutorials, by improving the tools like Jupyter Book,\nsphinx-thebe, MyST-Parser\nwhich will provide better integration to write educational tutorials.\nThe final goal is to enable researchers, educators and experts that don\u2019t\nnecessarily have a software engineering background to build scientific\nprogramming tutorials easily and provide them as online learning materials\nin Spyder. Once the infrastructure is built, we can develop several examples\nto demonstrate Spyder capabilities and teach basic scientific programming\nconcepts applicable to a variety of fields.",
      "tags": "community,funding,grant,Spyder",
      "url": "https://labs.quansight.org/blog/2021/04/a-step-towards-educating-with-spyder/"
    },
    {
      "title": "PyTorch TensorIterator Internals - 2021 Update",
      "text": "For contributors to the PyTorch codebase, one of the most commonly encountered\nC++ classes is\nTensorIterator.\nTensorIterator offers a standardized way to iterate over elements of\na tensor, automatically parallelizing operations, while abstracting device and\ndata type details.\nIn April 2020, Sameer Deshmukh wrote a blog article discussing\nPyTorch TensorIterator Internals. Recently,\nhowever, the interface has changed significantly. This post describes how to\nuse the current interface as of April 2021. Much of the information from the\nprevious article is directly copied here, but with updated API calls and some\nextra details.\n\n\nAll of the code examples below can be compiled and run in this GitHub\nrepo.\nBasics of TensorIterator and TensorIteratorConfig\nIn order to create a TensorIterator, a TensorIteratorConfig must be created\nfirst. TensorIteratorConfig specifies the input and output tensors that will\nbe iterated over, whether all tensors are expected to share the same data type\nand device, and a handful of other settings. After setting up the\nconfiguration, we can call TensorIteratorConfig::build() to obtain\na TensorIterator that has the specified settings. TensorIterator is\nimmutable, so once it is created, its configuration cannot be changed.\nIn the following example, a tensor named out is configured as the output\ntensor and a and b are the input tensors. Calling build creates the\nTensorIterator object from the specified configuration.\nat::TensorIteratorConfig iter_config;\niter_config\n  .add_output(out)\n  .add_input(a)\n  .add_input(b);\n\nauto iter = iter_config.build();\n\n\n\nPerforming iterations\nIterations using TensorIterator can be classified as point-wise iterations or\nreduction iterations. This plays a fundamental role in how iterations using\nTensorIterator are parallelized. Point-wise iterations can be freely\nparallelized along any dimension and grain size while reduction operations have\nto be either parallelized along dimensions that you're not iterating over or by\nperforming bisect and reduce operations along the dimension being iterated.\nNote that for CUDA, it is possible to parallelize along the reduction\ndimension, but synchronizations are needed to avoid race conditions.\nParallelization with vectorized operations can also be implemented.\nIteration details\nThe simplest iteration operation can be performed using the\nfor_each\nfunction. This function has two overloads: one takes a function object which\niterates over a single dimension (loop_t); the other takes a function object\nwhich iterates over two dimensions simultaneously (loop2d_t). Find their\ndefinitions\nhere.\nThe simplest way of using for_each is to pass it a lambda of type loop_t\n(or loop2d_t).\nIn the example below, the char** data argument of the copy_loop function\n(which is an instance of the loop_t lambda) contains a char* pointer for\neach of the tensors, in the order that they are specified in the\nTensorIteratorConfig. To make the implementation agnostic of any particular\ndata type, the pointer is typecast to char, so we can access it as an array\nof bytes.\nThe second argument is const int64_t* strides, which is an array containing\nthe strides of each tensor in the dimension that you're iterating over. We can\nadd this stride to the pointer received in order to reach the next element in\nthe tensor. The last argument is int64_t n which is the size of the dimension\nbeing iterated over.\nfor_each implicitly parallelizes the operation by executing copy_loop in\nparallel if the number of iterations is more than the value of\ninternal::GRAIN_SIZE, which is a value that is determined as the 'right\namount' of data to iterate over in order to gain a significant speedup using\nmulti-threaded execution. If you want to explicitly specify that your operation\nmust run in serial, then use the serial_for_each loop.\nat::TensorIteratorConfig iter_config;\niter_config\n  .add_output(out)\n  .add_input(a)\n\n  // call if output was already allocated\n  .resize_outputs(false)\n\n  // call if inputs/outputs have different types\n  .check_all_same_dtype(false);\n\nauto iter = iter_config.build();\n\n// Copies data from input into output\nauto copy_loop = [](char** data, const int64_t* strides, int64_t n) {\n  auto* out_data = data[0];\n  auto* in_data = data[1];\n\n  for (int64_t i = 0; i < n; i++) {\n    // assume float data type for this example\n    *reinterpret_cast<float*>(out_data) = *reinterpret_cast<float*>(in_data);\n    out_data += strides[0];\n    in_data += strides[1];\n  }\n};\n\niter.for_each(copy_loop);\n\n\n\nUsing kernels for iterations\nFrequently we want to create a kernel that applies a simple point-wise function\nonto entire tensors. TensorIterator provides various such generic kernels\nthat can be used for iterating over the elements of a tensor without having to\nworry about the stride, data type of the operands or details of the\nparallelism.\nFor example, say we want to build a function that performs the point-wise\naddition of two tensors and stores the result in a third tensor. We can use the\ncpu_kernel function. Note that in this example we assume a tensor of float,\nbut you can use one of the AT_DISPATCH_ALL_TYPES* macros to support multiple\ndata types.\nat::TensorIteratorConfig iter_config;\niter_config\n  .add_output(c)\n  .add_input(a)\n  .add_input(b);\n\nauto iter = iter_config.build();\n\n// Element-wise add\nat::native::cpu_kernel(iter, [] (float a, float b) -> float {\n  return a + b;\n});\n\n\n\nWriting the kernel in this way ensures that the value returned by the lambda\npassed to cpu_kernel will populate the corresponding position in the target\noutput tensor, as long as the inputs strictly broadcast over the output--that\nis, if the output's shape is equal to or greater than the input shape in all\ndimensions.\nSetting tensor iteration dimensions\nThe value of the sizes and strides will determine which dimension of the tensor\nyou will iterate over. TensorIterator performs optimizations to make sure\nthat at least most of the iterations happen on contiguous data to take\nadvantage of hierarchical cache-based memory architectures (think dimension\ncoalescing and reordering for maximum data locality).\nA multi-dimensional tensor has a stride value for each dimension. So the stride\nthat TensorIterator needs to use will be different depending on which\ndimension you want to iterate over. TensorIterator directly computes the\nstrides that get passed into the loop by itself within the build() function.\nHow exactly it computes the dimension to iterate over is something that should\nbe properly understood in order to use TensorIterator effectively.\nWhen performing a reduction operation (see the sum_out code in\nReduceOps.cpp),\nTensorIterator will figure out the dimensions that will be reduced depending\non the shape of the input and output tensor, which determines how the input\nwill be broadcast over the output. If you're performing a simple pointwise\noperation between two tensors (like a addcmul from\nPointwiseOps.cpp)\nthe iteration will happen over the entire tensor, without providing a choice of\nthe dimension. This allows TensorIterator to freely parallelize the\ncomputation, without guaranteeing the order of execution, since it does not\nmatter anyway.\nFor something like a cumulative sum operation, where you want be able to choose\nthe dimension to reduce but iterate over multiple non-reduced dimensions\n(possibly in parallel), you must be careful to take into account two different\nstrides--one for the dimension being reduced and one for all other dimensions.\nTake a look at the following example of a somewhat simplified version of the\ncumsum\nkernel.\nFor a 1-D input,\ntorch.cumsum\ncalculates the sum of all elements from the beginning of the vector up to and\nincluding each position in the input. A 2-D input is treated as a list of\nvectors, and the cumulative sum is calculated for each vector. Higher\ndimensional inputs follow the same logic--everything is just a list of 1-D\nvectors.  So to implement a cumulative sum, we must take into account two\ndifferent strides: the stride between elements in a vector (result_dim_stride\nand self_dim_stride in the example below) and the stride between each vector\n(strides[0] and strides[1] in the example below).\n// A cumulative sum's output is the same size as the input\nat::Tensor result = at::empty_like(self);\n\nat::TensorIteratorConfig iter_config;\nauto iter = iter_config\n  .check_all_same_dtype(false)\n  .resize_outputs(false)\n  .declare_static_shape(self.sizes(), /*squash_dim=*/dim)\n  .add_output(result)\n  .add_input(self)\n  .build();\n\n// Size of dimension to calculate the cumulative sum across\nint64_t self_dim_size = at::native::ensure_nonempty_size(self, dim);\n\n// These strides indicate number of memory-contiguous elements, not bytes,\n// between each successive element in dimension `dim`.\nauto result_dim_stride = at::native::ensure_nonempty_stride(result, dim);\nauto self_dim_stride = at::native::ensure_nonempty_stride(self, dim);\n\nauto loop = [&](char** data, const int64_t* strides, int64_t n) {\n  // There are `n` individual vectors that span across dimension `dim`, so\n  // `n` is equal to the number of elements in `self` divided by the size of\n  // dimension `dim`.\n\n  // These are the byte strides that separate each vector that spans across\n  // dimension `dim`\n  auto* result_data_bytes = data[0];\n  const auto* self_data_bytes = data[1];\n\n  for (int64_t vector_idx = 0; vector_idx < n; ++vector_idx) {\n\n    // Calculate cumulative sum for each element of the vector\n    auto cumulative_sum = (at::acc_type<float, false>) 0;\n    for (int64_t elem_idx = 0; elem_idx < self_dim_size; ++elem_idx) {\n      const auto* self_data = reinterpret_cast<const float*>(self_data_bytes);\n      auto* result_data = reinterpret_cast<float*>(result_data_bytes);\n      cumulative_sum += self_data[elem_idx * self_dim_stride];\n      result_data[elem_idx * result_dim_stride] = (float)cumulative_sum;\n    }\n\n    // Go to the next vector\n    result_data_bytes += strides[0];\n    self_data_bytes += strides[1];\n  }\n};\n\niter.for_each(loop);\n\n\n\nHelper functions\nThere are many helper functions within PyTorch that can simplify the creation\nand execution of a TensorIterator. We cannot cover all of them in this blog\npost, so you would need to discover them on your own. However, let's discuss\none of the most common ones:\nmake_reduction\nmake_reduction creates a TensorIterator specifically for a reduction\noperation with one input and one output. It handles all of the\nTensorIteratorConfig setup internally, so we don't need to write as much\nboiler-plate code.\nThe following example uses make_reduction to create a TensorIterator which\nis used to calculate the sum reduction of a 2-D input across dimension 1. This\nis equivalent to torch.sum(self, dim=1) in Python. If we didn't use\nmake_reduction, this code would be a bit more complex and more difficult to\nwrite.\nIn this example, as opposed to the previous examples, we do not need to advance\nthe out_data pointer. In fact, the value of strides[0] in this case is 0.\nThe reason for this is that the TensorIterator generated by make_reduction\nwas initialized with is_reduction(true), and when for_each is called,\nsum_reduce_loop is executed once per element of the output tensor. Thus,\nsum_reduce_loop only needs to iterate across the input data, adding each\ninput element to the corresponding reduced output element. The operation is\nthread-safe as well, so the for_each call is free to split up individual\nsum_reduce_loop executions across multiple threads to parallelize the\ncalculation.\nat::Tensor self = at::randn({10, 10, 10});\nint64_t dim = 1;\nbool keepdim = false;\n\n// `make_reduction` will resize result tensor for us, so we\n// can set its size to (0)\nat::Tensor result = at::empty({0}, self.options());\n\nauto iter = at::native::make_reduction(\n  \"sum_reduce\",\n  result,\n  self,\n  dim,\n  keepdim,\n  self.scalar_type());\n\n// Sum reduce data from input into output\nauto sum_reduce_loop = [](char** data, const int64_t* strides, int64_t n) {\n  auto* out_data = data[0];\n  auto* in_data = data[1];\n\n  assert(strides[0] == 0);\n\n  *reinterpret_cast<float*>(out_data) = 0;\n\n  for (int64_t i = 0; i < n; i++) {\n    // assume float data type for this example\n    *reinterpret_cast<float*>(out_data) += *reinterpret_cast<float*>(in_data);\n    in_data += strides[1];\n  }\n};\n\niter.for_each(sum_reduce_loop);\n\n\n\nConclusion\nThis post was a very short introduction to what TensorIterator is actually\ncapable of. If you want to learn more about how it works and what goes into\nthings like collapsing the tensor size for optimizing memory access, a good\nplace to start would be the build() function in\nTensorIterator.cpp.\nAlso have a look at this wiki\npage from\nthe PyTorch team on using TensorIterator.",
      "tags": "",
      "url": "https://labs.quansight.org/blog/2021/04/pytorch-tensoriterator-internals-update/"
    },
    {
      "title": "Accessibility: Who's Responsible?",
      "text": "JupyterLab Accessibility Journey Part 1\nFor the past few months, I've been part of a group of people in the JupyterLab community \nwho've committed to start chipping away at the many accessibility failings of JupyterLab. \nI find this work is critical, fascinating, and a learning experience for everyone involved. \nSo I'm going to document my personal experience and lessons I've learned in a series of blog \nposts. Welcome!\n\n\nBecause this is the first of a series, I want to make sure we start with a good foundation. \nLet me answer some questions you might be having.\nQ: Who are you?\nA: I'm Isabela, a UX/UI designer at Quansight Labs, who \ncares about accessibility and is fortunate to work somewhere where that is a respected concern. \nI also spend time in the Jupyter ecosystem\u2014especially around JupyterLab \u2014though that is not the \nonly open-source community you can find me in. I like to collect gargoyles, my hair is pink, \nand I love the sunflower emoji :sunflower:. It's nice to meet you!\nQ: What is the Jupyter ecosystem and JupyterLab?\nA: Project Jupyter is an organization that produces open-source software \nand open standards. The Jupyter ecosystem is a term used to describe projects that are directly a \npart of or support Project Jupyter. JupyterLab is one of its primary projects and a staple for \nthe day-to-day work of many students, professionals, researchers, and more.\nQ: What is accessibility?\nA: Accessibility is a term used to describe the practice of creating things in a way that \nmakes them usable for people with disabilities.  I\u2019m going to be talking mostly about web accessibility \nsince JupyterLab is a web app. If you're asking why you should care about accessibility, please \ntake a moment to read why it matters \n(hint: there are ethical, legal, and business reasons to care). Inaccessible experiences can \nhave consequences, from people not being able to get information they need to being unable to \npursue whole careers that rigidly require the use of inaccessible software (such as JupyterLab).\nQ: Who is responsible for making things accessible?\nA: I'm so glad you asked! Let's dive into that...\nHow did we get here?\nThe Jupyter ecosystem is full of people who care about accessibility. I know this because I've heard \npeople ask about accessibility in community meetings. I know this because I've read discussions about \naccessibility on Github issues and PRs. I know this because the project has a\nrepository devoted to organizing community accessibility \nefforts. If this is the case, then why hasn't JupyterLab already been made more accessible in the past \nthree years it's been deemed \"ready for users?\" \n(I'm intentionally not mentioning other Jupyter projects to limit this post's scope.)\nBecause for every time accessibility is brought up, I've also experienced a hesitance around taking \naction. Even though I\u2019ve never heard it explicitly said, the way I\u2019ve seen these efforts get lost time and \ntime again has come to mean this in my head: \u201caccessibility is someone else\u2019s problem.\u201d But it can\u2019t always \nbe someone else\u2019s problem; at some point there is a person taking ownership of the work.\nSo who is responsible for making something accessible? Probably not the users, though feedback can be a \nhelpful step in making change. Certainly not the people that already can\u2019t use the tool because it isn\u2019t \naccessible. But I, personally, think anyone who is part of making that tool is responsible for building and \nmaintaining its accessibility. Just as any user experience encompasses the whole of a product, an \naccessible experience does the same. This should be a consideration from within the product, to its \nsupport/documentation, to any other interaction. A comprehensive team who thinks to ask questions like, \n\u201chow would I use this if I could only use my keyboard?\u201d or \u201cwould I be able to get the same information if \nI were colorblind?\u201d are starting to hold themselves and their team accountable. Taking responsibility is \nkey to starting and sustaining change.\nMisconceptions\nHere are a few common concerns I\u2019ve heard when people tell me why they can\u2019t or haven\u2019t worked on \naccessibility. I\u2019m going to paraphrase some replies I've heard when asking about accessibility in many \ndifferent environments (not only JupyterLab) over the years.\nI don\u2019t know anything!\nAnd that\u2019s fine. You don\u2019t have to be an expert! Fortunately, there are already a lot of resources out \non the wide open internet, some even focused on beginners (some of my personal favorites are at \nThe A11y Project and\nMDN). Of course, \nit\u2019s important to remember that learning will mean that you are likely to make mistakes and need to keep \niterating. This isn\u2019t a one-and-done deal. If you do have access to an expert, spending time to build \na foundation means they can help you tackle greater obstacles instead of just giving you the basics.\nI don\u2019t have time for another project!\nAccessibility doesn\u2019t have to be your only focus. JupyterLab sure isn\u2019t the only project I am working on, \nand it won\u2019t be in the near future. Any progress is better than no progress, and several people doing even \na little work can add up faster than you might think. Besides, there\u2019s a good chance you won\u2019t even have \nto go out of your way to start improving accessibility. Start by asking questions about a project you are \nalready working on. Is there a recommended way to design and build a component? Is information represented \nin more than one way? Is everything labeled?  It\u2019s good practice and more sustainable to consider \naccessibility as a regular part of your process instead of a special side project.\nIt\u2019s not a good use of my energy to work on something that only affects a few people!\nIt\u2019s not just a few people. Read what WHO \nand the CDC have \nto say about the number of people with disabilities.\nI don\u2019t want to make abled people\u2019s experience different than it already is!\nDepending on what you are doing, the changes might not be active or noticeable unless assistive technologies \nor accessibility features are being actively used. And in many cases, accessibility features improve the \nexperience for all users and not just those they were designed for (sometimes called the curb cut effect). Even if you aren\u2019t convinced, I\u2019d encourage you to ask yourself why creating the user \nexperience you want and making that experience accessible are mutually exclusive. What are people missing \nout on if they can\u2019t use your product? What are you missing out on if they can\u2019t use your product?\nWhat could responsibility be like?\nWith JupyterLab, it was just a matter of a few people who were willing to say they were tired of waiting and able \nto spend time both learning what needed to be done as well as doing it. Speaking for myself, I did not come in as \nan expert or with undivided obligations or even someone with all the skills to make changes that are needed. I \nthink this is important to note because it seems to me that it could have just as easily been other members of \nthe community in my position given similar circumstances.\nOur first step in taking responsibility was setting up a regular time to meet so we could check-in and help \none another. Then we set reasonable goals and scoped the work: we decided to focus on JupyterLab rather \nthan multiple projects at once, address WCAG 2.1 standards in parts of JupyterLab we were already \nworking on, and follow up on past work that other community members began. This is just the beginning, \nbut I hope it was a helpful peek into the process we are trying out.\nBut wait, there's more!\nDeciding to make accessibility a priority in Jupyter spaces isn't where this work ends. Join me for the next post in this series \nwhere I'll talk about my not-so-subtle panic at the amount of problems to be solved, how to move forwards in spite of panic, and \nthe four experience types in JupyterLab that we must address to be truly accessible.\n\nThis is part of a series of blogs around making JupyterLab more accessible. You can read the \nwhole series here.\nInterested in getting involved? Join our community via the JupyterLab accessibility meetings \nlisted every other week on the Jupyter community calendar.",
      "tags": "Accessibility,JLabA11y,JupyterLab",
      "url": "https://labs.quansight.org/blog/2021/03/accessibility-whos-responsible/"
    },
    {
      "title": "Enhancements to Numba's guvectorize decorator",
      "text": "Starting from Numba 0.53, Numba will ship with an enhanced version of the @guvectorize decorator. Similar to the @vectorize decorator, @guvectorize now has two modes of operation:\n\nEager, or decoration-time compilation and\nLazy, or call-time compilation\n\nBefore, only the eager approach was supported. In this mode, users are required to provide a list of concrete supported types beforehand as its first argument. Now, this list can be omitted if desired and as one calls it, Numba dynamically generates new kernels for previously unsupported types.\n\n\nNumPy Universal Functions\nNumPy has functions called Universal functions or ufuncs. Ufuncs are functions that operate on ndarrays element-by-element. Examples of universal functions are np.log and np.log2, which compute the natural and base-2 logarithms, respectively. Alongside ufuncs, NumPy also has the notion of generalized ufuncs or gufuncs. While the former is limited to element-by-element operations, the latter supports subarray-by-subarray operations.\nCreating new NumPy ufuncs is not an easy process and may require one to write some C code. Numba extends the NumPy mechanism for registering and using (generalized) universal functions with two decorators: @vectorize and @guvectorize. Those decorators allow one to easily create universal functions from Python, leaving the grunt work to Numba.\nFor instance, consider the function guvec, which adds a scalar to every element in an array:\nfrom numba import guvectorize, int64\nimport numpy as np\n\n@guvectorize([(int64[:], int64, int64[:])], '(n),()->(n)')\ndef guvec(x, y, res):\n    for i in range(x.shape[0]):\n        res[i] = x[x > i].sum() + y\n\n>>> x = np.arange(10).reshape(5, 2)\n>>> y = 10\n>>> res = np.zeros_like(x)\n>>> guvec(x, y, res)\n>>> res\narray([[ 4,  3],\n       [ 8,  8],\n       [12, 12],\n       [16, 16],\n       [20, 20]])\n\n\n\nNotice that guvectorize functions don't return their result value. Instead, they have to have the return array passed as an argument.\nPreviously, to use this decorator, one would have to declare the argument types in advance. One can inspect the supported types through the .types property.\n>>> guvec\n<ufunc 'guvec'>\n\n>>> guvec.types\n['ll->l']  # l is a shorthand for int64\n\n\n\nThe commands above also show that guvec is a NumPy ufunc and behaves like it. If one attempts to call it with non-supported argument types, it will fail with the following error message:\n>>> x, y = np.arange(10, dtype=np.float), 10\n>>> res = np.zeros_like(x)\n>>> guvec(x, y, res)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: ufunc 'guvec' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n\n\n\nDynamic compilation\nIn Numba 0.53, one can omit the first argument to build a dynamic generalized universal function. For instance, consider the function dyn_gufunc below:\n@guvectorize('(n),()->(n)')\ndef dyn_guvec(x, y, res):\n    for i in range(x.shape[0]):\n        res[i] = x[x > i] + y\n\n>>> dyn_guvec\n<numba._GUFunc 'dyn_guvec'>\n>>> dyn_guvec.ufunc.types\n[]\n\n\n\nAs one makes calls to dyn_guvec, new kernels will be generated for previously unsupported input types. The following set of interactions will illustrate how dynamic compilation works for a dynamic generalized ufunc:\n>>> x = np.arange(10).reshape(5, 2)\n>>> y = 10\n>>> res = np.zeros_like(x)\n>>> dyn_guvec(x, y, res)\n>>> res\narray([[ 4,  3],\n       [ 8,  8],\n       [12, 12],\n       [16, 16],\n       [20, 20]])\n>>> dyn_guvec.types\n['ll->l']\n\n\n\nIf this was a normal guvectorize function, one would have seen an exception complaining that the gufunc could not handle the given input. One can add additional loops by calling dyn_guvec with new types:\n>>> x_f = np.arange(5, dtype=np.float)\n>>> y_f = 10.0\n>>> res_f = np.zeros_like(x_f)\n>>> dyn_guvec(x_f, y_f, res_f)\n>>> dyn_guvec.types  # shorthand for dyn_guvec.ufunc.types\n['ll->l', 'dd->d']\n\n\n\nCurrent limitations\nIn NumPy, it is fine to omit the output argument when calling a generalized ufunc.\n>>> a = np.arange(10).reshape(5, 2)\n>>> b = 10\n>>> guvec(a, b)\narray([[ 4,  3],\n       [ 8,  8],\n       [12, 12],\n       [16, 16],\n       [20, 20]])\n\n\n\nThe same is not possible in a dynamic ufunc. Numba would have to guess the output type and shape to correctly generate code based on the input and signature.\n>>> dyn_guvec(a, b)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"path/to/numba/np/ufunc/gufunc.py\", line 134, in __call__\n    raise TypeError(msg)\nTypeError: Too few arguments for function 'dyn_guvec'. Note that the pattern `out = gufunc(Arg1, Arg2, ..., ArgN)` is not allowed. Use `gufunc(Arg1, Arg2, ..., ArgN, out) instead.\n\n\n\nNext steps\nIn the future we would like to bring the @guvectorize capabilities closer to the @vectorize ones. For instance, currently it is not possible to call a guvectorize function from a jitted (@jit) function. Some work needs to be done in this direction.\nWe would like to thank the D. E. Shaw group for sponsoring this work. The D. E. Shaw group collaborates with Quansight on numerous open source projects, including Numba, Dask and Project Jupyter.",
      "tags": "Labs,Numba",
      "url": "https://labs.quansight.org/blog/2021/02/enhancements-to-numba-guvectorize-decorator/"
    },
    {
      "title": "Python packaging in 2021 - pain points and bright spots",
      "text": "At Quansight we have a weekly \"Q-share\" session on Fridays where everyone can\nshare/demo things they have worked on, recently learned, or that simply seem\ninteresting to share with their colleagues. This can be about anything, from\nnew utilities to low-level performance, from building inclusive communities\nto how to write better documentation, from UX design to what legal &\naccounting does to support the business. This week I decided to try something\ndifferent: hold a brainstorm on the state of Python packaging today.\nThe ~30 participants were mostly from the PyData world, but not exclusively -\nit included people with backgrounds and preferences ranging from C, C++ and\nFortran to JavaScript, R and DevOps - and with experience as end-users,\npackagers, library authors, and educators. This blog post contains the raw\noutput of the 30-minute brainstorm (only cleaned up for textual issues) and\nmy annotations on it (in italics) which capture some of the discussion during\nthe session and links and context that may be helpful. I think it sketches a\ndecent picture of the main pain points of Python packaging for users and\ndevelopers interacting with the Python data and numerical computing ecosystem.\n\n\nThe main intended audience for this post is maintainers working on\npackaging and distribution tools, and library authors needing to package\ntheir projects. Readers newer to Python packaging may want to start with the\nblog posts linked in the Conclusion section.\nTopic 1: The pip/wheel/PyPI world\n\nPyPI file size limit ... deep learning, CUDA 11\n\nRG: the default file size limit on PyPI is (I believe) 60 MB currently, which is not enough for many libraries with heavy dependencies. You must file an issue to ask for an exemption, which can take quite a while to be resolved because PyPI has only a few volunteers. See, e.g., this issue where MXNet asks for a 1 GB limit. The issue will get significantly worse with CUDA 11, which is several times larger than CUDA 10. Looking at for example PyTorch wheels, which are already 776 MB now, that simply won't fit. When I asked one of the PyPI maintainers recently, the answer was \"interesting - that will be a problem\".\n\nNative dependencies for wheels\ntwo proposed solutions: pynativelib, or spec to interface with outside packages\n\n\n\nRG: what projects do with non-Python dependencies today is simply don't declare the dependency, and use either static linking or put the needed shared libraries into the wheel with name mangling. The pynativelib idea is technically feasible for simple scenarios, but will always be limited because of how pip, wheels and PyPI work. It's not in use today. The more sensible route seems to be to work out how to let packages interact with external package managers, and figure out what's out of scope completely. I'm not aware of significant work happening on this topic.\n\nABI tags are limited: multi-CUDA options, multi-SIMD options\n\nRG: for packages with compiled code, ABI tags in the wheel filename can be used to specify certain properties, like what Python version, operating system or set of base libraries like glibc the wheel is for. Anything that doesn't have an ABI tag cannot be supported, though. Examples of unsupported features are CUDA version and CPU capabilities for SIMD instruction sets. Therefore projects can only make a single choice there for what they upload to PyPI; everything else must be hosted elsewhere.\n\nThe rise of new (and old) architectures: Linux on ARM64, macOS M1, PowerPC, AIX, Z/OS\n\nRG: this is mostly a problem for library authors, who typically rely on public CI for each supported platform. When users clamour for macOS M1 support so they can make optimal use of their shiny new hardware, this puts maintainers in a difficult position - CI support appearing is likely to take many months still.\n\nUniform install instructions for venv's cannot be given.\n\nRG: using virtual environments is good practice, but writing install instructions for them is very difficult because (a) there are too many tools and (b) activating environments is clunky and platform-specific (see, e.g., this section of the Python Packaging User Guide).\n\nSetuptools incorporating distutils with a TBD new build API\n\nRG: distutils will be deprecated in Python 3.10 and removed in Python 3.12. This is likely good news in the long term, but for now it means building packages with nontrivial compiled dependencies is going to be unstable. See for example this issue about numpy.distutils and setuptools interaction.\n\nmanylinux: Alpine Linux, and (maybe) the death of CentOS\nIs musl compatible yet?\n\n\n\nRG: I'm not sure if this is a major isssue. Right now Alpine Linux doesn't work with the manylinux1/2010/2014 wheels on PyPI, because Alpine uses musl libc rather than glibc. The manylinux standard is based on CentOS, which is being discontinued. If that leads to growth of Alpine (perhaps not likely) then we'll be getting more users asking for wheels. Right now all we can say is \"that's unsupported, sorry\".\n\nIsolated builds with pyproject.toml are quite complicated to spec, and still unclear how that will hold up in the real world\n\nRG: Isolated builds are a good idea, but with ABI compatibility constraints one needs to deal with like when building against the NumPy C API, getting pyproject.toml right is complicated. See for example the oldest-supported-numpy meta-package to help with this.\n\nFilling out configuration is tedious\nWhich freaking tool should I use!\nReproducible builds! (practically impossible) \u2013 IPython is reproducible for both .tar.gz anf .zip, but needs custom post-process.\n\nRG: See this blog post from Matthias Bussonnier for more on this topic.\n\nsetup.py is dynamic making downstream packaging/automation much harder (including for conda) \nUse flit \u2013 completely static! \n\n\n\nRG: For pure Python packages Flit is great indeed. For packages with compiled code, we'll be stuck with setup.py for a while longer, I'm afraid. It is what it is - this has gotten better already with pyproject.toml and it's not one of the biggest issues anymore imho.\n\nLack of relocation tools in Windows, i.e., mangling a DLL name and change it on DLLs that depend on it\n\nRG: Probably a gap indeed, not 100% sure. Mach-O Mach-O mangler supports Windows, but a Windows equivalent of auditwheel and delocate seems to be missing.\n\nThis world does not know that the conda world exists.\n\nRG: yeah, kinda. Devs do know conda and conda-forge exist, but as far as I can tell they do see the world as centered around PyPI, wheels and pip, and conda as one of many other package managers (next to Homebrew, Linux distros, Chocolatey, and so on). Which isn't quite right - PyData users now represent over half of all Python users, and the Conda ecosystem has specific capabilities related to that stack that are (and may remain) missing from PyPI/pip.\n\nShipping JupyterLab 3.0 extensions without Node.js is kinda cool.\nRust/Maturin to develop Python packages is nice.\nThe relocation of depending binaries must be done manually in some cases.\n\n\n\nTopic 2: The conda world\n\nConda itself is almost unmaintained\n\nRG: a quick peek at the conda repo indeed shows declining activity. Hopefully the new Conda Community organization will help generate new energy for conda contributions.\n\nMamba is still a little tricky/slow to install\n\nRG: this is definitely true if you start with conda; it can take minutes for conda to do the solve to install mamba and its dependencies from conda-forge. With the recently added Mambaforge installer that problem should be mostly solved now.\n\nNo TensorFlow \nDoes conda-forge count?\n\n\n\nRG: this is actually no longer true, the TensorFlow package in defaults is reasonably but not completely up to date (2.3.0 CPU and GPU packages at the moment, missing 2.3.1-2.4.1). In general it's a problem indeed that important packages like TensorFlow and PyTorch in defaults may be one or even more versions behind.\n\nConda-forge is growing too large\nIn which respect?\n\n\n\nRG: As discussed in this blog post on conda performance, the more packages there are in a channel, the slower the conda solves get. Given the ever-growing size of conda-forge, conda is getting slower and slower when used with conda-forge.\n\nWhy oh why are defaults and conda-forge still not using a 100% compatible toolchain??\n\nRG: This would be so helpful. Not being able to mix defaults and conda-forge, and users doing it anyway because they need packages from both, is a long-standing issue.\n\nConda solves in CI often get really bogged down\nAlso conda is never the default in CI so you need to do quite a bit of manipulation on some platforms\n\n\nConfusion in the community over the new Anaconda ToS\n\nRG: Anaconda has done the right thing here imho: make large institutional users pay if they use the defaults channel. Packaging is hard and labour-intensive, and having a sustainable revenue stream to offset the cost of packaging difficult libraries, maintaining core tooling like conda-build, and serving packages to millions of users is important. Conda-forge and all other channels are still completely free. Unfortunately Anaconda communicated that very poorly, which led to unnecessary confusion. See this blog posts from the conda-forge team for more.\n\nThere's no way to distinguish packages that provide the same binaries, e.g., turbojpeg vs. libjpeg, both package libjpeg\nNeed to separate metadata (especially dependencies) from packages.\n\nRG: this to me seems like a much smaller issue than with PyPI. The comment was about the ability to patch metadata of published conda packages more easily.\n\nR community seems slow to adopt it\n\nRG: I've heard from R users before that conda felt a bit \"foreign\". In the discussion it was suggested that part of that is that conda is Python-based which can still bubble up to the surface on occasion. And that Mamba, which is written in C++, may find a more positive reception in the R community.\n\nEnd-to-end security?\n\nRG: Purism and this blog post from its devs about security in the software supply chain were mentioned.\nTopic 3: Other\n\nThe Conda and PyPA worlds still aren't talking to each other much.\n\nRG: It's great to see that both the PyPA team and the Conda community are getting better organized. There's now more funding for work on PyPI, Warehouse and Pip; the teams working on it have grown, there's good project management, roadmapping and even Pip user experience research going on. And the Conda community has set up a new organization to align on standards, incubate promising new projects. The next step will hopefully be more productive interactions between those two somewhat separate groups.\n\npip and conda interaction still not great\nI'd like a way to have \"safe\" (e.g., pure Python) packages so you can confidently mix them. +100\nThe pip dependency resolver is not great yet and it also ends up with conflicts with conda.\n\n\n\nRG: True. For doing serious scientific computing, data science and ML type work, you often need the combination, using conda for the core libraries, and filling in the gaps with pip for packages that are not packaged for conda or installing things from source.\nTopic 4: There's always good news too\n\nThe death of easy_install!!!\nMambaforge\nmacos-arm64 support in conda-forge\n\nRG: It'll be a while before we get wheels for core PyData packages, but conda-forge already has support, see this blog post.\n\nThe new conda community umbrella org\nPip and PyPI are now well-managed and have a nontrivial amount of funding\nPip has a solver :) ; the solver is a \"backtracking solver\" because metadata on PyPI is incomplete :(\n\nBonus topic: A puzzle\nA challenge with a prize for who posts the most concise correct solution by\ntomorrow (note, you do need a very fast internet connection for this): create\na concise recipe for installing the latest versions of NumPy, TensorFlow,\nPyTorch, CuPy, Dask, JAX and MXNet in a single environment.\n\ud83d\udc46\ud83c\udffc This is a trick question I am sure \nRG: everyone had till the end of the next day to submit a solution. It turns out to be not so simple, the winning entry (with six out of seven packages, JAX is missing) came from Chris Ostrouchov - our local Nix evangelist:\n$ nix-shell -I nixpkgs=https://github.com/nixos/nixpkgs/archive/986cf21c45b798c97f2424e17b819af3ecf8083e.tar.gz\n -p python3Packages.numpy\n python3Packages.tensorflow python3Packages.pytorch python3Packages.cupy python3Packages.dask python3Packages.mxnet\n\n\n\nMy solution:\n# To create an environment with the most recent version of all mainstream\n# Python array/tensor libraries installed:\n$ conda create -n many-libs python=3.7\n$ conda activate many-libs\n$ conda install cudatoolkit=10.2\n$ pip install numpy torch jax jaxlib tensorflow mxnet cupy-cuda102 dask toolz sparse  # you may need to turn off the resolver if you're on pip >= 20.3.1\n\n# Conda doesn't manage to find a winning combination here; pip has a hard time\n# too and probably not all constraints are satisfied, but nothing crashes\n# and basic tests work as they are supposed to.\n\n\n\nConclusion\nA lot of good things are happening in various places in the Python packaging world. We're in a better situation than several years ago. As the brainstorm content above shows there are challenges as well, but that has always been the case. If I could have just one wish, I'd wish for good program management across and coordination between the PyPA and Conda communities.\nThis is my blog post though, so I get a few more wishes:\n\nMerging Conda and Mamba so they share a single installer and UX with two modes ('fast' and 'stable/legacy').\nStrictly enforced metadata correctness and manylinux compliance for new PyPI uploads. The stricter the better, see packaging-problems #264.\nGPU hardware for conda-forge so it can test its GPU packages in CI.\nConsolidate the zoo of virtual environment tools (venv, virtualenv, poetry, pipenv, pyenv, pyflow - probably there are even more) so we can actually recommend one to users of our projects.\nNot having to think about problems around Fortran on Windows ever again.\n\nWhile I'm at it, let me also link to some of the most informative blog posts on the topic I know of and enjoyed reading over the years:\n\nTarek Ziad\u00e9 - The fate of Distutils - Pycon Summit + Packaging Sprint detailed report (2010)\nTravis Oliphant - Why I promote conda (2013)\nJake VanderPlas - Conda: Myths and Misconceptions (2016)\nWes McKinney - conda-forge and PyData's CentOS moment (2016)\nDonald Stufft - Powering the Python Package Index (2016)\nPauli Virtanen - Building Python wheels with Fortran for Windows (2017)\nUwe Korn - How we build Apache Arrow's manylinux wheels (2019)\nPradyun Gedam - Testing the next-gen pip dependency resolver (2020)\nSumana Harihareswara - Releasing pip 20.3, featuring new dependency resolver (2020)\n\nI hope this sketches a useful picture of where we are today. If we missed any major issues (I'm sure we did), I'd love to hear them!",
      "tags": "Conda,conda-forge,CUDA,manylinux,packaging,pip,PyData,PyPI,Python,setuptools",
      "url": "https://labs.quansight.org/blog/2021/01/python-packaging-brainstorm/"
    },
    {
      "title": "Making SciPy's Image Interpolation Consistent and Well Documented",
      "text": "SciPy n-dimensional Image Processing\nSciPy's ndimage module provides a powerful set of general, n-dimensional image processing operations, categorized into areas such as filtering, interpolation and morphology. Traditional image processing deals with 2D arrays of pixels, possibly with an additional array dimension of size 3 or 4 to represent color channel and transparency information. However, there are many scientific applications where we may want to work with more general arrays such as the 3D volumetric images produced by medical imaging methods like computed tomography (CT) or magnetic resonance imaging (MRI) or biological imaging approaches such as light sheet microscopy. Aside from spatial axes, such data may have additional axes representing other quantities such as time, color, spectral frequency or different contrasts. Functions in ndimage have been implemented in a general n-dimensional manner so that they can be applied across 2D, 3D or more dimensions. A more detailed overview of the module is available in the\nSciPy ndimage tutorial. SciPy's image functions are also used by downstream libraries such as scikit-image to implement higher-level algorithms for things like image restoration, segmentation and registration.\n\n\nInterpolation in scipy.ndimage\nIn this blog post we will focus specifically on recent improvements that have been made to SciPy's interpolation functions. The image interpolation functions provided by scipy.ndimage include basic operations like shifts, rotations and resizing as well as more general coordinate transformations like affine transforms or warping. When the value of a coordinate location in the output image is to be determined, we must use information from nearby pixels in the input image to determine an appropriate value at this location. The accuracy of the interpolation depends upon the \"order\" parameter of these functions, which provides a tradeoff between higher speed for order 0 (nearest-neighbor) and order 1 (linear interpolation) vs. higher accuracy (order 2-5 spline interpolation). In general, the interpolation operation will involve a weighted combination of (order + 1) ** ndim input image values to determine each value in the output image (here ndim corresponds to the spatial axes of the array). Readers interested in a technical overview of image interpolation methods are referred to the following review of Linear Methods for Image Interpolation and the Theory and Practice of Image B-Spline Interpolation.\nAside from the choice of the interpolation order, when a coordinate outside the boundary of the original image is requested, we must decide how to determine a value for that coordinate. ndimage supports many such boundary modes, as selected via the mode parameter of each function. These include, for example, a periodic boundary ('wrap'), mirrored boundary ('mirror'), constant boundary, etc. Unfortunately the behavior of a subset of these modes either had bugs or did not operate in the manner a user might expect due to ambiguity of coordinate conventions used. Specifically, not all modes handled boundaries accurately for spline interpolation orders >= 2. Overall these various rough edges had resulted in more than a dozen issues reported over the past several years on SciPy's GitHub repository.\nA recent NumFOCUS small development grant awarded to the SciPy developers allowed a dedicated effort to fix existing bugs in boundary handling and improve the documentation of the behavior of these modes. The code underlying ndimage is in C for efficiency, but this code base is somewhat complicated due to its general n-dimensional nature and flexible data type support. This complexity coupled with the lack of a dedicated maintainer for the ndimage module over the past 15 years, had led to a number of these issues being long unaddressed. The work carried out under this proposal resulted in closing more than a dozen such long-standing SciPy issues.\nInterpolation order example\n\n\nAn illustration of interpolation order is given for a synthetic brain MRI (only one axial slice of the 3D brain is displayed). At the left is a simulated volume at 2 mm resolution. We then use scipy.ndimage.zoom to upsample the volume by a factor fo 2.0 along all axes to give an interpolated image where each voxel is of size 1 x 1 x 1 mm.\n\nHere we can see that nearest neighbor (order=0) gives a blocky appearance, while order=1 (trilinear interpolation) also has some loss of detail relative to order 3 spline interoplation. No interpolation method can perfectly recover the true 1 mm simulated image shown on the right, indicating that one cannot rely on interpolation alone as a method to reduce MRI exam time without a sacrifice in image quality.\nFurther details of the digital MRI phantom use for this example have been described in the following publications: 1, 2.\nCoordinate conventions\nAlthough it was not previously well documented, scipy.ndimage uses the convention that pixel centers fall on integer coordinate indices between 0 and shape[axis] - 1 along each axis. Two potential coordinate conventions are diagrammed in 1D in the figure below.\n\nAt the top, the samples are treated as isolated points, whereas in the bottom panel points are considered as the centers of regulary spaced grid elements (i.e. pixels in 2D, voxels in 3D). The top interpolation has historically been used for ndimage's wrap (aka periodic) boundary mode. This leads to surprising behavior, because the period for n samples is n - 1 with the first and last sample exactly overlapping. In SciPy 1.6, a new mode, grid-wrap was introduced which uses the grid interpretation of the lower panel so that all n samples are replicated.\nSimilarly the scipy.ndimage.zoom function has a new grid_mode option that, when set to True, will cause resizing based on the full grid extent. With grid_mode=True, zoom's behavior becomes consistent with similar functions in scikit-image (skimage.transform.resize) and OpenCV (cv2.resize).\nNew documentation of boundary handling behavior\nDocumentation of the boundary modes has been improved, adding illustrations for the behavior of all modes and cross-linking to these figures from each individual function's docstring. Additionally, the coordinate origin being at the center of a pixel is more clearly documented and the difference between a pixel vs. point data model is better described.\nDownstream Impact\nThis work has enabled resolving bugs and simplifying resize code in scikit-image. GPU-based implementations of the changes implemented for SciPy 1.6 have also recently been merged into CuPy, and will be available in the upcoming CuPy 9.0.0 release.\nAcknowledgements\nThe work described in this blog post was enabled by a NumFOCUS Small Development Grant awarded in 2020.",
      "tags": "Labs,Open-Source,Python,SciPy",
      "url": "https://labs.quansight.org/blog/2021/01/scipy-ndimage-interpolation/"
    },
    {
      "title": "Welcoming Tania Allard as Quansight Labs co-director",
      "text": "Today I'm incredibly excited to welcome Tania Allard to Quansight as\nCo-Director of Quansight Labs. Tania (GitHub,\nTwitter, personal\nsite) is a well-known and prolific PyData\ncommunity member. In the past few years she has been involved as a conference\norganizer (JupyterCon, SciPy, PyJamas, PyCon UK, PyCon LatAm, JuliaCon and\nmore), as a community builder (PyLadies, NumFOCUS, RForwards), as a\ncontributor to Matplotlib and Jupyter, and as a regular speaker and mentor.\nShe also brings relevant experience in both industry and academia - she joins\nus from Microsoft where she was a senior developer advocate, and has a PhD in\ncomputational modelling.\n\n\nTania and I will be working closely together and will jointly lead Labs. This\nis an important step in the growth of Labs - in the past two years it grew\nfrom an idea to an open source lab with over 30 members who\ncontribute to key projects in the PyData ecosystem and beyond. Now is the\nright time to strengthen the leadership capability of Labs. I already found\nTania and I are both very interested in and on the same wavelength about\ncommunity building. That's also how we met in person for the first time,\nTania and I were speaking back to back in the Open Source Communities track\nat SciPy'19. I'm looking forward to many interesting conversations about\neverything from governance in open source projects to diversity, equity and\ninclusion (DEI) to funding. We have complementary technological strengths and\ninterests. Tania will be taking the lead on DevOps, visualization and IDEs\nwhile I will focus on numerical topics.\nWhere Labs is today\nLabs has a clear mission and vision, and a model for open source contribution, funding\nand employment that I think is both unique and sustainable (see\nAbout for more). I'm particularly proud of the team we've built,\nwith both established maintainers and new technical talent, as well as a\nstrong focus on roles other than software development and on DEI. Our team\nincludes people from 12 countries in 4 continents, and we contribute to many\nof the projects at the core of the PyData ecosystem.\n\nI'm thinking about Labs projects as falling into two fairly distinct\ncategories: contributions to established and widely used projects, and\ninnovation initiatives. In the former category, I would particularly like to\nmention our contributions in 2020 to NumPy, Spyder, JupyterLab and SciPy.\nContributions included a healthy mix of maintenance and innovation. For\nNumPy, Spyder and JupyterLab we have 5 maintainers or contributors each, and\nfunding that's continuing in 2021 - from numpy.f2py and documentation\nimprovements to real-time collaboration in JupyterLab and completing Spyder 5\nwith a new plugin architecture.\n\nOur largest innovation initiative is the\nConsortium for Python Data API Standards, which we\ncreated in mid-2020. This consortium brings together many array, tensor and\ndataframe library maintainers and has the ambitious aim to tackle the problem\nof fragmentation of Python array and dataframe libraries. We published our\nfirst RFC for an array API standard in November, and there's more to come in\n2021.\nOther significant innovation activities this year included pushing Ibis and\nPyData Sparse forward, two projects with potential that fill holes in the\ndataframe and array library landscape.\nWhat's next?\nNow that we have an established model and a set of major projects that are\nwell-defined and continuing in 2021, we can focus more on our diverse set of\nsmaller activities in addition to growing the team and organization. New\nprojects in the pipeline will be announced in the coming weeks and months, \nbe sure to watch this space. Tania will also inject her own ideas\nand energy, and I'm looking forward to how that will help shape and improve\nthe Labs' roadmap.\nMany people in Labs contribute to or lead projects we haven't highlighted a\nlot yet - from SymPy to Numba, Holoviz to IPython, stdlib.js to Zarr. The\nfreedom to do what's needed or seems worthwhile on such projects is one of\nthe main reasons for people to join Labs. I'd like us to get a bit better at\nsetting concrete goals for each of those activities, which will naturally be\na conversation between Labs leadership and Board, and the individual\ncontributors to each project. Furthermore we are developing some new\ncompetencies that apply across projects. A great example is accessibility -\nwe are starting to gain more expertise and are driving activities to improve\naccessibility in Jupyter and Spyder, as well as on websites of other\nprojects.\nLooking forward to what 2021 will bring, and to working with Tania on growing Labs!",
      "tags": "Labs",
      "url": "https://labs.quansight.org/blog/2021/01/welcoming-tania-allard-labs-codirector/"
    },
    {
      "title": "Develop a JupyterLab Winter Theme",
      "text": "JupyterLab 3.0 is about to be released and provides many \nimprovements to the extension system. Theming is a way to extend JupyterLab and \nbenefits from those improvements.\nWhile theming is often disregarded as a purely cosmetic endeavour, it can greatly\nimprove software. Theming can be great help for accessibility, and the Jupyter team\npays attention to making the default appearance accessibility-aware by using\nsufficient contrast.  For users with a high visual acuity you may also choose \nto increase the information density.\nTheming can also be a great way to improve communication by increasing or\ndecreasing emphasis of the user interface, which can be of use for teaching or\npresenting. Theming may also help with security, for example, by having a clear\ndistinction between staging and production.\nFinally Theming can be a great way to express oneself, for example, by using\na branded version of software that fits well into a context, or expressing one's artistic\npreferences or opinions. \nIn the following blog post, we will show you step-by-step how you can\ndevelop a custom theme for JupyterLab, distribute it, and take the example of the\njupyterlab-theme-winter theme we release today to celebrate the end of 2020.\n\n\nJupyterLab and Themes\nJupyterLab customisation can be done via what are called Extensions.  All\nbehaviors and user interface elements of JupyterLab can be changed by providing\nextensions; this is true for elements that are added to JupyterLab, but also for\nthe core components. A Theme is one of those extensions. The default light and dark\nthemes are always good examples to look at if you want to understand how to build\na theme.\nGenerally all information about how extensions work in JupyterLab is applicable\nto a theme, though there are a number of optional steps and behaviors that are not\nnecessary for themes, and a few configurations that are needed for themes.\nA lot of boilerplate can also be extracted, which makes creating most themes \nsimpler that full-fledged extensions.\nLet's first see what we are trying to accomplish in a screenshot of the \"winter 2020\" theme.\nYou will have the option to choose this theme in the dropdown menu of JupyterLab:\n\nTo do so we'll need to:\n\nInstall JupyterLab\nCreate a new theme extension\nInstall this extension\nSwitch to our new theme\nProgressively change the appearance of our JupyterLab components until we get\n  the desired outcome\n\nOptionally, once you are happy with the result, or would like to get feedback from the\ncommunity, you can publish the theme so that people can complain suggest improvements\nand contribute. \nInstalling JupyterLab\nTo get started you do not need a development version of JupyterLab.  This can\nbe achieved with:\n$ pip install --pre jupyterlab==3.0rc14 jupyter_packaging cookiecutter\n\n\n\nWhile we are at it we will need to install nodejs. Because nodejs is not a\nPython package you will need to use conda or another package manager.\n$ conda install nodejs\n\n\n\nCreating a Theme Extension\nNow the hard part: you have an idea, you need a name. Naming is one of the\nhardest things in development, along with caching results and off-by-one errors,\nbut it is critical to adoption and discoverability.\nNow that you have your perfect name, create your project. We suggest using\nthe JupyterLab cookiecutter to create the initial boilerplate, and, as a\nprerequisite, install the jupyter_packaging library needed to develop extensions.\nWe won't actually create the boilerplate until farther below in this post.\n$ pip install jupyter_packaging cookiecutter\n\n\n\nNormally we would use theme-cookiecutter but it is not\nyet updated for JupyterLab 3, so we fall back to the more generic\nextension-cookiecutter-ts \nand will highlight the specifics of a Theme extension compared to a standard one.\nRun cookiecutter and provide some information about your extension:\n$ cookiecutter https://github.com/jupyterlab/extension-cookiecutter-ts --checkout 3.0\nauthor_name []: My Name\npython_name [myextension]: jupyterlab-theme-winter\nlabextension_name [myextension]: @my-repo/jupyterlab-theme-winter\nproject_short_description [A JupyterLab theme extension.]: A winter theme for jupyterlab\nhas_server_extension [n]: \nhas_binder [n]: \nrepository [https://github.com/my_name/myextension]: \n\n\n\nThis has created the extension jupyterlab-theme-winter.  We can go to the directory and\nimmediately turn it into a git repository.\n$ cd jupyterlab-theme-winter\n$ git init\n$ git add .\n$ git commit -am 'initial commit'\n\n\n\nBuilding and Installing the Extension\nJupyterLab 3 has focused on make extension authors' lives easier, and it has done a great\njob of it. As a developer, you just need to run a single command to be up and running\nwith your extension. You only need to do this once.\njupyter labextension develop --overwrite\n\n\n\nWe are now ready for the development session. As we want to iterate fast, we will launch\na watch process that continuously compiles your extension on each of your changes\nand will make it available in the JupyterLab frontend so that you can see the changes immediately.\njlpm watch\n\n\n\nRemember, we have not yet created our boilerplate from the theme cookiecutter, so we need to\nmake sure we turn the code into a theme extension with the following two actions.\nFirst, replace the content of src/index.ts with the following content\nimport {\n  JupyterFrontEnd,\n  JupyterFrontEndPlugin\n} from '@jupyterlab/application';\n\nimport { IThemeManager } from '@jupyterlab/apputils';\n\n/**\n * Initialization data for the @quansight-labs/jupyterlab-theme-winter extension.\n */\nconst extension: JupyterFrontEndPlugin<void> = {\n  id: '@quansight-labs/jupyterlab-theme-winter',\n  requires: [IThemeManager],\n  autoStart: true,\n  activate: (app: JupyterFrontEnd, manager: IThemeManager) => {\n    console.log('JupyterLab extension @quansight-labs/jupyterlab-theme-winter is activated!');\n    const style = '@quansight-labs/jupyterlab-theme-winter/index.css';\n    manager.register({\n      name: 'JupyterLab Winter',\n      isLight: true,\n      load: () => manager.loadCSS(style),\n      unload: () => Promise.resolve(undefined)\n    });\n  }\n};\n\nexport default extension;\n\n\n\nThen in the package.json, add \"jupyterlab-theme\" to the list of keywords and ensure that the jupyterlab stanza looks like this.\n  \"jupyterlab\": { \n    \"extension\": true,\n    \"themePath\": \"style/index.css\",\n    \"outputDir\": \"jupyterlab_theme_winter/labextension\"\n  }\n\n\n\nIn a separate terminal you can now start JupyterLab in watch mode.\njupyter lab --watch\n\n\n\nNow you will see the theme available from the Settings menu.\nYou can switch themes; but as you will see; the current theme is identical to\nthe light-theme. Now it's time to modify some values in the styles/variables.css file\nwith a valid design.\nDesign Considerations\nIn the words of Jurassic Park\u2019s Dr. Ian Malcolm, \u201cYour scientists were so preoccupied with whether they could, \nthey didn't stop to think if they should.\u201d And now that you can modify JupyterLab\u2019s theme to your heart\u2019s content, \nhere is some design advice to help keep you from accidentally creating a theme that visually destroys your \nworkspace like a rampaging tyrannosaurus rex.\nJupyterLab design system\nWhen making a theme, it\u2019s likely you\u2019ll want to change things that already exist in JupyterLab. Much of the UI relies \non relevant CSS variables with naming conventions (--jp-ui-font-color3 or --jp-elevation-z0) to help you find \nwhat you need. Think of the system like this:--jp-region-contenttype-unit1\n\nThe --jp prefix is a constant. \nThe middle holds various details like what type of UI element it is for, if it is standard (no tag) or if \nit has a specific use. \nregion is for variables that are only be used in a certain area of the UI .\nThe content type is something like font or border. It describes the variable based on its use. \nThe unit is the smallest unit of the variable like a color, shadow, or spacing. It is labeled by a number when \nthere is more than one of that unit; 0s are almost never used, and 1s are some of the most frequently used. \nWhenever a variable name does not have one of these sections, that means it doesn\u2019t have specific rules about \nits use in that area.\n\nCommon labels you might want to know:\n\nlayout is used for large areas of the interface, especially backgrounds. \ninverse  indicates the opposite color scale of the rest of the UI. For example in\n  light mode, inverse 0 and 1 are dark instead of light.\nelevation and z are for shadows. While these might not be the main things\n  you want to change, their unfamiliar names might make them harder to find.\nfont is for variables tied to text. These have forms for color, size, and spacing of text.\n\nThere are also variables specific to different types of text and display\nmodes, so you could have a theme that looks like the standard light\ntheme until you enter Presentation mode.\nColor\nLess is more. Choosing a color palette of three or fewer hues can be easier to manage and make the whole interface \nmore cohesive since those colors will likely be repeated across the UI. Try it out; it might be surprising how just \nchanging a few color variables can create a very different JupyterLab.\nA color's value\u2014or how light or dark it is\u2014also determines contrast. Contrast is key to legibility and creating an \nexperience that includes low-vision users. WCAG color contrast guidelines \napply to text and graphic interactive \nelements. There are many tools available, including downloadable contrast checkers \nand web app versions.\nText\nJupyterLab is full of text, so this can be a place for major changes with very little code. You can easily change font, \ncolor, spacing, and size. It's a good idea not have text below 10pt in size, or smaller than the default \n--jp-ui-font-size0 \n(and it's an accessibility recommendation).\nIcons\nJupyterLab's icons live in the packages directory \nand are part of or based on Material Icons. \nIf you want to change or add icons and keep them matching, finding one from this system will fit best. The Material Design system \nalso points out some of their icon design principles which are good to follow if you need to make custom icons that \nmatch with the rest. Use SVGs, not PNGs, and remember to give it a tooltip. \nMost of all, make sure to give it an ARIA label (like this recommendation).\nModifying the Theme Variables\nAfter each modification the watch process will build the extension for you. \nNo need to stop and restart JupyterLab server; simply refresh the page.\nNow we are going to modify some values in the file variables.css in our\nproject. This file controls many of the colors of JupyterLab, and is a nice place\nto start to change the overall color scheme before doing more detailed\ncustomisation.\nWe'll try to update the current theme from orange and blue to more blue-ish tones, \nwhich tend to remind us of the holiday\nseason.  Afterward, in the diff, see how we changed some of the colors:\ndiff --git a/style/variables.css b/style/variables.css\nindex f8c738f..9f39f14 100644\n--- a/style/variables.css\n+++ b/style/variables.css\n@@ -87,9 +87,9 @@ all of MD as it is not optimized for dense, information rich UIs.\n    */\n\n   --jp-border-width: 1px;\n-  --jp-border-color0: var(--md-grey-400);\n-  --jp-border-color1: var(--md-grey-400);\n-  --jp-border-color2: var(--md-grey-300);\n+  --jp-border-color0: #168EA8;\n+  --jp-border-color1: #168EA8;\n+  --jp-border-color2: #93D3E1;\n   --jp-border-color3: var(--md-grey-200);\n   --jp-border-radius: 2px;\n\n@@ -118,10 +118,10 @@ all of MD as it is not optimized for dense, information rich UIs.\n\n\n\nWe'd like the \"running notebook\" dot in the filesystem browser to be a\nsnowman instead of a blue dot. Using the browser inspector, we can look at the CSS\ndoing this and override it in my theme:\n@@ -368,0 +369,26 @@                                                                  \n+\n+.jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon:before {\n+    content: '\\2603'; /* snowman */\n+    font-size: 10px;\n+    position: absolute;\n+    left: -8px;\n+}\n\n\n\nWe can also add backgrounds to many of the panels.  By adding a file snowflakepatterns.svg to our style directory,\nwe can now refer to it from our file and add the following CSS to include snowflakes in the background of\nour directory listing.\n/* DirListing */\n\nbody[data-jp-theme-name=\"JupyterLab Winter\"] .jp-DirListing-content {\n  background-image: url('./snowflakepattern.svg');\n }\n\n\n\nAwesome!\nHere is the final result, jupyterLab-theme-winter theme provided by Quansight.  Feel\nfree to modify it, and please suggest some themes you might like and share\nin the comments. For example we'd love to see a \"summer 2020 theme\" for our Southern Hemisphere friends.\nWe will dive into how to distribute your themes and make them high-quality in a later blog post.\n\nAnd as a bonus, a Christmas theme with more green-ish color and some lights shining\nat the bottom of your notebooks!\n\n\nThis is part of a series of Jupyter tutorials. Find more tutorials here.",
      "tags": "Jupyter,JupyterLab,JupyterTutorials,Labs,Theme",
      "url": "https://labs.quansight.org/blog/2020/12/jupyterlab-winter-theme/"
    },
    {
      "title": "A second CZI grant for NumPy and OpenBLAS",
      "text": "I am happy to announce that NumPy and OpenBLAS have once again been awarded a\ngrant from the Chan Zuckerberg Initiative through\nCycle 3 of the Essential Open Source Software for Science (EOSS) program.\nThis new grant totaling $140,000 will fund part of our efforts to improve\nusability and sustainability in both projects and is excellent news for the\nscientific computing community, which will certainly benefit from this work\ndownstream.\n\n\nWhy this is important for NumPy and OpenBLAS\nThe Essential Open Source Software for Science program is visionary and unique.\nThe activities supported by this funding are not restricted to new technical\nfeatures. Rather, it supports maintenance, community engagement, and other types\nof work that often go unrewarded in open source projects. And although\nits primary goal is to fund the computational foundations of biology, the benefits of the work done under the Chan\nZuckerberg Initiative's program extend well beyond specific scientific areas.\nThis is the second grant from CZI that NumPy and OpenBLAS have received.\nThis past year, the funding has enabled several high-impact improvements to the\nNumPy and OpenBLAS communities. For NumPy, this has meant\na new website, the establishment of the\nDocumentation and Website Teams, and\nmentorship and onboarding activities to grow and diversify the maintainers for\nthe project. For OpenBLAS, the focus has been on technical improvements (thread\nsafety, support for the 512-bit Advanced Vector Extensions (AVX-512) code, and\nthread-local storage). The impacts of these developments extend throughout the\nscientific Python ecosystem and beyond, and the increased activity has also\nattracted several new contributors.\nOur plans for 2021 include continuing with our efforts in documentation and\ncommunity building for NumPy and also working to modernize its integration with\nFortran tools via F2py, in addition to dedicating resources towards ensuring the\nsustainability of both NumPy and OpenBLAS.\nYou can read the full proposal here.\nDocumentation\nThe establishment of the NumPy documentation team has resulted not only in a\nrestructuring of the documentation\nand a new repository for educational content\nbut has also created the opportunity for many new contributors to join the\nproject. Part of this new grant will enable us to continue that work, resulting\nin a set of high-quality documents around NumPy while at the same time\ndiversifying opportunities for community contributions.\nF2py\nAs a part of NumPy, the F2py tool is used by numerous projects in the Python\nscientific ecosystem, including SciPy itself, to automate the compilation of\nFortran code into Python modules, making it an essential piece of the\ninfrastructure for these projects.\nHowever, Fortran has evolved over the years. Although considered by some as a\nniche language, it is still very popular\namong scientific computing and high-performance developers and is in\nactive development. Due to a lack of dedicated\ndevelopment work in the past years, F2py does not support many current\nfeatures of the language. In\naddition, because of limited resources dedicated to testing and maintenance,\nregressions have been introduced in the F2py code that may result in a\npropagation of bugs for many other projects. With this grant, we plan to address\nthe maintenance backlog and extend the functionality of F2py and bring back\nPearu Peterson, the original author for the F2py tool, to its development.\nCurrently, F2py supports wrapping Fortran 77 and Fortran 90 code, except for\nderived types introduced in Fortran 90. To support modern use cases, we plan to\nadd support for derived types and work towards full Fortran 2003 compatibility.\nWhile there have been\nattempts at resolving these issues in the past,\nthey were never integrated into NumPy, and at the moment, this would require a\nlarger refactor of F2py, which we propose to do. We are also writing a new F2py\nuser guide with comprehensive examples and feature descriptions.\nOpenBLAS\nOpenBLAS is a highly optimized library for linear algebra computations. It\nprovides accelerated versions of BLAS (Basic Linear Algebra Subroutines and\nLAPACK (Linear Algebra Package), which are the two universal interfaces to\nperform linear algebra with. It is a key dependency of NumPy and SciPy, as well\nas of R and Julia. While most users will not realize they are using it, it is\nessential for the three leading scientific open-source programming ecosystems.\nThis grant will enable the OpenBLAS maintainers to continue addressing critical\ntechnical issues. These include issues with Thread Local Storage, performance at\ncertain problem sizes, and the implementation of BLAS extensions already\nprovided by similar libraries.\nIn practice\nAs part of the third cycle of the \u200bEssential Open Source Software for Science\n(EOSS)\u200b program, CZI awarded $3 million for 17 new grants. These awards bring the\ntotal number of funded proposals to 67 projects and the EOSS program's total\ncommitment to funding scientific open source to $11.8 million. View the\n\u200bfull list of grantees\u200b.\nAs for our project, the funds will be again managed by NumFOCUS as a fiscal\nsponsor. Of the total value (removing the overhead charges), about two-thirds\nwill go to NumPy. This will support me, as the PI of this proposal, and Pearu\nPeterson part-time for a year, as a subcontract with Quansight. The remaining\nfunds will support Martin Kroeker part-time for a year.\nTo work!\nThe actual work begins on January 1st, and I'm looking forward to this. Moving\nfrom being a part of the grant project for NumPy in 2020 to being the PI in 2021\nis exciting - I can't wait to see what we do next!",
      "tags": "CZI,funding,grant,NumPy,OpenBLAS",
      "url": "https://labs.quansight.org/blog/2020/11/a-second-czi-grant-for-numpy-and-openblas/"
    },
    {
      "title": "Introduction to Design in Open Source",
      "text": "This blog post is a conversation. Portions lead by Tim George are marked with\nTG, and those lead by Isabela Presedo-Floyd are marked with IPF.\nTG: When I speak with other designers, one common theme I see concerning why\nthey chose this career path is they want to make a difference in the world. We\ndesign because we imagine a better world and we want to help make it real. Part\nof the reason we design as a career is we're unable to go through life without\ndesigning; we're always thinking about how things are and how they could be\nbetter. This ethos also exists in many open-source communities. It seems like it\nought to be an ideal match.\nSo what's the disconnect? I'm still exploring that myself, but after a few years\nin open source I want to share my observations, experiences, and hope for a\nstronger collaboration between design and development. I don't think I have a\ncomplete solution, and some days I'm not even sure I grasp the entire problem.\nWhat I hope is to say that which often goes unsaid in these spaces: design and\ndevelopment skills in open source coexist precariously.\n\n\nWhy design might be missing in open source\nTG: If you're looking for a quick and singular answer, it might be best\nsummed up by a quote from Marshal McLuhan in his book The Medium is the Message:\n\"We shape our tools, then our tools shape us.\" GitHub has become the place where\nmany open-source projects exist. GitHub (I don't want to suggest that it is the\nonly place open source exists, just that it's the only place where I've been\ninvolved with open source) is also designed first and foremost with code\ncontributions in mind. The ripple effect is that the people and the project will\nsimilarly center around code and it will take effort to use the tool in a way it\nis not designed to support. While it has a number of features that support\ndesign input, it doesn't integrate directly with design tools, and design\nartifacts aren't tracked in the git history in any meaningful way. Unless, of\ncourse, the designer is directly committing code, but then the design process\nstill isn't accurately reflected, only the outcome of it.\nIPF: That may be one theory as to the problem's root, but it's certainly not\nthe only explanation. Because the question of missing skills in open source\ncould be the source of many blog posts, I've narrowed it down to a list. Here\nare some other thoughts I've had or heard floating about (like in\nthis discussion)\nexplaining the absence of design in this space:\n\nDesigners might be unaware of open source.\nDesigners might be wary of working in public. (\"You want me to post my\nhorrible first draft somewhere anyone could see it forever?\")\nSome open-source communities don't want or need conventional design.\nSome open-source communities might not welcome different skill sets.\nDesigners have a history of being underpaid or not paid for their work; we\n  are often encouraged to be wary of working for free, especially if it's not\n  for a group/project/product we've heard of.\nAs has been mentioned about code and documentation, lots of what is needed\n  most in open-source projects is not the clear or glamourous design work that\n  might draw in designers. There are mostly question marks and maintenance\n  work.\nThis type of work lacks recognition in design communities.\n\nAnd here are some issues we'll tackle below:\n\nAn open-source community might be relatively unaware of design and what it\n  can mean in practice.\nLack of design talent and design literacy is common in open-source projects.\n  Designers are likely to be teaching as they work, which means more work.\nOpen-source projects tend to lack community recognition beyond what's tied to\n  code.\n\nTG: For many (most) open-source projects, designers are not a part of the\nfounding team. The infrastructure used to support many (most) open-source\nprojects was not made for designers. Many people who work in open source have\nbackgrounds that have given them limited interaction with design work. This\nisn't intended as a gripe so much as a statement of facts because if we aren't\nin agreement about this then the rest of this post doesn't make sense. What\nthese facts mean is that doing design work for open-source projects is unusual,\nhas a high barrier to entry, and sometimes doesn't feel like design at all. If\nthis is all sounding vague and nebulous, you're right. Welcome to navigating\ndesign in open source!\nBreaking it Down\nDesign literacy\nIPF: One person wants a light yellow icon on a white background.\u00b9 One person\nwants the warning text written in all caps.\u00b2 One person thinks this specific\nfeature should use interactions that don't match the ones used everywhere else.\u00b3\nIt's not malicious, but it represents differing goals and knowledge that need to\nbe united to move forward in a collaborative environment. If any of this doesn't\nseem like a problem, that's why we're here.\nDisconnects between a design team and anyone else on a project are not unique to\nopen source. Designing often alone and outnumbered by people who might not know\nwhat you are talking about is where open source hits harder. Designers present\nnew information in this space, and so they constantly bear the burden of proof\nto explain every move. They usually bear this alone. While it can be very\nfulfilling to know you are helping people learn a new perspective, it detracts\ntime and energy from design itself.\nCommunity Trust and Recognition\nTG: A developer decides to contribute to their favorite open-source project\nfor the first time. On the PR, there's one person who keeps asking them lots of\nquestions, attaching a lot of images, and requesting changes. They sound like\nthey understand the project, but they don't have the contributor tag and there\nisn't any record of them working directly with the project. In fact, there's\nbarely anything at all on their GitHub profile. Who could it be? Probably a\ndesigner.\nIPF: In my experience, communities have always been welcoming of design\nwork, so this scenario usually ends well. Still, it exposes exactly how the\nvalue system (trust and reputation) that drives a lot of open-source work fails\npeople who do non-development work (not only design) in this field. Personally,\nI'm grateful that people tend to trust my word when I say I'm a designer, or\nthat there have always been developers in the communities I've worked with\nwilling to vouch for me if needed. However, it still is extra time and work\nsiphoning energy from the design, and continues a greater trend of lack of\ntrackable credit for design work.\nCoordination and Product Direction\nTG: None of this is to say that design contributions are not welcome in\nopen source, in fact I've personally found the opposite to be true. But at its\ncore, open source is, and has generally been, focused on generating code. This\nfocus is a choice, and this choice has impacts on the community, governance,\ndirection, and more. Given the constraints that go into a decentralized team,\noften working on different priorities, it can be difficult to know in what\ndirection the project is heading. This process often generates useful software\nthat's technically sound, after all, as Vincent Van Gogh said, \"Great things are\ndone by a series of small things being brought together.\" Open source does a\nwonderful job of improving large complex software systems by iterating on\nbite-sized pieces, but this work often comes in spurts when maintainers have\ntime and inspiration.\nGreat design, on the other hand, is often the product of a well-defined vision.\nSteve Jobs was famous for saying, \"Design is not what it looks like and feels\nlike, it's how it works.\" But we don't often have the luxury, as designers, of\ndesigning how it works, we're asked to pick up wherever the project is, with no\nguarantee of impact in the area we are working on.\nIt can be hard to invest in a thorough design process if you're not sure\nwhat's going to be developed next, or ever. And if you do take that chance and\ninvest in a design, there's an uncomfortably high chance you're missing the mark\nof what the maintainer thinks the next big steps of their project are, (possibly)\ndocumented only in their mind. If members of a project don't intentionally take\ntime to prioritize design, it can be easier to just merge changes directly into\nthe code base and see how the community likes it, as opposed to putting\nsignificant amounts of time into design. Many features that end up in\nopen-source software are the result of one or two software developers writing\nthe code and offering it directly to the project, while even the most well\nexecuted design still needs to be coded. It often happens that the solution\nthat's working now, is the priority over the solution that might work better,\nif it is even built.\nIPF: Because features are often already or mostly built by the time they are\nrevealed to the team (designers included), they have also already been\ndesigned\u2014intentionally or not\u2014by the people who built that feature. This process\nworks against involving designer perspectives where they might be most helpful\nor impactful: when initially solving the problem.\nTo a brighter future\nTG: In my experience, which is mostly in Jupyter-related projects, design\ninput is desired, and well received. Different projects, depending on size and\nscope, have different understandings of, and interest in, design. If designers\nare going to make a larger impact on open-source projects, committers need to\nunderstand the value of design beyond simply \"making things look good.\" Many\nopen-source initiatives would benefit from a visual overhaul, especially of the\ntype that leans into consistent looks for similar interactions throughout the\nsoftware. This is usually the limit of other contributors' expectations of\ndesigners, but that's just scratching the surface of the problems we've worked\nthrough so far, and what we're capable of in the future.\nIPF: At the beginning, we mentioned imagining a better world as a part of\nbeing a designer, and imagining a healthier open-source experience is no\nexception! Changing the collaboration process is a good first step.\nTG: As a Human-Computer Interaction practitioner, I always strive to\nunderstand \"why\" a problem exists before I decide what to do about it. There's a\nlot of \"what\" and \"how\" in open-source development. Oftentimes, a scientist or\ndeveloper simply solves the problem they have. But if we take some time to walk\nthrough the design process and understand more about why the question arose and\nwho else has the problem, we can get to the core issue, and design both a\nsolution and an interface for it in a way that is knowledgeable of the problem as\nwell as the people trying to solve it. In essence, we build software that the user\ncan partner with to process data, instead of building a tool that accomplishes a\nsingle task.\nIPF: But if designers are brought in just at the review or UI decision\nstage, as they often are, it can be a little late to start asking \"why\" with so\nmuch work already done. The ideal answer is to bring designers in early,\npreferably as early as when the problem is discovered. This currently isn't\npossible, there are still so few designers in this space and so many problems to\nbe solved. The mainstream field of User Experience Design describes two\noverarching duties for designers in any environment: creating new features\n(discovering what's useful), and refining software (to make what's useful\nbetter). How often do I get to deep dive into either of these directions?\nRarely. Creating new features is more likely to be driven by developer\ninspiration, while refinement usually must rely on external standards and\ncomparable software experiences to make up for the resources open-source\nprojects don't often have. So what might be the path forward? Stop only trying\nto lean into what designers think they do best and work with open source's\nstrengths of innovation and pushing technical limitations.\nTG: This brings us back to a foundational aspect of open source, mainly,\nopen-source software is built on trust and relationships between contributors.\nDesign in open source has to be a two-way street. Developers need to be open to\ndigging in and understanding the problem from its source, and designers need to\nbe willing to put in the work to understand the technical limitations of the\ntechnology that's being utilized to build the software. Fortunately this gives\ndesigners and developers a place to meet in the middle (and that place will\nalmost always be GitHub). I've had a number of experiences where simply\nasking hard questions has helped developers to better understand why they're\nsolving that problem at all. At the same time, there have been hundreds of\nsolutions I have conceived of that just aren't yet possible to implement.\nGetting involved early, and understanding what the limitations are is a great\nway to help existing teams integrate design into their process.\nIPF: Want to hear us talk more about this? Tim and I had a conversation\nwith Ani Krishnan and Tony Fast\nhere.\n\n\u00b9 Wondering what's wrong with this scenario? Yellow and white is a low contrast\ncombination that makes it hard to see the icon. It also fails accessibility\ncontrast standards.\n\u00b2 All caps aren't recommended. Depending how it's implemented, it can be an\naccessibility concern, too.\n\u00b3 A common goal of interface design is to teach users via interaction patterns.\nInconsistency is confusing and can break user trust both short and long term.",
      "tags": "design,Open-Source,User Experience,UX",
      "url": "https://labs.quansight.org/blog/2020/11/introduction-to-design-in-open-source/"
    },
    {
      "title": "Querying multiple backends with Ibis",
      "text": "In our recent Ibis post, we discussed querying & retrieving data using a familiar Pandas-like interface.\nThat discussion focused on the fluent API that Ibis provides to query structure from a SQLite database\u2014in particular, using a single specific backend.\nIn this post, we'll explore Ibis's ability to answer questions about data using two different Ibis backends.\nimport ibis.omniscidb, dask, intake, sqlalchemy, pandas, pyarrow as arrow, altair, h5py as hdf5\n\n\n\nIbis in the scientific Python ecosystem\nBefore we delve into the technical details of using Ibis, we'll consider Ibis in the greater historical context of the scientific Python ecosystem. It was started by Wes McKinney, the creator of Pandas, as way to query information on\nthe Hadoop distributed file system and PySpark. More backends were added later as Ibis became a general tool for data queries.\nThroughout the rest of this post, we'll highlight the ability of Ibis to generically prescribe\nquery expressions across different data storage systems.\n\n\nThe design of Ibis backends\nCurrently, Ibis supports more than ten different backends:\n>>> import ibis\n>>> dir(ibis)\n[...HDFS...WebHDFS...bigquery...clickhouse...hdf5...impala...omniscidb...pandas...pyspark...spark...sql...sqlite...]\n\n\n\nA backend manages the computation of an Ibis query expression; the query expression itself is independent of the computation.\nA backend implementation that can be queried with Ibis has one of the three following architectures:\n\nDirect execution backends (e.g., Pandas and HDF5);\nExpression-generating backends that create SQLAlchemy expressions (e.g., ibis.sql); or\nString-generating backends (e.g., ibis.bigquery and ibis.omniscidb)\n\nWe'll unravel some of the different capabilities of each approach below.\nA data-driven history of Ibis compatibility\nThe table below looks at over 2000 issues in the Ibis project.\nIt provides an annual summary of the issues tagged in Ibis\nfor different backends over a span of six years.\n\n\n\n\n\u00a0 \u00a0 2015\n\u00a0 \u00a0 2016\n\u00a0 \u00a0 2017\n\u00a0 \u00a0 2018\n\u00a0 \u00a0 2019\n\u00a0 \u00a0 2020\n\n\n\n\nOmnisci\n\n\n\n31\n33\n38\n\n\nSpark\n\n\n1\n\n22\n3\n\n\nPostgreSQL\n2\n3\n21\n10\n17\n4\n\n\nBigQuery\n\n\n15\n71\n12\n2\n\n\nPandas\n2\n\n49\n35\n32\n4\n\n\nSQLite\n25\n2\n10\n8\n1\n1\n\n\nImpala\n52\n4\n15\n17\n4\n2\n\n\nKudu\n\n\n\n\n\n1\n\n\nGeospatial\n\n\n\n\n7\n3\n\n\nClickHouse\n\n\n8\n9\n1\n4\n\n\nMySQL\n\n\n\n2\n2\n4\n\n\nSQLAlchemy\n17\n3\n10\n2\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe note an early focus SQLite, SQLAlchemy and Impala.\nLater, work began on the Pandas backend rounding out the three different types of backgrounds.\nFrom this point, improvements were made to these key backends as ClickHouse, Spark and PostgreSQL.\nFor the past two years, Quansight, in partnership with OmniSci, added the ibis.omniscidb\nstring-generating backend. Further, our responsibilities have expanded\nto support Ibis as community maintainers through Quansight Labs.\nThis collaboration introduced geospatial functionality to Ibis for several backends.\nThere are currently ongoing efforts to introduce support for SQL Server backends.\nWhat other backends would you like to see for Ibis?  Maybe our community could benefit from a Dask\nbackend or an Altair backend?\nIbis direct execution backends\nIbis direct execution backends like Pandas and HDF5 operate on conventional in-memory python objects.\nPandas is the gold standard for structured data in Python and inspires the API for Ibis.\npd = ibis.pandas.connect({'A': pandas.util.testing.makeDataFrame()})\n\n\n\nThe object pd is an Ibis backend based on pandas.DataFrame.\nexpression = pd.table('A').head()\n\n\n\nexpression is an Ibis query that has expression.compile and expression.execute methods.\nWe'll recognize the execute method when we return pandas.DataFrames from Ibis expression.\nThe compile method does not trigger any computation, rather it constructs an intermediate form\nthat is interpreted by a backend.\n>>> assert isinstance(expression.compile(), ibis.expr.types.TableExpr)\n\n\n\nIn the case of direction execution backends, the expression compiles to an the original Ibis\nexpression.  The computation itself is carried out based on a set of recipes defined in Ibis.\nIn general, we would typically do this work directly in Pandas; however this approach is\npractical in making tests for backend-independent expressions.\nIbis expression-generating backends.\ndb = ibis.sqlite.connect('lahmansbaseballdb.sqlite')\nexpression = db.table('halloffame').head()\n\n\n\nExpression-generating backends operate on SQL databases that interoperate with SQLAlchemy.\n>>> assert isinstance(expression.compile(), sqlalchemy.sql.Select)\n\n\n\nIn the case of expression-generating backends, the intermediate representation is a sqlalchemy object.\nSQLAlchemy is the Database Toolkit for Python and Ibis leverages its compatibility\nwith traditional SQL databases.\nIbis string generating backends.\nThere are two options for downloading Ibis: using pip or using conda.\n\npip: the extra backends from Ibis need to be requested explicitly, e.g.:\n\npip install --upgrade ibis-framework[omniscidb] ibis-framework[sqlite]\n\n\n\n\nconda: all the supported backends (e.g., SQL, Pandas, BigQuery, Omnisci, etc.) are bundled in a single conda package and can be downloaded/installed simultaneously:\n\nconda install -c conda-forge ibis-framework # installs all the backends!\n\n\n\nString-generating backends allow Ibis to interface with big data systems that manage\ntheir own computation. For example, we may connect to an example Omnisci database.\nimport ibis.omniscidb\nomnisci = ibis.omniscidb.connect(host='metis.omnisci.com',\n                                 user='demouser',\n                                 password='HyperInteractive',\n                                 port=443,\n                                 database='omnisci',\n                                 protocol='https')\n\n\n\nThe omnisci object is described as a string-generating backend because the intermediate representation of the query is a flavor of SQL.\nexpression = omnisci.table('upstream_reservoir').head()\n\n\n\nA string-generating expression compiles to ibis.omniscidb flavored SQL, while ibis.bigquery may have a different string representation.\n>>> expression.compile()\n'SELECT *\\nFROM upstream_reservoir\\nLIMIT 5'\n\n\n\nAcknowledgements\nMajor credit goes to @xmnlab in his heroic PR to introduce ibis.omniscidb into Ibis. You can watch\nthe drama play out in this Github Issue. We'd like to thank the maintainers of Ibis for\ntheir effort in supporting the Ibis community.\nLearn more about OmniSci and ibis.omniscidb in this Quansight Labs post:\nIbis: Python data analysis productivity framework.",
      "tags": "Ibis,OmniSci,Pandas,SQL",
      "url": "https://labs.quansight.org/blog/2020/11/the-ibis-backends/"
    },
    {
      "title": "Manylinux1 is obsolete, manylinux2010 is almost EOL, what is next?",
      "text": "The basic installation format for users who install packages via pip is\nthe wheel format. Wheel names are composed of four parts: a\npackage-name-and-version tag (which can be further broken down), a Python tag,\nan ABI tag, and a platform tag. More information on the tags can be found in\nPEP 425.  So a package like NumPy\nwill be available on PyPI as numpy-1.19.2-cp36-cp36m-win_amd64.whl for 64-bit\nwindows and numpy-1.19.2-cp36-cp36m-macosx_10_9_x86_64.whl for macOS. Note\nthat only the plaform tag win_amd64 or macosx_10_9_x86_64 differs. \nBut what about Linux? There is no single, vendor controlled, \"Linux platform\"\ne.g., Ubuntu, RedHat, Fedora, Debian, FreeBSD all package software at slightly\ndifferent versions. What most Linux distributions do have in common is the\nglibc runtime library, and a smattering of various additional system libraries.\nSo it is possible to define a least common denominator (LCD) of software\nexpected to be on a Linux platform (exceptions apply, e.g. non-glibc\ndistributions).\nThe decision to converge on a LCD common platform gave birth to the\nmanylinux1 standard. Going back\nto our example, numpy is available as\nnumpy-1.19.2-cp36-cp36m-manylinux1_x86_64.whl.\nThe first manylinux standard, manylinux1, was based on CentOS5 which has been\nobsolete since\nMarch 2017. The subsequent manylinux2010 standard is based on CentOS6, which\nwill hit end-of-life in December 2020. The manylinux2014 standard still has some\nbreathing room. Based on CentOS7, it will reach end-of-life in July 2024.\nSo what is next for manylinux, and what manylinux should users and package\nmaintainers use?\n\n\nIf manylinux1 is obsolete, why are there still manylinux1 wheels?\nWheels are typically consumed by Pip via pip install. Manylinux wheels are\nused for projects that require compilation, otherwise they would ship\npure Python wheels with the \"none\" platform tag, meaning they are compatible with\nany platform. So say you are a library author and want to make it convenient\nfor users to install your package. If you ship a manylinux2014 wheel, but the\nversion of Pip your users have is too old to support manylinux2014 wheels, Pip\nwill happily download the source package and compile it for them. Havoc ensues:\nWindows users typically cannot compile, prerequisites will be missing. Pip has\na --only-binary option to prevent it from downloading source code and\ncompiling, and a --prefer-binary option to prefer older binary packages over\ncompiling from source, but neither is on by default.\nPip began supporting manylinux2010 wheels with version\n19.0,\nreleased in Jan 2019. The version of Pip that is officially shipped with Python 3.6, via\nthe ensurepip module, is\nversion 18. Python 3.7 ships pip 20. It is easy enough to upgrade, but to be on\nthe safe side, and prevent havoc, library authors will ship a manylinux1 wheel\nfor Python 3.6 support.\nWhat happens now that Python 3.6 is falling out of favor?\nPython 3.6 is no longer in active\ndevelopment. In fact, the scientific\nPython community has decided to stop actively supporting Python 3.6 from\nJuly 2020.\nSo I would expect to see projects begin to drop the older manylinux1 format,\nand drop support for Python 3.6 sooner rather than later, meaning that\nmanylinux2014 may soon become the only option for new versions.\nWhat about Conda packages?\nConda does not use the same kind of wheel format provided by PyPI and Pip. Conda's\nbuild system is internally consistent, and Conda packagers build a binary\npackage for each supported OS, thus they are not bound to the manylinux\ndesignation. Conda does not have a declared policy around deprecating Python\n3.6 yet. Conda does support pip (but try not to mix conda and pip\nusage!), and the Pip provided should be version 20 or later. If needed,\nconda upgrade pip should get you a modern version, so here too\nmanylinux2014 will soon become the only option.\nWhat comes after manylinux2014?\nThe glibc used in manylinux2014 is defined as the one used by CentOS7. This OS\nwas released in June 2014. This manylinux standard, for the first time,\ndeclared support for non-x86 hardware systems like ARM64 (aarch64), Power\n(ppc64) and S390X.  However the ARM platform has grown greatly since 2014, and\nglibc has moved from version 2.17 to 2.31, fixing many bugs. Since the real\ndriver for platform compatibility is glibc, PEP\n600 defined a \"perennial manylinux\nspec\" that is based on the glibc version number. A lot of work has already\ntaken place to support the next\nversion. Now we need to take the dive: decide what the base OS for the next\nmanylinux tag will be, roll out a Docker image and tooling around it, and\nconvince library packaging teams to adopt it. This is needed to allow libraries\nlike NumPy to confidently use the glibc routines fixed after 2014. For\ninstance, this issue is\npreventing NumPy from properly supporting np.float128 on Power and S390X.\nWhat about non-x86 machines and Linux?\nAs mentioned before, starting with manylinux2014 pip and wheel supports\nnon-x86 architectures like ARM64. Many packages are just now starting to roll\nout support for these architectures, as the CI systems that support open source\nprojects (like TravisCI) have only recently made those platforms available.\nIt might be easier for users to adopt Conda and the conda-forge channel\nsince conda-forge has support for non-x86 architectures today.\nOK, so what is the bottom line?\n\nUse pip v20 or later to make it easier on libarary packagers: modern pip\n  versions will take the latest manylinux package they can support and will be\n  forward-compatible with the PEP 600 perennial manylinux standard.\nManylinux1 and Python 3.6 are going away. Update your systems.\nFor people looking to move PEP 600 forward, the next step is to dive into the\n  auditwheel repo to define and support\n  the next manylinux version.",
      "tags": "manylinux,packaging,pip",
      "url": "https://labs.quansight.org/blog/2020/11/manylinux1-is-obsolete-manylinux2010-is-almost-eol-what-is-next/"
    },
    {
      "title": "Design of the Versioned HDF5 Library",
      "text": "In a previous\npost, we\nintroduced the Versioned HDF5 library and described some of its features. In\nthis post, we'll go into detail on how the underlying design of the library\nworks on a technical level.\n\nVersioned HDF5 is a library that wraps h5py and offers a versioned abstraction\nfor HDF5 groups and datasets. Versioned HDF5 works fundamentally as a\ncopy-on-write system. The basic\nidea of copy-on-write is that all data is effectively immutable in the\nbackend. Whenever a high-level representation of data is modified, it is\ncopied to a new location in the backend, leaving the original version intact.\nAny references to the original will continue to point to it.\nAt a high level, in a versioned system built with copy-on-write, all data in\nthe system is stored in immutable blobs in the backend. The immutability of\nthese blobs is often enforced by making them\ncontent-addressable,\nwhere the blobs are always referred to in the system by a cryptographic hash\nof their contents. Cryptographic hashes form a mapping from any blob of data\nto a fixed set of bytes, which is effectively one-to-one, meaning if two blobs\nhave the same hash, they must be exactly the same data. This means that it is\nimpossible to mutate a blob of data in-place. Doing so would change its hash,\nwhich would make it a different blob, since blobs are referenced only by their\nhash.\nWhenever data for a version is committed, its data is stored as blobs in the\nbackend. It may be put into a single blob, or split into multiple blobs. If it\nis split, a way to reconstruct the data from the blobs is stored. If a later\nversion modifies that data, any blobs that are different are stored as new\nblobs. If the data is the same, the blobs will also be the same, and hence\nwill not be written to twice, because they will already exist in the backend.\nAt a high level, this is how the git version control system works, for example\n(this is a good talk on the\ninternals of git). It is also how versioning constructs in some modern\nfilesystems like APFS and Btrfs.\nVersioned HDF5 Implementation\nIn Versioned HDF5, this idea is implemented using two key HDF5 primitives:\nchunks and virtual datasets.\nIn HDF5, datasets are split into multiple chunks. Each chunk is of equal size,\nwhich is configurable, although some chunks may not be completely full. A\nchunk is the smallest part of a dataset that HDF5 operates on. Whenever a\nsubset of a dataset is to be read, the entire chunk containing that dataset is\nread into memory. Picking an optimal chunk size is a nontrivial task, and\ndepends on things such as the size of your L1 cache and the typical shape of\nyour dataset. Furthermore, in Versioned HDF5 a chunk is the smallest amount of\ndata that is stored only once across versions if it has not changed. If the\nchunk size is too small, it would affect performance, as operations would\nrequire reading and writing more chunks, but if it is too large, it would make\nthe resulting versioned file unnecessarily large, as changing even a single\nelement of a chunk requires rewriting the entire chunk. Versioned HDF5 does\nnot presently contain any logic for automatically picking a chunk size. The\npytables\ndocumentation has some\ntips on picking an optimal chunk size.\nBecause chunks are the most basic HDF5 primitive, Versioned HDF5 uses them as\nthe underlying blobs for storage. This way operations on data can be as\nperformant as\npossible.\nVirtual datasets are a special kind\nof dataset that reference data from other datasets in a seamless way. The data\nfrom each part of a virtual dataset comes from another dataset. HDF5 does this\nseamlessly, so that a virtual dataset appears to be a normal dataset.\nThe basic design of Versioned HDF5 is this: whenever a dataset is created for\nthe first time (the first version containing the dataset), it is split into\nchunks. The data in each chunk is hashed and stored in a hash table. The\nunique chunks are then appended into a raw_data dataset corresponding to the\ndataset. Finally, a virtual dataset is made that references the corresponding\nchunks in the raw dataset to recreate the original dataset. When later\nversions modify this dataset, each modified chunk is appended to the raw\ndataset, and a new virtual dataset is created pointing to corresponding\nchunks.\nFor example, say we start with the first version, version_1, and create a\ndataset my_dataset with n chunks. The dataset chunks will be written into the\nraw dataset, and the final virtual dataset will point to those chunks.\n\nIf we then create a version version_2 based off version_1, and modify only\ndata contained in CHUNK 2, that new data will be appended to the raw dataset,\nand the resulting virtual dataset for version_2 will look like this:\n\nSince both versions 1 and 2 of my_dataset have identical data in chunks other than\nCHUNK 2, they both point to the exact same data in raw_data. Thus, the\nunderlying HDF5 file only stores the data in version 1 of my_dataset once, and\nonly the modified chunks from version_2's my_dataset are stored on top of that.\nAll extra metadata, such as attributes, is stored on the virtual dataset.\nSince virtual datasets act exactly like real datasets and operate at the HDF5\nlevel, each version is a real group in the HDF5 file that is exactly that\nversion. However, these groups should be treated as read-only, and you should\nnever access them outside of the Versioned HDF5 API (see below).\nHDF5 File Layout\nInside of the HDF5 file, there is a special _versioned_data group that holds\nall the internal data for Versioned HDF5. This group contains a versions\ngroup, which contains groups for each version that has been created. It also\ncontains a group for each dataset that exists in a version. These groups each\ncontain two datasets, hash_table, and raw_data.\nFor example, consider a Versioned HDF5 file that contains two versions,\nversion1, and version2, with datasets data1 and data2. Suppose also\nthat data1 exists in both versions and data2 only exists in version2.\nThe HDF5 layout would look like this\n/_versioned_data/\n\u251c\u2500\u2500 data1/\n\u2502   \u251c\u2500\u2500 hash_table\n\u2502   \u2514\u2500\u2500 raw_data\n\u2502\n\u251c\u2500\u2500 data2/\n\u2502   \u251c\u2500\u2500 hash_table\n\u2502   \u2514\u2500\u2500 raw_data\n\u2502\n\u2514\u2500\u2500 versions/\n    \u251c\u2500\u2500 __first_version__/\n    \u2502\n    \u251c\u2500\u2500 version1/\n    \u2502   \u2514\u2500\u2500 data1\n    \u2502\n    \u2514\u2500\u2500 version2/\n        \u251c\u2500\u2500 data1\n        \u2514\u2500\u2500 data2\n\n__first_version__ is an empty group that exists only for internal\nbookkeeping purposes.\nThe hash_table datasets store the hashes for each chunk of data, so that\nduplicate data will not be written twice, and the raw_data dataset stores\nthe chunks for all versions of a given dataset. It is referenced by the\nvirtual datasets in the corresponding version groups in versions/. For\nexample, the chunks for the data data1 in versions version1 and version2\nare stored in _versioned_data/data1/raw_data.\nVersioned HDF5 API\nThe biggest challenge of this design is that the virtual datasets representing\nthe data in each versioned data all point to the same blobs in the backend.\nHowever, in HDF5, if a virtual dataset is written to, it will write to the\nlocation it points to. This is at odds with the immutable copy-on-write\ndesign. As a consequence, Versioned HDF5 needs to wrap all the h5py APIs that\nwrite into a dataset to disallow writing for versions that are already\ncommitted, and to do the proper copy-on-write semantics for new versions that\nare being staged. Several classes that wrap the h5py Dataset and Group objects\nare present in the versioned_hdf5.wrappers submodule. These wrappers act\njust like their h5py counterparts, but do the right thing on versioned data.\nThe top-level versioned_hdf5.VersionedHDF5File API returns these objects\nwhenever a versioned dataset is accessed. They are designed to work seamlessly\nlike the corresponding h5py objects.\nThe Versioned HDF5 library was created by the D. E. Shaw\ngroup in conjunction with\nQuansight.",
      "tags": "h5py,HDF5",
      "url": "https://labs.quansight.org/blog/2020/09/design-of-the-versioned-hdf5-library/"
    },
    {
      "title": "Performance of the Versioned HDF5 Library",
      "text": "In several industry and science applications, a filesystem-like storage model such as HDF5 is the more appropriate solution for manipulating large amounts of data. However, suppose that data changes over time. In that case, it's not obvious how to track those different versions, since HDF5 is a binary format and is not well suited for traditional version control systems and tools. \nIn a previous post, we introduced the Versioned HDF5 library, which implements a mechanism for storing binary data sets in a versioned way that feels natural to users of other version control systems, and described some of its features. In this post, we'll show some of the performance analysis we did while developing the library, hopefully making the case that reading and writing versioned HDF5 files can be done with a nice, intuitive API while being as efficient as possible. The tests presented here show that using the Versioned HDF5 library results in reduced disk space usage, and further reductions in this area can be achieved with the use of HDF5/h5py-provided compression algorithms. That only comes at a cost of <10x file writing speed.\n\n\nWhat are we measuring?\nPerformance can mean different things for different operations. For the tests presented here, the main goals are:\n\nTo evaluate the performance (size on disk and I/O speed) of reading/writing versioned HDF5 files and compare it with non-versioned files (that is, files where only the last version of the datasets is stored);\nTo evaluate the performance when reading/writing data to a versioned HDF5 file in a set of different use cases;\nTo evaluate the performance when different parameter options are considered for chunking and compression on versioned HDF5 files.\n\nWhen different versions of a dataset are stored in a versioned HDF5 file, modified copies of the data are stored as new versions. This means that there may be duplication of data between versions, which might impact the performance of reading, writing or manipulating these files.\nIn order to analyze this, we will consider test files created with a variable number of versions (or transactions) of a dataset consisting of three ndarrays of variable length. One test includes a two-dimensional ndarray as part of this dataset, but all other test cases consist of three one-dimensional ndarrays per dataset.\nWith each new version a certain number of rows are added, removed, and modified. For\nthese tests, all changes are made to elements chosen according to a power law which biases the modifications towards the end of the array, simulating a possible use case of modifying more recent results in a given timeseries.\nThe tests are as follows:\n\n\nA large fraction of changes is made to the dataset with each new version: The dataset initially has three arrays with 5000 rows, and 1000 positions are chosen at random and changed, and a small number (at most 10) rows are added or deleted with each new version. We will refer to this test as test_large_fraction_changes_sparse.\n\n\nA small fraction of changes is made to the dataset with each new version: The dataset initially has three arrays with 5000 rows, but only 10 positions are chosen at random and changed, and a small number (at most 10) rows are added or deleted with each new version. We will refer to this test as test_small_fraction_changes_sparse.\n\n\nA large fraction of changes is made to the dataset with each version, with the same three arrays of 5000 rows defined initially, 1000 positions are chosen at random and changed, but the size of the final array remains constant (no new rows are added and no rows are deleted). We will refer to this test as test_large_fraction_constant_sparse.\n\n\nIn the next tests, the number of modifications is dominated by the number of appended rows. There are two such cases:\n\n\nIn the first case, the dataset contains three one-dimensional arrays with 1000 rows initially, and 1000 rows are added with each new version. A small number (at most 10) values are chosen at random, following the power law described above, and changed or deleted. We call this test test_mostly_appends_sparse.\n\n\nIn the second case, the dataset contains one two-dimensional array with shape (30, 30) and two one-dimensional arrays acting as indices to the 2d array. In this case, rows are only appended in the first axis of the two-dimensional array, and a small number of positions (at most 10) is chosen at random and changed. We call this test test_mostly_appends_dense.\n\n\nTo test the performance of the Versioned HDF5 library, we have chosen to compare a few different chunk sizes and compression algorithms. These values have been chosen arbitrarily, and optimal values depend on different use cases and on the nature of the datasets stored in the file.\nFile sizes\nAs the number of versions in a file grows, its size on disk is also expected to grow. However, it is reasonable to expect that the overhead of storing metadata for versioned files doesn't cause the file sizes to explode as the number of versions increases.\nWe\u2019ll start by analyzing how the HDF5 file sizes grow as the number of versions grows. Using a chunk size of 4096, we can see the following results for the 4 one-dimensional test cases:\n\nWe can see from the figure that in test_large_fraction_constant_sparse case, writing 5000 versions of a 117KB array, which would take around 572MB in separate files, takes around 252MB - less than half the storage size. Note that the other examples the size of the arrays stored in the file also grow as the number of versions grows, since each new version is changing the original arrays by adding, deleting and changing values in the original arrays. Keep in mind there is redundant data as some of it is not changed during the staging of a new version but it is still being stored. It's also worth noting that for these tests we don't use compression, even though algorithms available in h5py can be used in Versioned HDF5. You can see the Versioned HDF5 documentation for more detailed tests regarding chunking and compression.\nCreation times\nIf we look at the time spent creating the files for each example, comparing chunk sizes but not considering compression, we have something like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we can look at the time required to stage a new version in the file, that is, to add a new transaction. The graphs below show, for each fixed number of transactions, the time required to add new versions as the file is created. Note that the scales vary for each test.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt is clear that as the number of versions stored in the file increases, the times required to create versioned HDF5 files increases significantly when compared to regular HDF5 files. However, note that the increase is linear, consistent with what is expected from adding new versions to the file. For example, looking at test_large_fraction_constant_sparse, where the size of the arrays do not increase as new versions are added, choosing (again) a chunk size of 4096 means that writing each new version to file takes about 6-8x as much as in the unversioned case, with more than 50% savings on disk storage. Note that a larger chunk size may mean faster read and write times but can also mean larger file sizes if no compression is used, because of how Versioned HDF5 is designed. This is expected, since all chunks where data has been changed from one version to the next have to be stored. Also, in test_mostly_appends_sparse, where the size of the arrays stored in the file grow significantly with each new version, we can see a marked increase in the times required to stage new versions.\nFinally, we'll look at a two-dimensional dataset. In this case, we have chosen different chunk sizes to test, considering that larger chunk sizes increase file sizes considerably.\n\n\nWe can also look at the times required to create each new version and write it to file in the versioned and unversioned cases. This is shown in the image below (note the different axis scale from previous figures.)\n\nThis test case is unique for a few reasons. First, having a two-dimensional dataset introduces new considerations, such as the number of rows being added in each axis. For this test case, we have only added (few) new rows to the first axis with each new version, and this might explain why we don\u2019t see an increase in the time required to write new versions to file as the number of versions grow. While these are preliminary tests, and multidimensional datasets are still experimental at this point in Versioned HDF5, we can see that there are no unexpected drops in performance and the results can generally be explained by the size of the data stored in each file.\nI/O performance for versioned HDF5 files\nFirst, we'll look at the time required to read data from all versions in a file, sequentially. To keep this test short, we\u2019ll only analyze the tests using no compression, and chunk size 16384 for the one-dimensional datasets and 256 for the two-dimensional dataset in test_mostly_appends_dense.\nPlotting with respect to the number of versions, we get the following:\n\nAs expected, read times increase for files with a larger number of versions, but the growth is close to linear in all cases except for test_mostly_appends_sparse, where the large array sizes explain the larger read times.\nNext, we\u2019ll compare the times necessary to read the latest version on each file. Because of how Versioned HDF5 is designed, this is the same as reading the last array stored in the HDF5 file. For each test, we made 20 different reads of the latest version in order to eliminate fluctuations generated by background processes or the OS (solid lines). We also compare these read times with the time required to read an unversioned file (which only stored the latest version - dashed black line).\n\nIn this case, we can see that on average, reading the latest version on a VersionedHDF5File is ~5x-10x slower than reading an unversioned file. Also, the time required to read the latest version from a versioned HDF5 file increases modestly with the number of versions stored in the file, except when the size of the array increases significantly with each new version.\nSummary\nThese results show that the library behaves reasonably well without unexpected overhead. The reduction in file size comes at a moderate cost for file writing speed, and file reading speed for the latest version of the data is unaffected. The largest impact on I/O performance and storage is in fact explained by the size of the datasets stored in the file, and the Versioned HDF5 abstraction does not significantly reduce this performance. \nOverall, the worst performance was observed for tests with large array sizes. This seems to show that the file sizes and I/O performance of versioned HDF5 files are significantly affected by the size of the unique datasets stored in each file, which is to be expected. Also, choosing the right chunk size parameter can have an impact on the performance of the library.\nNext steps\nThis is the second post in a series about the Versioned HDF5 library. In our next post, we'll discuss the the design of the Versioned HDF5 library.\nversioned-hdf5 1.0 has recently been released, and is available on PyPI and conda-forge. You can install it with\nconda install -c conda-forge versioned-hdf5\n\n\n\nDevelopment for versioned-hdf5 happens on GitHub. Currently, the library supports basic use cases, but there is still a lot to do. We welcome community contributions to the library, including any issues or feature requests.\nFor now, you can check out the\ndocumentation for more details on\nwhat is supported and how the library is built.\nThe Versioned HDF5 library was created by the D. E. Shaw\ngroup in conjunction with\nQuansight.",
      "tags": "h5py,HDF5",
      "url": "https://labs.quansight.org/blog/2020/09/versioned-hdf5-performance/"
    },
    {
      "title": "PyTorch-Ignite: training and evaluating neural networks flexibly and transparently",
      "text": "Authors: Victor Fomin (Quansight), Sylvain Desroziers (IFPEN, France)\n\nThis post is a general introduction of PyTorch-Ignite. It intends to give a brief but illustrative overview of what PyTorch-Ignite can offer for Deep Learning enthusiasts, professionals and researchers. Following the same philosophy as PyTorch, PyTorch-Ignite aims to keep it simple, flexible and extensible but performant and scalable.\n\n\nThroughout this tutorial, we will introduce the basic concepts of PyTorch-Ignite with the training and evaluation of a MNIST classifier as a beginner application case. We also assume that the reader is familiar with PyTorch.\n This tutorial can be also executed in Google Colab. \nContent\u00b6\nPyTorch-Ignite: What and Why?\nQuick-start example\nAdvanced features\nPower of Events & Handlers\nOut-of-the-box metrics\nOut-of-the-box handlers\nDistributed and XLA device support\n\n\nProjects using PyTorch-Ignite\nProject news\n\n\n\n\n\n\n\n\nPyTorch-Ignite: What and Why ?\u00b6PyTorch-Ignite is a high-level library to help with training and evaluating neural networks in PyTorch flexibly and transparently.\nPyTorch-Ignite is designed to be at the crossroads of high-level Plug & Play features and under-the-hood expansion possibilities. PyTorch-Ignite aims to improve the deep learning community's technical skills by promoting best practices. \nThings are not hidden behind a divine tool that does everything, but remain within the reach of users.\nPyTorch-Ignite takes a \"Do-It-Yourself\" approach as research is unpredictable and it is important to capture its requirements without blocking things.\n\n\n\n\n\n\n\n\ud83d\udd25 PyTorch + Ignite \ud83d\udd25\u00b6PyTorch-Ignite wraps native PyTorch abstractions such as Modules, Optimizers, and DataLoaders in thin abstractions which allow your models to be separated from their training framework completely. This is achieved by a way of inverting control using an abstraction known as the Engine. The Engine is responsible for running an arbitrary function - typically a training or evaluation function - and emitting events along the way.\nA built-in event system represented by the Events class ensures Engine's flexibility, thus facilitating interaction on each step of the run.\nWith this approach users can completely customize the flow of events during the run.\nIn summary, PyTorch-Ignite is\n\nExtremely simple engine and event system = Training loop abstraction\nOut-of-the-box metrics to easily evaluate models\nBuilt-in handlers to compose training pipelines, save artifacts and log parameters and metrics\n\nAdditional benefits of using PyTorch-Ignite are\n\nLess code than pure PyTorch while ensuring maximum control and simplicity\nMore modular code\n\n\n\n\n\nPyTorch-Ignite\n\n\n\n    \n\n\n    \n\n\n    \n\n\n    \n\n\n    \n\n\n    \n\n\n    \n\n\n\nPure PyTorch\n\n\n\n\n\n\n\n\n\n\n\n\nAbout the design of PyTorch-Ignite\u00b6PyTorch-Ignite allows you to compose your application without being focused on a super multi-purpose object, but rather on weakly coupled components allowing advanced customization.\nThe design of the library is guided by:\n\nAnticipating new software or use-cases to come in in the future without centralizing everything in a single class.\nAvoiding configurations with a ton of parameters that are complicated to manage and maintain.\nProviding tools targeted to maximizing cohesion and minimizing coupling.\nKeeping it simple.\n\n\n\n\n\n\n\n\nQuick-start example\u00b6In this section we will use PyTorch-Ignite to build and train a classifier of the well-known MNIST dataset. This simple example will introduce the principal concepts behind PyTorch-Ignite.\nFor additional information and details about the API, please, refer to the project's documentation.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n!pip install pytorch-ignite\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nCollecting pytorch-ignite\n  Downloading https://files.pythonhosted.org/packages/c0/8e/08569347023611e40e62a14162024ca6238d42cb528b2302f84d662a2033/pytorch_ignite-0.4.1-py2.py3-none-any.whl (166kB)\n     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 174kB 8.4MB/s eta 0:00:01\nRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch-ignite) (1.6.0+cu101)\nRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<2,>=1.3->pytorch-ignite) (0.16.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch<2,>=1.3->pytorch-ignite) (1.18.5)\nInstalling collected packages: pytorch-ignite\nSuccessfully installed pytorch-ignite-0.4.1\n\n\n\n\n\n\n\n\n\n\n\nCommon PyTorch code\u00b6First, we define our model, training and validation datasets, optimizer and loss function:\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import SGD\nfrom torch.utils.data import DataLoader\n\nfrom torchvision.transforms import Compose, ToTensor, Normalize\nfrom torchvision.datasets import MNIST\n\n# transform to normalize the data\ntransform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n\n# Download and load the training data\ntrainset = MNIST(\"data\", download=True, train=True, transform=transform)\ntrain_loader = DataLoader(trainset, batch_size=128, shuffle=True)\n\n# Download and load the test data\nvalidationset = MNIST(\"data\", train=False, transform=transform)\nval_loader = DataLoader(validationset, batch_size=256, shuffle=False)\n\n# Define a class of CNN model (as you want)\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=-1)\n\ndevice = \"cuda\"\n\n# Define a model on move it on CUDA device\nmodel = Net().to(device)\n\n# Define a NLL loss\ncriterion = nn.NLLLoss()\n\n# Define a SGD optimizer\noptimizer = SGD(model.parameters(), lr=0.01, momentum=0.8)\n\n\n    \n\n\n\n\n\n\n\nThe above code is pure PyTorch and is typically user-defined and is required for any pipeline.\nTrainer and evaluator's setup\u00b6model's trainer is an engine that loops multiple times over the training dataset and updates model parameters. Let's see how we define such a trainer using PyTorch-Ignite. To do this, PyTorch-Ignite introduces the generic class Engine that is an abstraction that loops over the provided data, executes a processing function and returns a result. The only argument needed to construct the trainer is a train_step function.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \nfrom ignite.engine import Engine\n\ndef train_step(engine, batch):\n    x, y = batch\n    x = x.to(device)\n    y = y.to(device)\n\n    model.train()\n    y_pred = model(x)\n    loss = criterion(y_pred, y)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    return loss\n\n# Define a trainer engine\ntrainer = Engine(train_step)\n\n\n    \n\n\n\n\n\n\n\nPlease note that train_step function must accept engine and batch arguments. In the example above, engine is not used inside train_step, but we can easily imagine a use-case where we would like to fetch certain information like current iteration, epoch or custom variables from the engine.\nSimilarly, model evaluation can be done with an engine that runs a single time over the validation dataset and computes metrics.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \ndef validation_step(engine, batch):\n    model.eval()\n    with torch.no_grad():\n        x, y = batch[0], batch[1]\n        x = x.to(\"cuda\")\n        y = y.to(\"cuda\")\n\n        y_pred = model(x)\n\n        return y_pred, y\n\nevaluator = Engine(validation_step)\n\n\n    \n\n\n\n\n\n\n\nThis allows the construction of training logic from the simplest to the most complicated scenarios.\nThe type of output of the process functions (i.e. loss or y_pred, y in the above examples) is not restricted. These functions can return everything the user wants. Output is set to an engine's internal object engine.state.output and can be used further for any type of processing.\nEvents and Handers\u00b6To improve the engine\u2019s flexibility, a configurable event system is introduced to facilitate the interaction on each step of the run. Namely, Engine allows to add handlers on various Events that are triggered during the run. When an event is triggered, attached handlers (named functions, lambdas, class functions) are executed. Here is a schema for when built-in events are triggered by default:\n\nfire_event(Events.STARTED)\nwhile epoch < max_epochs:\n    fire_event(Events.EPOCH_STARTED)\n    # run once on data\n    for batch in data:\n        fire_event(Events.ITERATION_STARTED)\n\n        output = process_function(batch)\n\n        fire_event(Events.ITERATION_COMPLETED)\n    fire_event(Events.EPOCH_COMPLETED)\nfire_event(Events.COMPLETED)\nNote that each engine (i.e. trainer and evaluator) has its own event system which allows to define its own engine's process logic.\nUsing Events and handlers, it is possible to completely customize the engine's runs in a very intuitive way:\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \nfrom ignite.engine import Events\n\n# Show a message when the training begins\n@trainer.on(Events.STARTED)\ndef start_message():\n    print(\"Start training!\")\n\n# Handler can be want you want, here a lambda ! \ntrainer.add_event_handler(\n    Events.COMPLETED, \n    lambda _: print(\"Training completed!\")\n)\n\n# Run evaluator on val_loader every trainer's epoch completed\n@trainer.on(Events.EPOCH_COMPLETED)\ndef run_validation():\n    evaluator.run(val_loader)\n\n\n    \n\n\n\n\n\n\n\nIn the code above, the run_validation function is attached to the trainer and will be triggered at each completed epoch to launch model's validation with evaluator. This shows that engines can be embedded to create complex pipelines.\nHandlers offer unparalleled flexibility compared to callbacks as they can be any function: e.g., a lambda, a simple function, a class method, etc. Thus, we do not require to inherit from an interface and override its abstract methods which could unnecessarily bulk up your code and its complexity.\nThe possibilities of customization are endless as Pytorch-Ignite allows you to get hold of your application workflow. As mentioned before, there is no magic nor fully automatated things in PyTorch-Ignite.\nModel evaluation metrics\u00b6Metrics are another nice example of what the handlers for PyTorch-Ignite are and how to use them. In our example, we use the built-in metrics Accuracy and Loss.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \nfrom ignite.metrics import Accuracy, Loss\n\n# Accuracy and loss metrics are defined\nval_metrics = {\n  \"accuracy\": Accuracy(),\n  \"loss\": Loss(criterion)\n}\n\n# Attach metrics to the evaluator\nfor name, metric in val_metrics.items():\n    metric.attach(evaluator, name)\n\n\n    \n\n\n\n\n\n\n\nPyTorch-Ignite metrics can be elegantly combined with each other.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \nfrom ignite.metrics import Precision, Recall\n\n# Build F1 score\nprecision = Precision(average=False)\nrecall = Recall(average=False)\nF1 = (precision * recall * 2 / (precision + recall)).mean()\n\n# and attach it to evaluator\nF1.attach(evaluator, \"f1\")\n\n\n    \n\n\n\n\n\n\n\nTo make general things even easier, helper methods are available for the creation of a supervised Engine as above. Thus, let's define another evaluator applied to the training dataset in this way.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \nfrom ignite.engine import create_supervised_evaluator\n\n# Define another evaluator with default validation function and attach metrics\ntrain_evaluator = create_supervised_evaluator(model, metrics=val_metrics, device=\"cuda\")\n\n# Run train_evaluator on train_loader every trainer's epoch completed\n@trainer.on(Events.EPOCH_COMPLETED)\ndef run_train_validation():\n    train_evaluator.run(train_loader)\n\n\n    \n\n\n\n\n\n\n\nThe reason why we want to have two separate evaluators (evaluator and train_evaluator) is that they can have different attached handlers and logic to perform. For example, if we would like store the best model defined by the validation metric value, this role is delegated to evaluator which computes metrics over the validation dataset.\n\n\n\n\n\n\n\nCommon training handlers\u00b6From now on, we have trainer which will call evaluators evaluator and train_evaluator at every completed epoch. Thus, each evaluator will run and compute corresponding metrics. In addition, it would be very helpful to have a display of the results that shows those metrics.\nUsing the customization potential of the engine's system, we can add simple handlers for this logging purpose:\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n@evaluator.on(Events.COMPLETED)\ndef log_validation_results():\n    metrics = evaluator.state.metrics\n    print(\"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f} Avg F1: {:.2f}\"\n          .format(trainer.state.epoch, metrics[\"accuracy\"], metrics[\"loss\"], metrics[\"f1\"]))\n    \n@train_evaluator.on(Events.COMPLETED)\ndef log_train_results():\n    metrics = train_evaluator.state.metrics\n    print(\"  Training Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\n          .format(trainer.state.epoch, metrics[\"accuracy\"], metrics[\"loss\"]))\n\n\n    \n\n\n\n\n\n\n\nHere we attached log_validation_results and log_train_results handlers on Events.COMPLETED since evaluator and train_evaluator will run a single epoch over the validation datasets.\nLet's see how to add some others helpful features to our application.\n\nPyTorch-Ignite provides a ProgressBar handler to show an engine's progression.\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \nfrom ignite.contrib.handlers import ProgressBar\n\nProgressBar().attach(trainer, output_transform=lambda x: {'batch loss': x})\n\n\n    \n\n\n\n\n\n\n\n\nModelCheckpoint handler can be used to periodically save objects which have an attribute state_dict.\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \nfrom ignite.handlers import ModelCheckpoint, global_step_from_engine\n\n# Score function to select relevant metric, here f1\ndef score_function(engine):\n    return engine.state.metrics[\"f1\"]\n\n# Checkpoint to store n_saved best models wrt score function\nmodel_checkpoint = ModelCheckpoint(\n    \"quick-start-mnist-output\",\n    n_saved=2,\n    filename_prefix=\"best\",\n    score_function=score_function,\n    score_name=\"f1\",\n    global_step_transform=global_step_from_engine(trainer),\n)\n  \n# Save the model (if relevant) every epoch completed of evaluator\nevaluator.add_event_handler(Events.COMPLETED, model_checkpoint, {\"model\": model});\n\n\n    \n\n\n\n\n\n\n\n\nPyTorch-Ignite provides wrappers to modern tools to track experiments. For example, TensorBoardLogger handler allows to log metric results, model's and optimizer's parameters, gradients, and more during the training and validation for TensorBoard.\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \nfrom ignite.contrib.handlers import TensorboardLogger\n\n# Define a Tensorboard logger\ntb_logger = TensorboardLogger(log_dir=\"quick-start-mnist-output\")\n\n# Attach handler to plot trainer's loss every 100 iterations\ntb_logger.attach_output_handler(\n    trainer,\n    event_name=Events.ITERATION_COMPLETED(every=100),\n    tag=\"training\",\n    output_transform=lambda loss: {\"batchloss\": loss},\n)\n\n# Attach handler to dump evaluator's metrics every epoch completed\nfor tag, evaluator in [(\"training\", train_evaluator), (\"validation\", evaluator)]:\n    tb_logger.attach_output_handler(\n        evaluator,\n        event_name=Events.EPOCH_COMPLETED,\n        tag=tag,\n        metric_names=\"all\",\n        global_step_transform=global_step_from_engine(trainer),\n    )\n\n\n    \n\n\n\n\n\n\n\nIt is possible to extend the use of the TensorBoard logger very simply by integrating user-defined functions. For example, here is how to display images and predictions during training:\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \nimport matplotlib.pyplot as plt\n\n# Store predictions and scores using matplotlib\ndef predictions_gt_images_handler(engine, logger, *args, **kwargs):\n    x, _ = engine.state.batch\n    y_pred, y = engine.state.output\n    # y_pred is log softmax value\n    num_x = num_y = 8\n    le = num_x * num_y\n    probs, preds = torch.max(torch.exp(y_pred[:le]), dim=1)\n    fig = plt.figure(figsize=(20, 20))\n    for idx in range(le):\n        ax = fig.add_subplot(num_x, num_y, idx + 1, xticks=[], yticks=[])\n        ax.imshow(x[idx].squeeze(), cmap=\"Greys\")\n        ax.set_title(\"{0} {1:.1f}% (label: {2})\".format(\n            preds[idx],\n            probs[idx] * 100.0,\n            y[idx]),\n            color=(\"green\" if preds[idx] == y[idx] else \"red\")\n        )\n    logger.writer.add_figure('predictions vs actuals', figure=fig, global_step=trainer.state.epoch)\n\n# Attach custom function to evaluator at first iteration\ntb_logger.attach(\n    evaluator,\n    log_handler=predictions_gt_images_handler,\n    event_name=Events.ITERATION_COMPLETED(once=1),\n);\n\n\n    \n\n\n\n\n\n\n\nAll that is left to do now is to  run the trainer on data from train_loader for a number of epochs.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \ntrainer.run(train_loader, max_epochs=5)\n\n# Once everything is done, let's close the logger\ntb_logger.close()\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nStart training!\n\n\n\n\n\n\n    \n\n\n\n\n\n \n \n\n\n\nvar element = $('#c39b08bc-cc5d-4841-a075-6bdcaef746c0');\n\n\n{\"model_id\": \"9cf4a3960d9c4c31a775cf19c00fddd2\", \"version_major\": 2, \"version_minor\": 0}\n\n\n\n\n\n\n\n    \n\n\n\nValidation Results - Epoch: 1  Avg accuracy: 0.94 Avg loss: 0.20 Avg F1: 0.94\n  Training Results - Epoch: 1  Avg accuracy: 0.94 Avg loss: 0.21\n\n\n\n\n\n\n    \n\n\n\n\n\n \n \n\n\n\nvar element = $('#45285118-ff6d-4a0c-b1bf-7049360897c1');\n\n\n{\"model_id\": \"435204a5d8294b9faf37e86a2a2c6106\", \"version_major\": 2, \"version_minor\": 0}\n\n\n\n\n\n\n\n    \n\n\n\nValidation Results - Epoch: 2  Avg accuracy: 0.96 Avg loss: 0.12 Avg F1: 0.96\n  Training Results - Epoch: 2  Avg accuracy: 0.96 Avg loss: 0.13\n\n\n\n\n\n\n    \n\n\n\n\n\n \n \n\n\n\nvar element = $('#ccdd4b42-be1c-4959-93fa-7f7f0adc8067');\n\n\n{\"model_id\": \"2bf732207a604791b52f75e2c7acde77\", \"version_major\": 2, \"version_minor\": 0}\n\n\n\n\n\n\n\n    \n\n\n\nValidation Results - Epoch: 3  Avg accuracy: 0.97 Avg loss: 0.10 Avg F1: 0.97\n  Training Results - Epoch: 3  Avg accuracy: 0.97 Avg loss: 0.10\n\n\n\n\n\n\n    \n\n\n\n\n\n \n \n\n\n\nvar element = $('#b0287067-495f-4bc0-b9cf-5c05f42a5d54');\n\n\n{\"model_id\": \"a4ee6eaef1b34de3a18a48008b0f574b\", \"version_major\": 2, \"version_minor\": 0}\n\n\n\n\n\n\n\n    \n\n\n\nValidation Results - Epoch: 4  Avg accuracy: 0.98 Avg loss: 0.07 Avg F1: 0.98\n  Training Results - Epoch: 4  Avg accuracy: 0.97 Avg loss: 0.09\n\n\n\n\n\n\n    \n\n\n\n\n\n \n \n\n\n\nvar element = $('#ed00100c-523f-4cae-9aa5-f8617da7c2d9');\n\n\n{\"model_id\": \"4be1c90dbd704ebfb5291fdd2ca25ca9\", \"version_major\": 2, \"version_minor\": 0}\n\n\n\n\n\n\n\n    \n\n\n\nValidation Results - Epoch: 5  Avg accuracy: 0.98 Avg loss: 0.07 Avg F1: 0.98\n  Training Results - Epoch: 5  Avg accuracy: 0.98 Avg loss: 0.08\nTraining completed!\n\n\n\n\n\n\n\n\n\n\n\nWe can inspect results using tensorboard. We can observe two tabs \"Scalars\" and \"Images\".\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n%load_ext tensorboard\n\n%tensorboard --logdir=.\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\nvar element = $('#ce6ae335-f0cc-42f2-b4fd-784f47b68ad3');\n\n        (async () => {\n            const url = await google.colab.kernel.proxyPort(6006, {\"cache\": true});\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 takeaways\u00b6\nAlmost any training logic can be coded as a train_step method and a trainer built using this method.\n\nThe essence of the library is the Engine class that loops a given number of times over a dataset and executes a processing function.\n\nA highly customizable event system simplifies interaction with the engine on each step of the run.\n\nPyTorch-Ignite provides a set of built-in handlers and metrics for common tasks.\n\nPyTorch-Ignite is easy to extend.\n\n\n\n\n\n\n\n\n\nAdvanced features\u00b6In this section we would like to present some advanced features of PyTorch-Ignite for experienced users. We will cover events, handlers and metrics in more detail, as well as distributed computations on GPUs and TPUs. Feel free to skip this section now and come back later if you are a beginner.\n\n\n\n\n\n\n\nPower of Events & Handlers\u00b6We have seen throughout the quick-start example that events and handlers are perfect to execute any number of functions whenever you wish. In addition to that we provide several ways to extend it even more by\n\nBuilt-in events filtering\nStacking events to share the action\nAdding custom events to go beyond built-in standard events\n\nLet's look at these features in more detail.\n\n\n\n\n\n\n\nBuilt-in events filtering\u00b6Users can simply filter out events to skip triggering the handler. Let's create a dummy trainer:\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \nfrom ignite.engine import Engine, Events\n\n\ntrainer = Engine(lambda e, batch: None)\n\n\n    \n\n\n\n\n\n\n\nLet's consider a use-case where we would like to train a model and periodically run its validation on several development datasets, e.g. devset1 and devset2:\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# We run the validation on devset1 every 5 epochs\n@trainer.on(Events.EPOCH_COMPLETED(every=5))\ndef run_validation1():\n    print(\"Epoch {}: Validation on devset 1\".format(trainer.state.epoch))\n    # evaluator.run(devset1)  # commented out for demo purposes\n\n# We run another validation on devset2 every 10 epochs\n@trainer.on(Events.EPOCH_COMPLETED(every=10))\ndef run_validation2():\n    print(\"Epoch {}: Validation on devset 2\".format(trainer.state.epoch))\n    # evaluator.run(devset2)  # commented out for demo purposes\n\ntrain_data = [0, 1, 2, 3, 4]\ntrainer.run(train_data, max_epochs=50);\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nEpoch 5: Validation on devset 1\nEpoch 10: Validation on devset 1\nEpoch 10: Validation on devset 2\nEpoch 15: Validation on devset 1\nEpoch 20: Validation on devset 1\nEpoch 20: Validation on devset 2\nEpoch 25: Validation on devset 1\nEpoch 30: Validation on devset 1\nEpoch 30: Validation on devset 2\nEpoch 35: Validation on devset 1\nEpoch 40: Validation on devset 1\nEpoch 40: Validation on devset 2\nEpoch 45: Validation on devset 1\nEpoch 50: Validation on devset 1\nEpoch 50: Validation on devset 2\n\n\n\n\n\n\n\n\n\n\n\nLet's now consider another situation where we would like to make a single change once we reach a certain epoch or iteration. For example, let's change the training dataset on the 5-th epoch from low resolution images to high resolution images:\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \ndef train_step(e, batch):\n    print(\"Epoch {} - {} : batch={}\".format(e.state.epoch, e.state.iteration, batch))\n\ntrainer = Engine(train_step)\n\nsmall_res_data = [0, 1, 2, ]\nhigh_res_data = [10, 11, 12]\n\n# We run the following handler once on 5-th epoch started\n@trainer.on(Events.EPOCH_STARTED(once=5))\ndef change_train_dataset():\n    print(\"Epoch {}: Change training dataset\".format(trainer.state.epoch))\n    trainer.set_data(high_res_data)\n\ntrainer.run(small_res_data, max_epochs=10);\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nEpoch 1 - 1 : batch=0\nEpoch 1 - 2 : batch=1\nEpoch 1 - 3 : batch=2\nEpoch 2 - 4 : batch=0\nEpoch 2 - 5 : batch=1\nEpoch 2 - 6 : batch=2\nEpoch 3 - 7 : batch=0\nEpoch 3 - 8 : batch=1\nEpoch 3 - 9 : batch=2\nEpoch 4 - 10 : batch=0\nEpoch 4 - 11 : batch=1\nEpoch 4 - 12 : batch=2\nEpoch 5: Change training dataset\nEpoch 5 - 13 : batch=10\nEpoch 5 - 14 : batch=11\nEpoch 5 - 15 : batch=12\nEpoch 6 - 16 : batch=10\nEpoch 6 - 17 : batch=11\nEpoch 6 - 18 : batch=12\nEpoch 7 - 19 : batch=10\nEpoch 7 - 20 : batch=11\nEpoch 7 - 21 : batch=12\nEpoch 8 - 22 : batch=10\nEpoch 8 - 23 : batch=11\nEpoch 8 - 24 : batch=12\nEpoch 9 - 25 : batch=10\nEpoch 9 - 26 : batch=11\nEpoch 9 - 27 : batch=12\nEpoch 10 - 28 : batch=10\nEpoch 10 - 29 : batch=11\nEpoch 10 - 30 : batch=12\n\n\n\n\n\n\n\n\n\n\n\nLet's now consider another situation where we would like to trigger a handler with completely custom logic. For example, we would like to dump model gradients if the training loss satisfies a certain condition:\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n# Let's predefine for simplicity training losses\ntrain_losses = [2.0, 1.9, 1.7, 1.5, 1.6, 1.2, 0.9, 0.8, 1.0, 0.8, 0.7, 0.4, 0.2, 0.1, 0.1, 0.01]\n\ntrainer = Engine(lambda e, batch: train_losses[e.state.iteration - 1])\n\n# We define our custom logic when to execute a handler\ndef custom_event_filter(trainer, event):\n    if 0.1 < trainer.state.output < 1.0:\n        return True\n    return False\n\n# We run the following handler every iteration completed under our custom_event_filter condition:\n@trainer.on(Events.ITERATION_COMPLETED(event_filter=custom_event_filter))\ndef dump_model_grads():\n    print(\"{} - loss={}: dump model grads\".format(trainer.state.iteration, trainer.state.output))\n\ntrain_data = [0, ]\ntrainer.run(train_data, max_epochs=len(train_losses));\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n7 - loss=0.9: dump model grads\n8 - loss=0.8: dump model grads\n10 - loss=0.8: dump model grads\n11 - loss=0.7: dump model grads\n12 - loss=0.4: dump model grads\n13 - loss=0.2: dump model grads\n\n\n\n\n\n\n\n\n\n\n\nStack events to share the action\u00b6A user can trigger the same handler on events of differen types. For example, let's run a handler for model's validation every 3 epochs and when the training is completed:\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \ntrainer = Engine(lambda e, batch: None)\n\n@trainer.on(Events.EPOCH_COMPLETED(every=3) | Events.COMPLETED)\ndef run_validation():\n    print(\"Epoch {} - event={}: Validation\".format(trainer.state.epoch, trainer.last_event_name))\n    # evaluator.run(devset)\n\ntrain_data = [0, 1, 2, 3, 4]\ntrainer.run(train_data, max_epochs=20);\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nEpoch 3 - event=epoch_completed: Validation\nEpoch 6 - event=epoch_completed: Validation\nEpoch 9 - event=epoch_completed: Validation\nEpoch 12 - event=epoch_completed: Validation\nEpoch 15 - event=epoch_completed: Validation\nEpoch 18 - event=epoch_completed: Validation\nEpoch 20 - event=completed: Validation\n\n\n\n\n\n\n\n\n\n\n\nAdd custom events\u00b6A user can add their own events to go beyond built-in standard events. For example,\nlet's define new events related to backward and optimizer step calls. This can help us to attach specific handlers on these events in a configurable manner.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \nfrom ignite.engine import EventEnum\n\n\nclass BackpropEvents(EventEnum):\n    BACKWARD_STARTED = 'backward_started'\n    BACKWARD_COMPLETED = 'backward_completed'\n    OPTIM_STEP_COMPLETED = 'optim_step_completed'\n\n\ndef update(engine, batch):\n    # ...\n    # loss = criterion(y_pred, y)\n    engine.fire_event(BackpropEvents.BACKWARD_STARTED)\n    # loss.backward()\n    engine.fire_event(BackpropEvents.BACKWARD_COMPLETED)\n    # optimizer.step()\n    engine.fire_event(BackpropEvents.OPTIM_STEP_COMPLETED)\n    # ...    \n\ntrainer = Engine(update)\ntrainer.register_events(*BackpropEvents)\n\ndef function_before_backprop():\n    print(\"{} - before backprop\".format(trainer.state.iteration))\n\ntrainer.add_event_handler(BackpropEvents.BACKWARD_STARTED, function_before_backprop)\n\ndef function_after_backprop():\n    print(\"{} - after backprop\".format(trainer.state.iteration))\n\ntrainer.add_event_handler(BackpropEvents.BACKWARD_COMPLETED, function_after_backprop)\n\ntrain_data = [0, 1, 2, 3, 4]\ntrainer.run(train_data, max_epochs=2);\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n1 - before backprop\n1 - after backprop\n2 - before backprop\n2 - after backprop\n3 - before backprop\n3 - after backprop\n4 - before backprop\n4 - after backprop\n5 - before backprop\n5 - after backprop\n6 - before backprop\n6 - after backprop\n7 - before backprop\n7 - after backprop\n8 - before backprop\n8 - after backprop\n9 - before backprop\n9 - after backprop\n10 - before backprop\n10 - after backprop\n\n\n\n\n\n\n\n\n\n\n\nOut-of-the-box metrics\u00b6PyTorch-Ignite provides an ensemble of metrics dedicated to many Deep Learning tasks (classification, regression, segmentation, etc.). Most of these metrics provide a way to compute various quantities of interest in an online fashion without having to store the entire output history of a model.\n\nFor classification : Precision, Recall, Accuracy, ConfusionMatrix and more!\nFor segmentation : DiceCoefficient, IoU, mIOU and more!\n~20 regression metrics, e.g. MSE, MAE, MedianAbsoluteError, etc \nMetrics that store the entire output history per epoch\nPossible to use with scikit-learn metrics, e.g. EpochMetric, AveragePrecision, ROC_AUC, etc\n\n\nEasily composable to assemble a custom metric\nEasily extendable to create custom metrics\n\nComplete lists of metrics provided by PyTorch-Ignite can be found here for ignite.metrics and here for ignite.contrib.metrics.\nTwo kinds of public APIs are provided:\n\nmetric is attached to Engine\nmetric's reset, update, compute methods \n\n\n\n\n\n\n\n\nMore on the reset, update, compute public API\u00b6Let's demonstrate this API on a simple example using the Accuracy metric. The idea behind this API is that we accumulate internally certain counters on each update call. The metric's value is computed on each compute call and counters are reset on each reset call.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \nimport torch\nfrom ignite.metrics import Accuracy\n\nacc = Accuracy()\n\n# Start accumulation\nacc.reset()\n\ny_target = torch.tensor([0, 1, 2, 1,])\n# y_pred is logits computed by the model\ny_pred = torch.tensor([\n    [10.0, 0.1, -1.0],  # correct \n    [2.0, -1.0, -2.0],  # incorrect\n    [1.0, -1.0, 4.0],   # correct\n    [0.0, 5.0, -1.0],   # correct\n])\nacc.update((y_pred, y_target))\n\n# Compute accuracy on 4 samples\nprint(\"After 1st update, accuracy=\", acc.compute())\n\ny_target = torch.tensor([1, 2, 0, 2])\n# y_pred is logits computed by the model\ny_pred = torch.tensor([\n    [2.0, 1.0, -1.0],   # incorrect\n    [0.0, 1.0, -2.0],   # incorrect\n    [2.6, 1.0, -4.0],   # correct\n    [1.0, -3.0, 2.0],   # correct\n])\nacc.update((y_pred, y_target))\n\n# Compute accuracy on 8 samples\nprint(\"After 2nd update, accuracy=\", acc.compute())\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nAfter 1st update, accuracy= 0.75\nAfter 2nd update, accuracy= 0.625\n\n\n\n\n\n\n\n\n\n\n\nComposable metrics\u00b6Users can compose their own metrics with ease from existing ones using arithmetic operations or PyTorch methods. For example, an error metric defined as 100 * (1.0 - accuracy) can be coded in a straightforward manner:\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \nimport torch\nfrom ignite.metrics import Accuracy\n\nacc = Accuracy()\nerror = 100.0 * (1.0 - acc) \n\n# Start accumulation\nacc.reset()\n\ny_target = torch.tensor([0, 1, 2, 1,])\n# y_pred is logits computed by the model\ny_pred = torch.tensor([\n    [10.0, 0.1, -1.0],  # correct \n    [2.0, -1.0, -2.0],  # incorrect\n    [1.0, -1.0, 4.0],   # correct\n    [0.0, 5.0, -1.0],   # correct\n])\nacc.update((y_pred, y_target))\n\n# Compute error on 4 samples\nprint(\"After 1st update, error=\", error.compute())\n\ny_target = torch.tensor([1, 2, 0, 2])\n# y_pred is logits computed by the model\ny_pred = torch.tensor([\n    [2.0, 1.0, -1.0],   # incorrect\n    [0.0, 1.0, -2.0],   # incorrect\n    [2.6, 1.0, -4.0],   # correct\n    [1.0, -3.0, 2.0],   # correct\n])\nacc.update((y_pred, y_target))\n\n# Compute err on 8 samples\nprint(\"After 2nd update, error=\", error.compute())\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nAfter 1st update, error= 25.0\nAfter 2nd update, error= 37.5\n\n\n\n\n\n\n\n\n\n\n\nIn case a custom metric can not be expressed as arithmetic operations of base metrics, please follow this guide to implement the custom metric.\n\n\n\n\n\n\n\nOut-of-the-box handlers\u00b6PyTorch-Ignite provides various commonly used handlers to simplify \napplication code:\n\nCommon training handlers: Checkpoint, EarlyStopping, Timer, TerminateOnNan\nOptimizer's parameter scheduling (learning rate, momentum, etc.)\nconcatenate schedulers, add warm-up, cyclical scheduling, piecewise-linear scheduling, and more! See examples.\n\n\nTime profiling\nLogging to experiment tracking systems:\nTensorboard, Visdom, MLflow, Polyaxon, Neptune, Trains, etc.\n\n\n\nComplete lists of handlers provided by PyTorch-Ignite can be found here for ignite.handlers and here for ignite.contrib.handlers.\n\n\n\n\n\n\n\nCommon training handlers\u00b6With the out-of-the-box Checkpoint handler, a user can easily save the training state or best models to the filesystem or a cloud.\nEarlyStopping and TerminateOnNan helps to stop the training if overfitting or diverging.\nAll those things can be easily added to the trainer one by one or with helper methods.\nLet's consider an example of using helper methods.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom ignite.engine import create_supervised_trainer, create_supervised_evaluator, Events\nfrom ignite.metrics import Accuracy\nimport ignite.contrib.engines.common as common\n\ntrain_data = [[torch.rand(2, 4), torch.randint(0, 5, size=(2, ))] for _ in range(10)]\nval_data = [[torch.rand(2, 4), torch.randint(0, 5, size=(2, ))] for _ in range(10)]\nepoch_length = len(train_data)\n\nmodel = nn.Linear(4, 5)\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n# step_size is expressed in iterations\nlr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=epoch_length, gamma=0.88)\n\n# Let's define some dummy trainer and evaluator\ntrainer = create_supervised_trainer(model, optimizer, nn.CrossEntropyLoss())\nevaluator = create_supervised_evaluator(model, metrics={\"accuracy\": Accuracy()})\n\n\n@trainer.on(Events.EPOCH_COMPLETED)\ndef run_validation():\n    evaluator.run(val_data)\n\n# training state to save\nto_save = {\n    \"trainer\": trainer, \"model\": model,\n    \"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler\n}\nmetric_names = [\"batch loss\", ]\n\ncommon.setup_common_training_handlers(\n    trainer=trainer,\n    to_save=to_save,\n    output_path=\"checkpoints\",\n    save_every_iters=epoch_length,\n    lr_scheduler=lr_scheduler,\n    output_names=metric_names,\n    with_pbars=True,\n)\n\ntb_logger = common.setup_tb_logging(\"tb_logs\", trainer, optimizer, evaluators=evaluator)\n\ncommon.save_best_model_by_val_score(\n    \"best_models\",\n    evaluator=evaluator,\n    model=model,\n    metric_name=\"accuracy\",\n    n_saved=2,\n    trainer=trainer,\n    tag=\"val\",\n)\n\ntrainer.run(train_data, max_epochs=5)\n\ntb_logger.close()\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n \n \n\n\n\nvar element = $('#015444a9-dd08-44fa-adee-c77c34069c56');\n\n\n{\"model_id\": \"ef6ae183108243b8b758fe7f4f05fe79\", \"version_major\": 2, \"version_minor\": 0}\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n!ls -all \"checkpoints\"\n!ls -all \"best_models\"\n!ls -all \"tb_logs\"\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\ntotal 12\ndrwxr-xr-x 2 root root 4096 Aug 31 11:27 .\ndrwxr-xr-x 1 root root 4096 Aug 31 11:27 ..\n-rw------- 1 root root 1657 Aug 31 11:27 training_checkpoint_50.pt\ntotal 16\ndrwxr-xr-x 2 root root 4096 Aug 31 11:27  .\ndrwxr-xr-x 1 root root 4096 Aug 31 11:27  ..\n-rw------- 1 root root 1145 Aug 31 11:27 'best_model_2_val_accuracy=0.3000.pt'\n-rw------- 1 root root 1145 Aug 31 11:27 'best_model_3_val_accuracy=0.3000.pt'\ntotal 12\ndrwxr-xr-x 2 root root 4096 Aug 31 11:27 .\ndrwxr-xr-x 1 root root 4096 Aug 31 11:27 ..\n-rw-r--r-- 1 root root  325 Aug 31 11:27 events.out.tfevents.1598873224.3aa7adc24d3d.115.1\n\n\n\n\n\n\n\n\n\n\n\nIn the above code, the common.setup_common_training_handlers method adds TerminateOnNan, adds a handler to use lr_scheduler (expressed in iterations), adds training state checkpointing, exposes batch loss output as exponential moving averaged metric for logging, and adds a progress bar to the trainer.\nNext, the common.setup_tb_logging method returns a TensorBoard logger which is automatically configured to log trainer's metrics (i.e. batch loss), optimizer's learning rate and evaluator's metrics.\nFinally, common.save_best_model_by_val_score sets up a handler to save the best two models according to the validation accuracy metric.\n\n\n\n\n\n\n\nDistributed and XLA device support\u00b6PyTorch offers a distributed communication package for writing and running parallel applications on multiple devices and machines. The native interface provides commonly used collective operations and allows to address multi-CPU and multi-GPU computations seamlessly using the torch DistributedDataParallel module and the well-known mpi, gloo and nccl backends. Recently, users can also run PyTorch on XLA devices, like TPUs, with the torch_xla package. However, writing distributed training code working on GPUs and TPUs is not a trivial task due to some API specificities. The purpose of the PyTorch-Ignite ignite.distributed package introduced in version 0.4 is to unify the code for native torch.distributed API, torch_xla API on XLA devices and also supporting other distributed frameworks (e.g. Horovod).\nTo make distributed configuration setup easier, the Parallel context manager has been introduced:\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \ncode = \"\"\"\nimport ignite.distributed as idist\n\ndef training(local_rank, config, **kwargs):\n    print(idist.get_rank(), ': run with config:', config, '- backend=', idist.backend())\n    # do the training ...\n  \nbackend = 'gloo' # or \"xla-tpu\" or None\ndist_configs = {'nproc_per_node': 2}  # or dist_configs = {...}\nconfig = {'c': 12345}\n\nif __name__ == '__main__':\n\n    with idist.Parallel(backend=backend, **dist_configs) as parallel:\n        parallel.run(training, config, a=1, b=2)\n\"\"\"\n!echo \"{code}\" > main.py\n!python main.py\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n2020-08-31 11:27:07,128 ignite.distributed.launcher.Parallel INFO: Initialized distributed launcher with backend: 'gloo'\n2020-08-31 11:27:07,128 ignite.distributed.launcher.Parallel INFO: - Parameters to spawn processes: \n\tnproc_per_node: 2\n\tnnodes: 1\n\tnode_rank: 0\n2020-08-31 11:27:07,128 ignite.distributed.launcher.Parallel INFO: Spawn function '<function training at 0x7f32b8ac9d08>' in 2 processes\n0 : run with config: {'c': 12345} - backend= gloo\n1 : run with config: {'c': 12345} - backend= gloo\n2020-08-31 11:27:09,959 ignite.distributed.launcher.Parallel INFO: End of run\n\n\n\n\n\n\n\n\n\n\n\nThe above code with a single modification can run on a GPU, single-node multiple GPUs, single or multiple TPUs etc. It can be executed with the torch.distributed.launch tool or by Python and spawning the required number of processes. For more details, see the documentation.\n\n\n\n\n\n\n\nIn addition, methods like auto_model(), auto_optim() and auto_dataloader() help to adapt in a transparent way the provided model, optimizer and data loaders to an existing configuration:\n# main.py\n\nimport ignite.distributed as idist\n\ndef training(local_rank, config, **kwargs):\n\n    print(idist.get_rank(), \": run with config:\", config, \"- backend=\", idist.backend())\n\n    train_loader = idist.auto_dataloader(dataset, batch_size=32, num_workers=12, shuffle=True, **kwargs)\n    # batch size, num_workers and sampler are automatically adapted to existing configuration\n    # ...\n    model = resnet50()\n    optimizer = optim.SGD(model.parameters(), lr=0.01)\n\n    # if training with Nvidia/Apex for Automatic Mixed Precision (AMP)\n    # model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)\n\n    model = idist.auto_model(model)\n    # model is DDP or DP or just itself according to existing configuration\n    # ...\n    optimizer = idist.auto_optim(optimizer)\n    # optimizer is itself, except XLA configuration and overrides `step()` method.\n    # User can safely call `optimizer.step()` (behind `xm.optimizer_step(optimizier)` is performed)\n\nbackend = \"nccl\"  # torch native distributed configuration on multiple GPUs\n# backend = \"xla-tpu\"  # XLA TPUs distributed configuration\n# backend = None  # no distributed configuration\nwith idist.Parallel(backend=backend, **dist_configs) as parallel:\n    parallel.run(training, config, a=1, b=2)\n\nPlease note that these auto_* methods are optional; a user is free use some of them and manually set up certain parts of the code if required. The advantage of this approach is that there is no under the hood inevitable objects' patching and overriding.\n\n\n\n\n\n\n\nMore details about distributed helpers provided by PyTorch-Ignite can be found in the documentation.\nA complete example of training on CIFAR10 can be found here.\nA detailed tutorial with distributed helpers will be published in another article.\n\n\n\n\n\n\n\nProjects using PyTorch-Ignite\u00b6There is a list of research papers with code, blog articles, tutorials, toolkits and other projects that are using PyTorch-Ignite. A detailed overview can be found here.\nTo start your project using PyTorch-Ignite is simple and can require only to pass through this quick-start example and library \"Concepts\".\nIn addition, PyTorch-Ignite also provides several tutorials:\n\nText Classification using Convolutional Neural\nNetworks \nVariational Auto\nEncoders \nConvolutional Neural Networks for Classifying Fashion-MNIST Dataset\nTraining Cycle-GAN on Horses to Zebras with Nvidia/Apex \nAnother training Cycle-GAN on Horses to Zebras with Native Torch CUDA AMP\nFinetuning EfficientNet-B0 on CIFAR100\nHyperparameters tuning with Ax \nBasic example of LR finder on MNIST\nBenchmark mixed precision training on Cifar100: torch.cuda.amp vs nvidia/apex \nMNIST training on a single TPU\nCIFAR10 Training on multiple TPUs\n\nand examples:\n\ncifar10 (single/multi-GPU, DDP, AMP, TPUs)\nbasic RL\nreproducible baselines for vision tasks:\nclassification on ImageNet (single/multi-GPU, DDP, AMP)\nsemantic segmentation on Pascal VOC2012 (single/multi-GPU, DDP, AMP)\n\n\n\nThe package can be installed with pip or conda. More info and guides can be found here.\n\n\n\n\n\n\n\nProject news\u00b6Instead of a conclusion, we will wrap up with some current project news:\n\n\n\n\n\n\ud83c\udf8a\ud83d\ude84 Trains Ignite server is open to everyone to browse our reproducible experiment logs, compare performances and restart any run on their own Trains server and associated infrastructure. Many thanks to the folks at Allegro AI who are making this possible!\n\n\n\n\ud83c\udf89\ud83c\udf8a Since June 2020, PyTorch-Ignite has joined NumFOCUS as an affiliated project as well as Quansight Labs. We believe that it will be a new step in our project\u2019s development, and in promoting open practices in research and industry.\n\n \ud83c\udf89 Hacktoberfest 2020 is the open-source coding festival for everyone to attend in October and PyTorch-Ignite is also preparing for it. Please, check out our announcement.\n\n\n\n\ud83c\udf89  We are pleased to announce that we will run a mentored sprint session to contribute to PyTorch-Ignite at PyData Global 2020. We are looking forward to seeing you in November at this event!\n\n\n\n\n.\\ | \ud83d\udcc8\ud83d\udcbb The project is currently maintained by a team of volunteers and we are looking for motivated contributors to help us to move the project forward. Please see the contribution guidelines for more information if this sounds interesting to you.\n\n  Check out the project on GitHub and follow us on Twitter. For any questions, support or issues, please reach out to us. For all other questions and inquiries, please send an email to contact@pytorch-ignite.ai\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Quansight Labs is a public-benefit division of Quansight created to provide a home for a \u201cPyData Core Team\u201d who create and maintain open-source technology around all aspects of scientific and data science workflows. PyTorch-Ignite being part of Labs benefits from Labs' community, supports PyTorch-Ignite's sustainability, and accelerates development of the project that users rely on.\n  IFP Energies nouvelles (IFPEN) is a major research and training player in the fields of energy, transport and the environment. Deep Learning approaches are currently carried out through different projects from high performance data analytics to numerical simulation and natural language processing. Contributing to PyTorch-Ignite is a way for IFPEN to develop and maintain its software skills and best practices at the highest technical level.\n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "Deep Learning,Labs,Machine Learning,Neural Networks,Python,PyTorch,Tutorial",
      "url": "https://labs.quansight.org/blog/2020/09/pytorch-ignite/"
    },
    {
      "title": "Traitlets - an introduction & use in Jupyter configuration management",
      "text": "You have probably seen Traitlets in applications, you likely even use it. The package has nearly 5 million downloads\non conda-forge alone.\nBut, what is Traitlets ?\nIn this post we'll answer this question along with where Traitlets came from, its applications, and a bit of history.\n\n\ntraitlets 5.0\ntraitlets 5.0 has recently been released; 5 years after the earliest versions of\ntraitlets 4.0. The latest and greatest 5.0 release brings\nnew features and a cleaner codebase while maintaining backward compatibility with 4.0.\nThis is a big upgrade to our interactive computing tools because Traitlets is used everywhere in Jupyter/IPython - for configuration, runtime type checking, widgets, and CLI parsing.\nTraitlets is a library that provides objects that\ncan expose individual configuration options in an intelligent way. They are used in almost all the Jupyter Projects.\nTraitlets began as a pure Python implementation of the Enthought Traits library.\nThese libraries implement the object-oriented trait pattern. Prior to 2015, traitlets was a part of the IPython (pre-jupyter) code base; then during \"The Big\nSplit\" it was moved to their own reusable package.\nBoth traitlets and traits addressed the challenge of using typed code in interactive Python REPLs. They offer type checking, coercion and validation at run time.\nTraitlets for development\nThe general idea when developing with Traitlets is:\n\n\nThe developer defines a class-level attribute,\n\n\nThis class level attribute will automatically be converted into\n    a property-like element with runtime type and value checking, configurability and\n    observable events and changes.\n\n\nTraitlets minimizes boilerplate for Python applications. Traitlets maintains a uniform\nnaming convention and helps your users configure their applications.\nA Traitlets usage example\nWe'll demonstrate the flexibility of Traitlets by configuring the autocall\ntraitlet/option of IPython from the command line interface, configuration\nfile, as well as observe effect of dynamically changing values.\nBelow is an excerpt of the IPython main class that defines\nIPython's autocall traitlet.\nfrom traitlets import SingletonConfigurable, Enum\n\nclass InteractiveShell(SingletonConfigurable):\n\n    ...\n\n    autocall = Enum((0,1,2), default_value=0, help=\n        \"\"\"\n        Make IPython automatically call any callable object even if you didn't\n        type explicit parentheses. For example, 'str 43' becomes 'str(43)'\n        automatically. The value can be '0' to disable the feature, '1' for\n        'smart' autocall, where it is not applied if there are no more\n        arguments on the line, and '2' for 'full' autocall, where all callable\n        objects are automatically called (even if no arguments are present).\n        \"\"\"\n    ).tag(config=True)\n\n    ...\n\n\n\nHere, the autocall class attribute will be converted at instantiation to an\ninstance property, in particular an Enum whose values are ensured to be\neither 0,1, or 2. Traitlets provides a number of utilities to decide\nwhether assigning incorrect values should raise an exception; or coerce to one\nof the valid ones. Here a wrong assignment will fail:\n$ ipython --no-banner\n\nIn [1]: ip = get_ipython()\n\nIn [2]: ip.autocall\nOut[2]: 0\n\nIn [3]: ip.autocall = 5\n...\nTraitError: The 'autocall' trait of a TerminalInteractiveShell instance expected any of [0, 1, 2], not the int 5.\n\n\n\nWhile type \u2013 and value \u2013 checking at runtime is a nice feature, most of these\noptions are usually user preferences. Traitlets provides a way to automatically\ncreate configuration files with help, as well as command line arguments\nparsing. The autocall traitlet has help= and default_value\nstrings that are tagged with config=True. This informs the current\napplication on how to automatically generate configuration files, decide on\nthe option name and document it. No need for the developer to spend time on\ndeciding on a configuration parameter name.\nA generated configuration file.\nOn a brand new machine with IPython installed you will find the following in\nyour default configuration file ~/.ipython/profile_default/ipython_config.py,\nreferring to our autocall traitlet of previous section:\n...\n## Make IPython automatically call any callable object even if you didn't type\n#  explicit parentheses. For example, 'str 43' becomes 'str(43)' automatically.\n#  The value can be '0' to disable the feature, '1' for 'smart' autocall, where\n#  it is not applied if there are no more arguments on the line, and '2' for\n#  'full' autocall, where all callable objects are automatically called (even if\n#  no arguments are present).\n#c.InteractiveShell.autocall = 0\n...\n\n\n\nYou will recognize there the help string provided before, as well as the default\nvalue.\nIf you update this file by uncommenting the last line and changing the value,\nas you would expect, new instances of InteractiveShell will be instantiated\nwith this new default value.\nAlternatively, Traitlets can also generate and parse command line arguments, so\nipython --InteractiveShell.autocall=3 will take precedence over\nconfiguration files and start IPython with this new option:\n$ ipython --no-banner --InteractiveShell.autocall=2\n\nIn [1]: get_ipython().autocall\nOut[1]: 2\n\n\n\nIt will display the same help if you try ipython --help-all:\n$ ipython --help-all\n...\n--InteractiveShell.autocall=<Enum>\n    Make IPython automatically call any callable object even if you didn't type\n    explicit parentheses. For example, 'str 43' becomes 'str(43)' automatically.\n    The value can be '0' to disable the feature, '1' for 'smart' autocall, where\n    it is not applied if there are no more arguments on the line, and '2' for\n    'full' autocall, where all callable objects are automatically called (even\n    if no arguments are present).\n    Choices: any of [0, 1, 2]\n    Default: 0\n\n\n\nAdding a configuration option is thus a breeze, for example IPython can\nreformat your code with Black since this pull\nrequest. Beyond the logic\nto actually do the reformatting, the complete diff to add the options to the CLI,\nconfiguration file, and automatic generation of the option documentation in Sphinx\ndocumentation is as follows:\n@IPython/terminal/interactiveshell.py:98\nClass TerminalInteractiveShell(InteractiveShell)\n... snip ...\n+     autoformatter = Unicode(None,\n+        help=\"Autoformatter to reformat code.\",\n+        allow_none=True\n+    ).tag(config=True)\n\n\n\nIf you have an application or library which potentially has a really large\nnumber of configuration knobs and you want to isolate changes:\nTraitlets can help you expose all of those cleanly to a user, without having to\nthink about the option name or writing the logic to set a default value.\nUnlike many other libraries likes argparse or click, the listing of\nconfiguration options is decentralized and lives on the object being\nconfigured. And there is no differences between adding a configuration option\nin the configuration file or at the command line interface.\nConfigure what you do not yet know about\nIn an application with few parameters and only a couple of plugins it might be\nrelatively straightforward to provide options and CLI arguments; this becomes\nharder when arbitrary plugins are involved and those plugins have arbitrary\nconfiguration options you may or may not know about at startup time.\nOne good example is JupyterHub. JupyterHub has various plugins, one category of\nwhich is Spawners. Spawners decide how notebook servers are started. You can\nuse a custom spawner,\nand many institutions employ only minimal changes to the default Spawner to\naccommodate their use case. A few of the common Spawners are SystemDSpawner,\nSlurmSpawner, and KubeSpawner, each with their own parameters.\nIt is critical to make it as simple as possible to provide configuration\noptions and make them available from Jupyter Configuration files and from the\ncommand line.\nUsing a custom Spawner is simple:\nc.JupyterHub.spawner_class = 'mypackage:MySpawner'\n\n\n\nand Traitlets allows you to also arbitrarily configure MySpawner with\nc.MySpawner.mem_limit = '200G'\n\n\n\nTraitlets is aware of class hierarchy, thus when MySpawner inherits from the\ndefault Spawner, all c.Spawner... options will affect MySpawner, but\nc.MySpawner... options will of course only affect MySpawner.\nIt is thus also easy to configure siblings differently; a good example is Terminal\nIPython vs IPykernel (used in notebooks and lab). Both applications are subclasses\nof InteractiveShell, namely ZMQInteractiveShell and TerminalInteractiveShell.\nI can configure both with c.InteractiveShell.attribute=, or decide that only\nterminal IPython will be affected via c.TerminalInteractiveShell.attribute=, I can also\ntarget only notebook-like interfaces with c.ZMQInteractiveShell.attribute=;\nOn my own machine for example, %matplotlib is setup to be inline only for\nkernels and not terminal.\nThus if you have an application or library with a number of plugins, and for\nwhich configurability can be thought of as tree-like similar to a class\nhierarchy, Traitlets can help you.\nOther functionalities\nObservability\nBeyond the configurability part is observability; as we already had a great\ntype system with hooks, and sometimes you may want to mutate configuration of\na running application, traitlets allow you to observe values and propagate\nthem to other places.\nTo look at the above example with code reformatting, the reformatter can be\nchanged dynamically to a different one with the following:\nClass TerminalInteractiveShell(InteractiveShell)\n...\n\n    @observe('autoformatter')\n    def _autoformatter_changed(self, change):\n        formatter = change.new\n        if formatter is None:\n            self.reformat_handler = lambda x:x\n        elif formatter == 'black':\n            self.reformat_handler = black_reformat_handler\n        else:\n            raise ValueError\n\n\n\nObservability is also what powers\nipywidgets;\nit allows dynamic binding of sliders, buttons and other controls to\nvisualisations.\nThis can be useful to glue together models, with parameters needing to be\nsynchronized and models needing to react to changes.\nDynamic defaults\nTraitlets supports dynamic defaults,\ni.e. your default values may depend on some context (OS, Python version).\nConfigurations are by default Python scripts (but can be JSON), so user\nconfig files can be shared across machines and have dynamic values.\nContext base configuration\nObject configuration can also look at the creator of the object at instantiation\ntime.\nc.Foo.Bar.attr = 1\nc.Qux.Bar.attr = 2\n\n\n\nWith the above, the Bar object created by Foo or by Qux will get different\ndefault values. This is for example used by nbconvert\nto decide which options to activate depending on whether you do --to=pdf,\nor --to-html\nFlags and aliases\nLong flag names like --InteractiveShell.debug=True can be cumbersome to type.\nThat is why traitlets provide aliases and flags, so that ipython --debug will\nset debug=True to enable logging on all classes supporting it while still\nallowing you to tweak the login level used on a per-class basis.\nLimitations\nOf course all this power doesn't come completely for free:\n\nYour class and attribute names become part of your API\nAs configuration is loaded/read before any potential plugins are loaded, it is\n   impossible to definitively detect typos or invalid configuration options.\nTraitlets relies heavily on metaclasses, so it can add a construction cost to\n   your objects and can be hard to debug.\n\nConclusion\nThis was a short introduction to Traitlets. I hope it made it a little clearer\nhow the Jupyter configuration system works, and we are looking forward to see\nhow this can be used to adapt Jupyter to your work flow.\nAre you struggling with a system that has too many configuration options? Do\nyou have a use case where you believe Traitlets can be useful? We'll be happy\nto hear from you.\nIf you want to learn more, see this JupyterCon 2018\ntalk and the documentation.\nTry the new 5.x version and let us know if you have questions.",
      "tags": "community,historical,IPython,Jupyter,Labs,Python,traitlets",
      "url": "https://labs.quansight.org/blog/2020/08/what-are-traitlets/"
    },
    {
      "title": "IPython reproducible builds",
      "text": "Starting with IPython 7.16.1 (released in June 2020), you should be able to recreate the sdist (.tar.gz) and wheel\n(.whl), and get byte for byte identical result to the wheels published on PyPI. This is a critical step toward being able\nto trust your computing platforms, and a key component to improve efficiency of build and packaging platforms. It also\npotentially impacts fast conda environment creation for users. The following goes into some reasons for why you should care.\n\n\nSince the cornerstone paper Refections on trusting Trust, there have always been advocates of reproducible builds. In\ntoday's highly interconnected world, and with the speed at which new software is released and deployed, being able to confirm\nthe provenance of build artifacts and verify that the supply chain has not been affected by a malicious actor is often critical. To help in this\nendeavour, the movement of reproducible builds, attempts to push software toward a deterministic and reproducible\nbuild process.\nWhile information security practitioners were one of the earliest groups who advocated for reproducible builds, there\nare a number of other advantages to ensure the same artifacts can be reproduced identically.\nFor the users of the Scientific Python ecosystem, the notion of reproducibility/replicability is not new, and is one of\nthe critical ideas behind the scientific process. Given some instructions from an author, you should be able to perform some\nexperiments or proof, and reach the same conclusion. When you see the opposite, your instructions or hypothesis are missing\nsome variable elements and your model is incomplete; having a complete model, reproducibility, is one of the necessary\ncomponent to be able to trust the results and build upon it. We are not going to enter into the exact distinction\nbetween reproducible and replicable, both have their goals and uses.\nAren't computers reproducible by design?\nOne of the prerequisites for reproducibility is to have a deterministic process, and while we tend to think about computers\nas deterministic things, they often are not, and on purpose. Mainly driven by security concerns, a number of processes in\na computer use pseudo-randomness to prevent an attacker from gaining control over a system. Thus by default, many of the\ntypical actions you may do in a software (iterating over a set in Python, the hash value of strings), will have some randomness in them,\nwhich impact the determinism of a process.\nThere are of course a number of sources of involuntary randomness due to the practicality of computer systems. These\ninclude: the current date and time, your user name and uid, hostname, order of files on your hard drive and in which\norder they may be listed.\nTo obtain a reproducible result, one thus often needs to make sure each step is deterministic and that all the variables\ninfluencing that process are either controlled or recorded.\nReproducible artifact build\nIn IPython 7.16.1 we have tried to removed all sources of variability, by controlling the order in which the files are\ngenerated, archived in the sdists, their metadata, timestamps, etc. Thus you should be able to go from the commit in the\nsource repository to the sdist/wheel and obtain an identical result. This should help you to trust that no backdoor has\nbeen introduced in the released packages. It is also critically useful for efficiency in package managers.\nOf course you have to trust that the IPython source itself and its dependencies are devoid of backdoors, but let's move\none step at a time. Reproducible build artifacts can also have impact on the build and installation process of packages.\nEfficient rebuilds of dependencies\nCurrently IPython depends on many packages: prompt_toolkit, traitlets, setuptools, and more. We also have a number of\ndownstream packages, like ipykernel, then jupyter notebook. When a dependency tree is rebuilt for one reason or another, a change of a\nsingle bit could trigger the rebuild of the whole chain. When a package like IPython is not reproducible, this means a\nrebuild of IPython \u2013\u00a0whether it has changed or not \u2013\u00a0could trigger a rebuild of all downstream elements.\nWith a reproducible build, you can trust that the artifact will not change after a rebuild. For the functional programmer\naround you: it indicates that the process of building from source is a pure function. Therefore it can safely be part\nof a distributed system (rebuilding on two different places will give the same result, so you can avoid costly data\nmovement), and we can also do caching of results, so for identical input we know the output will be identical.\nThis can allow breaking rebuild chains by stopping as soon as a dependency rebuild has no effect.\nThus, reproducible builds are a necessary but not sufficient condition to decrease the time spent by platforms like conda-forge on rebuilding the ecosystem\nfor a new version of Python; making new packages available faster.\nDeduplication\nOne rarely mentioned advantage is deduplication. In many cases there are no reasons why artifacts produced by a build\nstep would depend on all of their inputs. For example IPython has currently no reason to build differently on Python 3.7,\n3.8, and the soon-to-be-released 3.9, or on Linux/macOS/Windows. Nevertheless Conda provides no less than 10 downloads for each\nrelease of IPython.\n$ diff -U0  <(cd ipython-7.17.0-py37hc6149b9_0/ ; fd -tf --full-path  | xargs -L1 md5)  <(cd ipython-7.17.0-py38h1cdfbd6_0/ ; fd --full-path  -t f | xargs -L1 md5)\n--- /dev/fd/63  2020-08-05 15:15:47.000000000 -0700\n+++ /dev/fd/62  2020-08-05 15:15:47.000000000 -0700\n@@ -316 +316 @@\n-MD5 (Lib/site-packages/ipython-7.17.0.dist-info/RECORD) = 0ebe6e43ae9dcfc29b86338605fc9065\n+MD5 (Lib/site-packages/ipython-7.17.0.dist-info/RECORD) = 16f820e051e75462d970be438fbd2b0a\n@@ -319 +319 @@\n-MD5 (Lib/site-packages/ipython-7.17.0.dist-info/direct_url.json) = 2c37570ef1bd3eadd669649da321b69f\n+MD5 (Lib/site-packages/ipython-7.17.0.dist-info/direct_url.json) = b55d0dcd87b11218d41c34d8ee0a5016\n@@ -331 +331 @@\n-MD5 (info/files) = d24cc180f95193be847116340f1af63a\n+MD5 (info/files) = 0161e68902cb78c6b1aec564c0a9e808\n@@ -333,2 +333,2 @@\n-MD5 (info/hash_input.json) = d25b93fadc7421a297daf02f9a04584f\n-MD5 (info/index.json) = 0fb13436b493433c08c9b29c23b76180\n+MD5 (info/hash_input.json) = b7c843bd4a6cef64080e893a939e95bd\n+MD5 (info/index.json) = 6283e83efc554f9f5b4d4e2330d8ec4e\n@@ -336,3 +336,3 @@\n-MD5 (info/paths.json) = 06e6ba2378d6eecdfcf08e1c602d392b\n-MD5 (info/recipe/conda_build_config.yaml) = 1a98301b552bde7a25c99a39711c9fe2\n-MD5 (info/recipe/meta.yaml) = 1a823d7c8c2dac394617482c596f26f0\n+MD5 (info/paths.json) = 0a82a812984f98660fbf0244eddeed38\n+MD5 (info/recipe/conda_build_config.yaml) = e1c3ae7827bd7003e9034720c7b0f76c\n+MD5 (info/recipe/meta.yaml) = 08d5f54df6083bfb834b24b9ae4c4e0f\n@@ -342 +342 @@\n-MD5 (info/test/test_time_dependencies.json) = a66ce3a62bd757ceede3ad5ef4c2c4b6\n+MD5 (info/test/test_time_dependencies.json) = ca1e35258bf3ce4719b090a90f886cd6\n\n\n\nI'm sure some of these changes are necessary as they are related to which Python version the package refers to; but let's\nlook in more detail at one of those:\n$ fd test_time_dependencies.json | xargs diff -U2\n--- ipython-7.17.0-py37hc6149b9_0/info/test/test_time_dependencies.json 2020-07-31 21:41:11.000000000 -0700\n+++ ipython-7.17.0-py38h1cdfbd6_0/info/test/test_time_dependencies.json 2020-07-31 21:40:53.000000000 -0700\n@@ -1 +1 @@\n-[\"matplotlib\", \"nbformat\", \"pygments\", \"ipykernel\", \"nose >=0.10.1\", \"trio\", \"numpy\", \"pip\", \"testpath\", \"requests\"]\n+[\"requests\", \"numpy\", \"matplotlib\", \"trio\", \"testpath\", \"pip\", \"pygments\", \"nbformat\", \"ipykernel\", \"nose >=0.10.1\"]\n\n\n\nHere the changes are minor and completely inconsequential for the use of the package; those changes prevent us from\ndetecting that two builds are actually identical.\nThis could allow to decrease precious disk space, bandwidth, and time spent waiting for software to install.\nIf you'd like to learn more about this topic, I'd recommend talking to some Nix users to see how a purely functional package manager works and\nwhich other advantages this brings.\nBut in the meantime, please go track the various sources of randomness in your favorite library or build system, and\nlet's work together to change things so that things never change!",
      "tags": "IPython,Labs,packaging,reproducible-builds",
      "url": "https://labs.quansight.org/blog/2020/08/ipython-reproducible-builds/"
    },
    {
      "title": "Introducing Versioned HDF5",
      "text": "The problem of storing and manipulating large amounts of data is a challenge in\nmany scientific computing and industry applications. One of the standard data\nmodels for this is HDF5,\nan open technology that implements a hierarchical structure (similar to a\nfile-system structure) for storing large amounts of possibly heterogeneous data\nwithin a single file. Data in an HDF5 file is organized into groups and\ndatasets; you can think about these as the folders and files in your local\nfile system, respectively. You can also optionally store metadata associated\nwith each item in a file, which makes this a self-describing and powerful data\nstorage model.\n\n\n\nImage: Hierarchical Data Format (HDF5) Dataset (From https://www.neonscience.org/about-hdf5)\nSince reading and writing operations in these large data files must be fast,\nthe HDF5 model includes data compression and chunking. This technique allows\nthe data to be retrieved in subsets that fit the computer's memory or RAM,\nwhich means that it doesn't require the entire file contents to be loaded into\nmemory at once. All this makes HDF5 a popular format in several domains, and\nwith h5py it is possible to use a Pythonic interface to\nread and write data to a HDF5 file.\nNow, let's say you have an HDF5 file with contents that change over time. You\nmay want to add or remove datasets, change the contents of the data or the\nmetadata, and keep a record of which changes occurred when, with a way to\nrecover previous versions of this file. Since HDF5 is a binary file format,\nusing regular version control tools (such as git) may prove difficult.\nIntroducing the Versioned HDF5 library\nThe Versioned HDF5 library is a versioned abstraction on top of h5py. Because\nof the flexibility of the HDF5 data model, all versioning data is stored in the\nfile itself, which means that different versions of the same data (including\nversion metadata) can be stored in a single HDF5 file.\nTo see how this works in practice, let's say we create a regular HDF5 file with\nh5py called mydata.h5.\n    >>> import h5py\n    >>> fileobject = h5py.File('mydata.h5', 'w')\n\n\n\nNow, you can create a VersionedHDF5file object:\n    >>> from versioned_hdf5 import VersionedHDF5File\n    >>> versioned_file = VersionedHDF5File(fileobject)\n\n\n\nThis file still doesn't have any data or versions stored in it. To create a new\nversion, you can use a context manager:\n    >>> with versioned_file.stage_version('version1') as group:\n    ...     group['mydataset'] = np.ones(10000)\n\n\n\nThe context manager returns a h5py group object, which should be modified\nin-place to build the new version. When the context manager exits, the version\nwill be written to the file. From this moment on, any interaction with the\nversioned groups and datasets should be done via the Versioned HDF5 API, rather\nthan h5py.\nNow, the versioned_file object can be used to expose versioned data by version name:\n    >>> v1 = versioned_file['version1']\n    >>> v1\n    <Committed InMemoryGroup \"/_version_data/versions/version1\">\n    >>> v1['mydataset']\n    <InMemoryArrayDataset \"mydataset\": shape (10000,), type \"<f8\">\n\n\n\nTo access the actual data stored in version version1, we use the same syntax\nas h5py:\n    >>> dataset = v1['mydataset']\n    >>> dataset[()]\n    array([1., 1., 1., ..., 1., 1., 1.])\n\n\n\nSuppose now we want to commit a new version of this data, changing just a slice\nof the data. We can do this as follows:\n    >>> with versioned_file.stage_version('version2') as group:\n    ...     group['mydataset'][0] = -10\n\n\n\nBoth versions are now stored in the file, and can be accessed independently.\n    >>> v2 = versioned_file['version2']\n    >>> v1['mydataset'][()]\n    array([1., 1., 1., ...,  1.,  1.,  1.])]\n    >>> v2['mydataset'][()]\n    array([-10., 1., 1., ...,  1.,  1.,  1.])]\n\n\n\nCurrent status\nversioned-hdf5 1.0 has recently been released, and is available on PyPI and conda-forge. You can install it with\nconda install -c conda-forge versioned-hdf5\n\n\n\nThe development is on GitHub.\nCurrently, the library supports basic use cases, but there is still a lot to\ndo. We welcome community contributions to the library, including any issues or\nfeature requests.\nFor now, you can check out the\ndocumentation for more details on\nwhat is supported and how the library is built.\nNext steps\nThis is the first post in a series about the Versioned HDF5 library. Next,\nwe'll discuss the performance of Versioned HDF5 files, and the design of the\nlibrary.\nThe Versioned HDF5 library was created by the D. E. Shaw\ngroup in conjunction with\nQuansight.",
      "tags": "h5py,HDF5",
      "url": "https://labs.quansight.org/blog/2020/08/introducing-versioned-hdf5/"
    },
    {
      "title": "Designing with and for developers",
      "text": "Open source is notorious for lack of design presence, enough so that my search\nto prove this fact has turned up nearly nothing. There\u2019s many ways that such a\ngap in community might manifest, but one that I never anticipated was working\nwith developers that had never interacted with a designer before.\nA quick note for context: I\u2019m writing this as a UX/UI designer working with\nopen source projects for a little over a year. Because there are so many ways\ndesign processes can happen (enough to warrant its own blog post), this post is\nnot intended to discuss design process deeply. My goal here is to pass on some\nof what I\u2019ve learned that helps me design in this unusual space in hopes that\nit can help someone else. This post might seem most relevant for designers, but\nI think this experience could be helpful for developers as well.\n\n\nAfter a few years at more conventional design jobs where the workflow never\nallowed me to communicate with our team\u2019s developers, I started working with\nProject Jupyter on JupyterLab. This was my first experience working\ncollaboratively with developers. I was feeling lost enough trying to comprehend\nthe scope of the software I was helping design, and the enigma of the\nnear-silent developer I began working with was not helping my nerves.\nBut it would be okay, because I had a plan! Surely if I set a good tone for\nthis relationship and asked how design was most helpful for them, they\u2019d be\nable to give me an answer, right? Here\u2019s how that conversation went:\n\nMe: Hey, I\u2019ve never gotten to work directly with a developer like this\nbefore. How can I best support you? What are your expectations for working\nwith designers?\nDeveloper: I\u2019ve never worked with a designer, so I have no idea. Let me know\nif I can help you, though.\nMe: Oh, okay.\n\nWe didn\u2019t speak for the rest of the day. Not the overwhelming success story I\nwas looking for.\nOnce I learned I shouldn\u2019t expect that everyone on my team to knows what they\nwant from the design process, here\u2019s what helped me move forward and\ncollaborate with developers successfully.\nSay hello.\nDevelopers are the more established group in open source communities and may\nnot feel the pressure to adjust their workflow to bring in designers even if\nthey know it might benefit the project. Being on the outside, I\u2019ve found it can\nbe easier for designers to be the ones to reach out and intervene where they\nsee a need for design. Just be friendly and be sure to give specific examples\nof the kind of work you\u2019d like to do while listening to what they want.\n(Getting support from developers, however, is crucial, so if developers want to\nbe the ones to invite designers, it can be a big help.)\nExplore what\u2019s already there.\nWhether there is a product that\u2019s already been developed or it\u2019s brand new, I\nbelieve there\u2019s always information that can be helpful in orienting a designer\nin a project. Make sure to explore the product not just in the ways I find\ndesigners are used to (testing the product based on use cases, heuristic\nevaluation, competitive analysis, etc.), but in the ways developers might\nexperience or encounter it. This could be looking for what kinds of questions\npeople are frequently asking on Stack Overflow, issues on GitHub, or any other\nkinds of documentation. One of the most helpful things is to have a grasp on\nwhat words developers use to describe specific parts or experiences in the\nproject to make sure you are communicating with each other effectively.\nTrust your process.\nI hated being told this as a student, so hear me out before you immediately\nclose this blog post like I would have a year ago. Sometimes it really is best\nto just start working with the intention of stepping back to evaluate the work\nearly, often, and with others before getting too wrapped up in your own\nthoughts. I\u2019ve found that once a developer has seen the direction I\u2019m heading,\nthey often start to get concrete ideas about what kind of design information\nthey might need to turn those mockups and prototypes into real products. This\nopens up the possibility for more informed and stronger design work delivered\nin a way that makes everyone\u2019s job easier.\nTalk to each other.\nThis sounds obvious, but I find it easy to overlook by accident when both the\ndeveloper(s) and I start getting wrapped up in our own work. Ask them their\nexpectations about an interaction you are working on (developers are users,\ntoo) or what kinds of technical limitations you should be aware of. If you\ncreate a habit of bouncing ideas off one another, you\u2019ll understand each other\nbetter when solving harder problems and will feel more comfortable sharing\nhonest feedback. For me, this has been especially helpful since I work with\nprojects that draw users with specific technical knowledge, so I\u2019m unlikely to\nunderstand the user\u2019s workflow deeply without relying on a little research.\nCheck in.\nWould this kind of information be helpful when developing? Is this the work you\nwere expecting from me? Do you see anything missing from the design? Am I\nrepresenting this technical concept accurately? These are some of the typical\ncheck-in questions I ask developers I work with when reaching check points in a\nproject. This regular feedback aided my development of a stronger workflow\ndesigned to help developers get the work they needed to move forward. Like any\nproject, I find it critical to check in throughout working and at the end in\norder to learn from the experience and make sure I\u2019m cultivating a positive\nrelationship between design and development teams.\nAll these recommendations have grown and changed as the problems I face do the\nsame, and I\u2019m sure they will continue to develop as I work with different\npeople and projects. In previous jobs, I had learned how to prove that my work\nhad value and that design is critical to a company\u2019s success. I always had to\nconvince people to listen. Suddenly I\u2019ve had people who are open to listening,\nbut had absolutely no idea who I was, what I did, or how I could help them. The\nreal problem began when I realized that I didn\u2019t have confidence in my answers\nto those questions, either.\nAt just over a year into designing for open source, I remain surprised that\nthis is more common than I thought. Across multiple projects and on different\nteams, I\u2019ve worked with several experienced developers that aren\u2019t certain what\nit means for me to be a UX/UI designer. It\u2019s exciting in a unique way, and I\u2019m\ngrateful that I\u2019ve met friendly and curious people who want to collaborate in\nspite of any uncertainty.\nMost of all, I hope this helps someone looking for thoughts on navigating what\nit\u2019s like to work in open source from the standpoint of a non-developer. Thank\nyou for reading, and best of luck on your own collaborative adventures!",
      "tags": "design",
      "url": "https://labs.quansight.org/blog/2020/08/designing-with-and-for-developers/"
    },
    {
      "title": "Quansight Labs: what I learned in my first 3 months",
      "text": "I joined Quansight at the beginning of April, splitting my time between\nPyTorch (as part of a larger Quansight team) and contributing to Quansight Labs\nsupported community-driven projects in the Python scientific and data science\nsoftware stack, primarily to NumPy.  I have found my next home; the people, the\nprojects, and the atmosphere are an all around win-win for me and (I hope) for\nthe projects to which I contribute.\n\n\nI am not a newcomer to Open Source. I originally became involved in\nPyPy as an after-hours hobby to hone my developer\nskills, and quickly became enamoured with the people and the mission. Over the\nyears my efforts in the open source world moved more mainstream, and in 2018 I\ntook on a full-time position working on NumPy, funded through a grant to\nBIDS. Since April 2020, I have moved to Quansight\nLabs as a full-time developer. \nQuansight Labs is a subsidiary of Quansight\nLLC, and Labs' mission is \"Sustaining the future of open source\". It does this\nby gathering together an amazing group of software developers and letting them\nloose on a variety of open source projects: Numpy, Spyder, Jupyter, conda-forge,\nNumba and more. Of course, the\ndevelopers do not only volunteer their time. Quansight Labs is sponsored via\n\ngenerous support from Quansight LLC,\ngrants like the one from CZI for NumPy and\n  OpenBLAS,\nand via community work\n  orders\n  from companies like D. E. Shaw who sponsor work on\n  Dask.\n\nI personally split my sponsored time between PyTorch and NumPy, and in my free\ntime contribute to other projects like PyPy, helping with Python packaging, and\nmore.\nIn the past three months I have met a whole new group of developers. We all\nwork remotely, and meet up on Slack and video calls. In the past, the entire\nteam would meet once a year or so, until face-to-face gatherings were put on hold. But that does\nnot stop the interactions. The organization, with over 20 developers, has\nlittle hierarchy. Interactions are direct and it is customary to meet up\nfor virtual coffee or just to chat. Since we all come from different places\nand backgrounds, there are specialists with deep knowlege in many fields:\nmathematics, C++ and GPU programming, Javascript and web technologies, UX,\ntesting, and more that I have not yet explored. The synergy makes it a win-win\nfor all involved. When I get stuck, a world-class expert is available and we\ncan all help each other move forward.\nLucky me, you say, but why am I bothering to share my good fortune and make\nyou all jealous? Here is one take-away for the experienced developers out\nthere, keep posted for more as my journey progresses.\nI was very hesitant to make the leap into a new career. I am no spring chicken,\nand I was worried that I would not be able to find a suitable position. I only\nbegan my software developer career late in life after I tired of\nelectrical design and contracting. But I am here to tell you that even an old\ndog can learn new tricks. I began my open source chapter as a volunteer,\nwhich improved my programming powers and led to meeting people outside\nmy usual circle. This led to the NumPy grant that led to Quansight Labs. So if\nI could do it, others can too.\nJoining Quansight Labs was definitely the right step for me. I hope you also\nfind your paths enjoyable and rewarding.",
      "tags": "funding,Labs,NumPy,PyTorch",
      "url": "https://labs.quansight.org/blog/2020/07/a-win-win-all-around/"
    },
    {
      "title": "Learn NixOS by turning a Raspberry Pi into a Wireless Router",
      "text": "I recently moved, and my new place has a relatively small footprint.  (Yes, I\nmoved during the COVID-19 pandemic. And yes, it was crazy.) I quickly realized\nthat was going to need a wireless router of some sort, or more formally, a wireless\naccess point (WAP). Using my Ubuntu laptop's \"wireless hotspot\" capability was a\nnice temporary solution, but it had a few serious drawbacks.\n\n\nDrawbacks of hotspotting with a laptop\n\nThe wireless internet goes out whenever I would travel with the laptop,\nThe laptop had to be close to the modem, so that it could be plugged into\n  ethernet, making my laptop not even portable within the apartment,\nThe SSID was my laptop's hostname,\nThe WPA password would be set to a random string whenever the hotspot\n  was started, and so\nWhenever I moved my laptop I would also need to reset the credentials\n  on all of my wireless devices!\n\nAdditionally, some of my coworkers are Nix true believers.\nWhile I had read the NixOS docs, I had never actually taken it for a spin.\nConsider this my first few steps down the /etc/nixos path, because, while\nI lacked a WiFi router, I did have an errant Raspberry Pi 3B+ lying around...\nToo Long; Didn't Read\nBe sure to fill out the SSID and WPA passphrase in the file below.\n/etc/nixos/configuration.nix\n{ config, pkgs, lib, ... }:\n{\n  # NixOS wants to enable GRUB by default\n  boot.loader.grub.enable = false;\n  # Enables the generation of /boot/extlinux/extlinux.conf\n  boot.loader.generic-extlinux-compatible.enable = true;\n\n  # use an older kernel, so that we can actually boot\n  boot.kernelPackages = pkgs.linuxPackages_4_19;\n\n  # Needed for the virtual console to work on the RPi 3, as the default of 16M\n  # doesn't seem to be enough. If X.org behaves weirdly (I only saw the cursor)\n  # then try increasing this to 256M.\n  boot.kernelParams = [\"cma=32M\"];\n\n  # File systems configuration for using the installer's partition layout\n  fileSystems = {\n    \"/\" = {\n      device = \"/dev/disk/by-label/NIXOS_SD\";\n      fsType = \"ext4\";\n    };\n  };\n\n  # Recommended swap file is optional\n  swapDevices = [ { device = \"/swapfile\"; size = 1024; } ];\n\n  # packages\n  environment.systemPackages = with pkgs; [ hostapd dnsmasq bridge-utils ];\n\n  # add wireless service\n  hardware.enableRedistributableFirmware = true;\n  networking.wireless.enable = true;\n\n  # set up wireless access point\n  networking.networkmanager.unmanaged = [ \"interface-name:wlan*\" ]\n    ++ lib.optional config.services.hostapd.enable \"interface-name:${config.services.hostapd.interface}\";\n  services.hostapd = {\n    enable = true;\n    interface = \"wlan0\";\n    hwMode = \"g\";\n    ssid = \"<YOUR NETWORK NAME HERE>\";\n    wpaPassphrase = \"<YOUR PASSWORD HERE>\";\n  };\n\n  # set up wireless static IP address\n  networking.interfaces.wlan0.ip4 = lib.mkOverride 0 [ ];\n  networking.interfaces.wlan0.ipv4.addresses =\n    lib.optionals config.services.hostapd.enable [{ address = \"192.168.0.1\"; prefixLength = 24; }];\n\n  # set up wireless DNS\n  services.dnsmasq = lib.optionalAttrs config.services.hostapd.enable {\n    enable = true;\n    extraConfig = ''\n      interface=wlan0\n      bind-interfaces\n      dhcp-range=192.168.0.10,192.168.0.254,24h\n    '';\n  };\n  networking.firewall.allowedUDPPorts = lib.optionals config.services.hostapd.enable [53 67];\n  services.haveged.enable = config.services.hostapd.enable;\n\n  # Finally, bridge ethernet and wifi\n  networking.bridges.br0.interfaces = [ \"eth0\" \"wlan0\" ];\n}\n\n\n\nLearning to use Nix\nNix is great! Its mental model is really neat and there are some fantastic\nideas under the covers. However, the documentation has some glaring holes.\nIt almost all cases it either assumes that:\n\nYou already know how to use Nix, or\nYou already are on a Nix machine.\n\nThis makes it very frustrating to actually get started, especially on non-standard\nhardware such as the Raspberry Pi. A lot of these issues can be smoothed over by a\nfriend who can act as your spirit guide. I write this as someone who has been a\nLinux user for 20 years and who works on open source packaging problems\n(conda-forge).\nThe following are some basic Nix tips for helping get you started.\nDon't use nix-env\nThe first bit of philosophy to understand is that while much of the Nix documentation\ntouts the functional nature of its underlying language, the important aspect of Nix\nis that it is declarative.\nNix wants you to specify the configuration and layout of your Operating System (OS)\nahead of time in a relatively static way. This is very different from how other\nLinux distributions operate, which assume that you will procedurally build\nup your system from various available components, as needed. The declarative\napproach has advantages & disadvantages.\nAdvantages:\n\nYou can write out exactly what your OS is in a single text file (see above).\nYour OS can be built and tested before booting into it.\nErrors in configuration are caught and tested during the build process.\n\nDisadvantages:\n\nYou have to know what you want in your OS before you build it.\nChanges to the OS configuration require a rebuild.\n\nIn many cases, the advantages here outweigh the disadvantages. It is without\nquestion that Docker, CoreOS,\nand conda all owe a lot of conceptual\ninspiration to the work Nix has been performing for years.\nOf course, even a functional OS has to have an escape valve. This is called\nnix-env and is a command line utility for creating & managing environments\n(a collection of packages) in an existing OS.\n\nWarning\nDo not use nix-env!\n\nWe want to create a dedicated device that, when it boots up, is a WAP. To this\nend, it is important that we declare everything in the configuration file.\nIf we start creating environments willy-nilly, we won't obtain the proper\nboot behavior. This is very different from procedural OSes, where you can\nmodify live configuration files that affect boot processes. Not so here!\nAll boot config needs to be declared!\n(Unfortunately, much of the Nix documentation uses nix-env, because it\nassumes that you are a user on an existing Nix box just trying things out.)\nThe configuration.nix file\nSo where does this mysterious OS configuration file live? Well, the full path\nto this file is /etc/nixos/configuration.nix. It is written in the Nix language,\nand is used by the nixos-rebuild command line tool. We'll see this tool later\nto build the router's OS.\nAgain, unfortunately, when people report bugs or list a configuration snippet,\nthey are almost always referring to this file. However, they don't specify that\nthey are talking about this file. It is just known.\nNow you know too.\nYou need to be root (which is shockingly easy)\nAnother issue that is not clear from the docs is that any serious command you might\nwant to run needs to be run as root (or another user in the wheel group). However,\nthe default aarch64 image boots into a user named nixos. This requires you to\nsudo su to become the root user to run rebuild commands (or any of the commands\nprefixed by sudo).\nAlso, oddly, in the initial image neither the nixos nor the root user have passwords.\nSo you end up running sudo without needing a password. You will probably want to set a\npassword with the passwd utility, or via user management in /etc/nixos/configuration.nix.\nFirst Boot!\nAssuming you have the SD card for your Raspberry Pi handy, take it out of the Pi and\nplug it into another (Linux) computer. We are going to need to flash it with a basic\nNixOS. You can find generic instructions for\nNixOS on a Raspberry PI here and\ninstructions for NixOS on ARM here. However,\nI'll summarize the important bits here.\nFirst, go to\nthe 19.09 aarm64 landing page\nand download the latest NixOS image. It will be called something like\nnixos-sd-image-19.09.2435.9642f121eb1-aarch64-linux.img. We'll assume this is in\nyour ~/Downloads folder.\nSecond, figure out what device your SD card is. If you just plugged it in, you\ncan determine this by looking at the end of the output of the dmesg command.\nFor example,\n$ dmesg\n[ 4591.053095] usb 3-10: new high-speed USB device number 5 using xhci_hcd\n[ 4591.201911] usb 3-10: New USB device found, idVendor=14cd, idProduct=168a, bcdDevice= 0.01\n[ 4591.201915] usb 3-10: New USB device strings: Mfr=1, Product=3, SerialNumber=2\n[ 4591.201917] usb 3-10: Product: USB Mass Storage Device\n[ 4591.201919] usb 3-10: Manufacturer: USB Device\n[ 4591.201921] usb 3-10: SerialNumber: 816820130806\n[ 4591.202955] usb-storage 3-10:1.0: USB Mass Storage device detected\n[ 4591.203140] scsi host10: usb-storage 3-10:1.0\n[ 4592.205933] scsi 10:0:0:0: Direct-Access     USB Mass  Storage Device  1.00 PQ: 0 ANSI: 0\n[ 4592.206338] sd 10:0:0:0: Attached scsi generic sg1 type 0\n[ 4592.207288] sd 10:0:0:0: [sdb] 31116288 512-byte logical blocks: (15.9 GB/14.8 GiB)\n[ 4592.207421] sd 10:0:0:0: [sdb] Write Protect is off\n[ 4592.207425] sd 10:0:0:0: [sdb] Mode Sense: 03 00 00 00\n[ 4592.207561] sd 10:0:0:0: [sdb] No Caching mode page found\n[ 4592.207567] sd 10:0:0:0: [sdb] Assuming drive cache: write through\n[ 4592.212993]  sdb: sdb1 sdb2\n[ 4592.214220] sd 10:0:0:0: [sdb] Attached SCSI removable disk\n\n\n\nThis let's us know that the SD card is the /dev/sdb device and has two partitions.\nYours might be called /dev/sdc or something similar. It also might have more than\ntwo partitions. That is totally normal at this step.\nThird, we need to copy the NixOS image over to the SD card. We'll do this with\nthe dd command. The SD card should not be mounted right now. Run the following\ncommand with the path to the image and the SD card device replaced as appropriate.\n$ sudo dd if=~/Downloads/nixos-sd-image-19.09.2435.9642f121eb1-aarch64-linux.img of=/dev/sdb\n\n\n\nGreat! At this point, we have now flashed the SD card with our new NixOS!\nFourth, it will save us a lot of typing if we copy over an existing\nconfiguration file to the SD card. Copy the text of the\n/etc/nixos/configuration.nix file at the top of this article to a new file,\nlet's call it ~/Downloads/config.nix. Fill in this file with the SSID and\npassword that you want your network to have. Then, run the following commands\nto copy the configuration file to the SD card. Again, modify the paths here\nas needed.\n$ mkdir -p ~/mount\n$ sudo mount /dev/sdb2 ~/mount\n$ sudo mkdir -p ~/mount/etc/nixos\n$ sudo cp ~/Downloads/config.nix ~/mount/etc/nixos/configuration.nix\n$ sudo umount ~/mount\n\n\n\nFifth, now unplug the SD card from your main machine, plug it into the\nRaspberry Pi! Attach the ethernet, keyboard, monitor, and power supply to the\nPi you will be booting up into your first NixOS! \ud83c\udf89\nBuild the Router OS\nThe operating system that we have just booted into on the Raspberry Pi is a\ngeneric image that does not use the configuration file that we copied over.\nWe need the Nix tools to be able to build the image. Luckily, we are now on\na Nix machine!\nSixth, we need to be root to run a lot of these tools. However, we booted\ninto the nixos user. To make the rest of the process easier, let's just log\nin as root with the following:\n$ sudo su\n\n\n\nSeventh, now let's verify that we have a working internet connection and\nthat the network devices exist. To do so, start with a simple ping that\nshould looks like`\n$ ping 8.8.8.8 -c 3\nPING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.\n64 bytes from 8.8.8.8: icmp_seq=1 ttl=55 time=19.1 ms\n64 bytes from 8.8.8.8: icmp_seq=2 ttl=55 time=19.3 ms\n64 bytes from 8.8.8.8: icmp_seq=3 ttl=55 time=24.2 ms\n\n--- 8.8.8.8 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 3ms\nrtt min/avg/max/mdev = 19.098/20.847/24.154/2.345 ms\n\n\n\nIf you only see packet loss, then this means you do not have internet on the Pi\nand you cannot proceed. Nix requires internet access to build in all realistic\nscenarios.\nNext run the ifconfig command and verify that both eth0 (the ethernet device)\nand wlan0 (the wireless device) exist.\n$ ifconfig\n...\neth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\n        inet 37.128.100.42  netmask 255.255.255.0\n        ...\nwlan0: ...\n...\n\n\n\nEighth, we finally get to build the new OS for our router! We do this with\nthe nixos-rebuild command. All you have to do is run the following command\nand watch the text scroll by.\n$ nixos-rebuild switch -v\n\n\n\nNinth, we can now reboot into our router! Just run:\n$ reboot\n\n\n\nThe default boot option has been changed to be the OS we just built, which is the\nsame as the second option in the bootloader's listing. The original image will be\nthe last boot option (which for various reasons says it is from 1970). This allows\nus to always get back to a working NixOS to do another nixos-rebuild if something\nwent terribly wrong and we need to do another rebuild. For example, this could\nhappen if there was a typo in the configuration file.\nIf you ever modify /etc/nixos/configuration.nix, you'll need to rebuild & reboot.\nThe rebuild & reboot cycle is the fundamental implication of having a declarative OS.\nTenth, if you want to verify that everything is working on your router after\nreboot, you can log in as root and run ifconfig again. This time, you should\nsee eth0, wlan0, and br0 devices. Of course, the ping command should\nwork too.\nEleventh, you should now be able to connect a wireless device like a phone or\na laptop to your shiny new WAP!\nDeep dive into configuration.nix\nFor the truly inquisitive who are still reading, let's break down what the\ndifferent parts of the configuration file actually mean, and how they help\ndefine our wireless router.\nBootloader\nboot.loader.grub.enable = false;\nboot.loader.generic-extlinux-compatible.enable = true;\n\n\n\nThese lines are part of the standard ARM configuration and help speed up the boot\nprocess a bit by disabling the fancy GRUB bootloader.\nOlder Kernel\nboot.kernelPackages = pkgs.linuxPackages_4_19;\n\n\n\nThis line pins our packages to use and older version of the Linux kernel (v4.19).\nThis is super critical because, without this line, nixos-rebuild will end up\ngrabbing a kernel in the v5.x series. Unfortunately, the Raspberry Pi 3 has\nproblems starting up these more recent kernels and the Pi will hang indefinitely\non boot. Using an older kernel version avoids this problem for the time being.\nConsole Memory\nboot.kernelParams = [\"cma=32M\"];\n\n\n\nThis line gives the console more memory than the default value of 16 MB.\nThe Raspberry Pi seems to need this.\nPartition Layout\nfileSystems = {\n  \"/\" = {\n    device = \"/dev/disk/by-label/NIXOS_SD\";\n    fsType = \"ext4\";\n  };\n};\n\n\n\nThe above specifies where the root file system lives. It is part of the\nstandard ARM configuration.\nSwap\nswapDevices = [ { device = \"/swapfile\"; size = 1024; } ];\n\n\n\nThis line gives the machine some extra virtual memory, which is always a good idea.\nRouter packages\nenvironment.systemPackages = with pkgs; [ hostapd dnsmasq bridge-utils ];\n\n\n\nUnlike procedural OSes, we list all of the packages that we need inside the\nconfiguration file itself. This ensures that our router is running exactly\nthe software that we want it to. In this case, we only need three packages\nto enable the Pi to act as an access point, provide a domain name service, and\nbridge the ethernet and the WiFi device.\nFor comparison, in Ubuntu, we would install these packages after we installed\nUbuntu itself. In Nix, we install the OS and the packages at the same time!\nEnable the wireless device\nhardware.enableRedistributableFirmware = true;\nnetworking.wireless.enable = true;\n\n\n\nThese lines simply allow the wireless card to be used on the Raspberry Pi in\nthe simplest possible way.\nSet up the wireless access point\nnetworking.networkmanager.unmanaged = [ \"interface-name:wlan*\" ]\n  ++ lib.optional config.services.hostapd.enable \"interface-name:${config.services.hostapd.interface}\";\nservices.hostapd = {\n  enable = true;\n  interface = \"wlan0\";\n  hwMode = \"g\";\n  ssid = \"<YOUR NETWORK NAME HERE>\";\n  wpaPassphrase = \"<YOUR PASSWORD HERE>\";\n};\n\n\n\nBecause we are installing the router packages along with the OS, we also need\nto configure these packages at the same time we configure the OS itself.\nThe first line here tells Nix not to use normal networking management on the\nwlan0 device. This is because we'll be managing it as a WAP ourselves.\nThe remaining lines configure the access point, including the SSID and password\nfor the wireless network.\nSet up wireless static IP address\nnetworking.interfaces.wlan0.ip4 = lib.mkOverride 0 [ ];\nnetworking.interfaces.wlan0.ipv4.addresses =\n  lib.optionals config.services.hostapd.enable [{ address = \"192.168.0.1\"; prefixLength = 24; }];\n\n\n\nNow, we would like the router itself to have a consistent IP address. We set\nthis in the second line above as 192.168.0.1, though any value in 192.168.x.x\nwould work equally well. However, just providing the static IP on its own is\nnot enough. This is because NixOS will verify that wlan0 does not have an\nIP address during the nixos-rebuild process. Since we are giving wlan0\nan IP address, we need to turn off the IP address checking. If we do not\nremove this verification, the whole OS build process will fail. The first\nline in the above snippet removes this check with the mkOverride function.\nSet up the wireless DNS\nservices.dnsmasq = lib.optionalAttrs config.services.hostapd.enable {\n  enable = true;\n  extraConfig = ''\n    interface=wlan0\n    bind-interfaces\n    dhcp-range=192.168.0.10,192.168.0.254,24h\n  '';\n};\nnetworking.firewall.allowedUDPPorts = lib.optionals config.services.hostapd.enable [53 67];\nservices.haveged.enable = config.services.hostapd.enable;\n\n\n\nThe collection of lines above allows the wlan0 device to operate as a\ndomain name server, proxying a real DNS online. It also sets the range\nof IP addresses that the router will issue to other network devices.\nThis is seen in the dhcp-range=192.168.0.10,192.168.0.254 portion.\nThe first IP address is the lowest address the router will issue, and\nthe second IP address is the highest.\nBridge ethernet and wifi\nnetworking.bridges.br0.interfaces = [ \"eth0\" \"wlan0\" ];\n\n\n\nLastly, we 'bridge' the ethernet and wireless devices. This allows network\ntraffic to flow through the eth0 connection and into the wlan0.\nReflections\nThis was a really fun weekend project! I certainly learned a lot about Nix,\nRaspberry Pis, and about how to set up various parts of the Linux networking\nstack that I had never explored before. My main wish in this process was that\nNix had better documentation that was more aimed at,\n\nPeople who had never used Nix before, and\nPeople who are trying to build dedicated devices.\n\nA lot of the Nix documentation seems to be aimed at a very particular kind\nof desktop user: someone who already has Nix installed! Such users represent\nan important use case, and the nix build configurations are easy enough to read.\nHowever, I definitely think there is on-boarding improvement work to be done in\nthe Nix ecosystem.\nSo, will I ever go back? I don't think so! This router was so cheap (~$40) and\nthe Raspberry Pi 3B+ is so powerful that I get amazing performance throughout\nmy entire apartment. If it ever breaks, the Pi will be trivial to replace.\nI am really happy with what I created. Even if this little project isn't original,\nit solves a real problem in my day-to-day life.\nIn terms of NixOS as a Linux distribution, I think I now am totally on board.\nNix has so many incredible advantages that (as a control freak who builds his\nown WiFi router) I just can't ignore or give up. The feature of Ubuntu that\nwas keeping me on that distribution for so long was that \"it just works\"\n\u00a9 \u00ae.\nBut Nix \"just works\" too. The only catch is that you need to know what \"it\" is\nthat you want working ahead of time. I am also comfortable with responsibly\nusing environments, so I think that increases my willingness to jump into a new\nOS framework. I am a little worried about moving from Ubuntu to Nix on an\nexisting machine, but that is what external hard drive backups are for!\nThat is all folks! Thanks for reading \ud83d\udc4b",
      "tags": "NixOS,Raspberry Pi,Wireless Router",
      "url": "https://labs.quansight.org/blog/2020/07/nixos-rpi-wifi-router/"
    },
    {
      "title": "Writing docs is not just writing docs",
      "text": "I joined the Spyder team almost two years ago, and I never thought I was going to end up working on docs. Six months ago I started a project with CAM Gerlach and Carlos Cordoba to improve Spyder\u2019s documentation. At first, I didn\u2019t actually understand how important docs are for software, especially for open source projects. However, during all this time I\u2019ve learned how documentation has a huge impact on the open-source community and I\u2019ve been thankful to have been able to do this. But, from the beginning, I asked myself \u201cwhy am I the \u2018right person\u2019 for this?\u201d\n\n\nImproving Spyder\u2019s documentation started as part of a NumFOCUS Small Development Grant awarded at the end of last year. The goal of the project was not only to update the documentation for Spyder 4, but also to make it more user-friendly, so users can understand Spyder\u2019s key concepts and get started with it more easily.\nOne of the main ideas for this project was to create a series of short video tutorials, explaining the basic functionality of Spyder and its most important panes, allowing users to learn how to use Spyder faster and easier.\nCarlos Cordoba, our lead maintainer, thought I was the perfect person for this project because of my \u201cgood communication and organization skills\u201d, my \u201cclear and fluent English\u201d (his words) and my previous experience at video editing and recording, which I actually gained by recording singing videos during my \u201cYouTuber\u201d phase.\nI\u2019ve always been interested in education and worked as a tutor for several years learning different tools, gaining experience on how to teach and questioning the effectiveness of current educational methods. This was the first reason why I got interested in this project. For me, documentation is just a fancy way of saying \u201ceducating people on using software\u201d. The challenge here was not recording and editing the videos (which was actually a pretty arduous task), or planning the content for them; the real challenge was to make an impact in such a way that users could find documentation actually useful.\nWhen users start to use a new IDE, or any new software, they usually refer to its documentation, which sometimes doesn\u2019t give enough tools for them to start from zero. This was, then, the whole purpose of the tutorial videos. Spyder\u2019s documentation was already very complete in terms of explaining all the features and cool things you can do with each of its panes. However, if I\u2019m a completely new user and I don\u2019t even know how to open it, where do I start?\nI planned these videos as a series of progressive steps that can get users from zero experience to actually working with Spyder. Hence, I divided the videos into three sections, \u201cFirst Steps with Spyder\u201d, \u201cWorking with Spyder\u201d, and \u201cBuilding Projects with Spyder\u201d. Each section builds on the one before in a way that they are clear enough so that users can find their way through Spyder without knowing anything about it.\nThe \u201cFirst Steps with Spyder\u201d section, live now on our YouTube channel, has three videos that provide a starting point for new users before they even open Spyder for the first time. The first tutorial covers different ways of opening Spyder, the basics of using its interface and an introduction to its four main panes, along with a quick look at the others so that users can get familiar with how Spyder is organized.\nIn the second video, users can learn the basics of using Spyder\u2019s four main panes. The goal is that after this video, users are able to open and edit a file in the Editor, run a script and find the output in Spyder\u2019s IPython Console and execute basic Python commands. They should be able to interact with the Variable Explorer to browse and edit the objects in the console, and use the Help pane to get documentation in two different ways.\nThe third video is meant to show users how to customize Spyder\u2019s interface to start working with it in the way they feel more comfortable according to their preferences. It teaches users how to change the font and the theme of Spyder and rearrange its panes to display only the ones that they want such that it is easier for them to work.\nAfter these three videos, I learned that one of the most important things for writing documentation, more than having a lot of experience with the software, is empathy. Usually being a developer makes it hard to put yourself in the position of a user and understand exactly what a user needs. Now, I realize that this is what actually makes me the \u201cperfect person\u201d for the project. As a junior developer, without as much experience with Spyder, I was able to think more like new users and develop content in a way that they feel closer to us developers. In the end, as Melissa said in her blog post, I learned writing documentation is also a way of building community.\nI hope these videos are really useful, and I look forward to continue finding ways of making the open source community better.\nHappy Spydering!",
      "tags": "documentation,Labs,Spyder",
      "url": "https://labs.quansight.org/blog/2020/07/writing-docs-is-not-just-writing-docs/"
    },
    {
      "title": "Creating a Portable Python Environment from Imports",
      "text": "Python environments provide sandboxes in which packages can be added.\nConda helps us deal with the requirements and dependencies of those packages.\nOccasionally we find ourselves working in a constrained remote machine which\ncan make development challenging. Suppose we wanted to take our exact dev\nenvironment on the remote machine and recreate it on our local machine.\nWhile conda relieves the package dependency challenge, it can be hard to\nreproduce the exact same environment.\n\n\nCreating a Portable Python Environment\nThis walkthrough will demonstrate a method to copy an exact environment on\none machine and transfer it to a another machine. We'll start by collecting\nthe package requirements of a given set of python files, create an environment\nbased on those requirements, then export it as a tarball for distribution on a\ntarget machine.\nSetup\nSample files\nFor this walkthrough we'll assume you have a folder with some python files\nas a rudimentary \"package\".\nIf you want to create some example files, run the following commands:\nmkdir -p ./test_package\necho \"import scipy\" >> ./test_package/first_file.py\necho \"import numpy\" >> ./test_package/first_file.py\necho \"import pandas\" >> ./test_package/second_file.py\necho \"import sklearn\" >> ./test_package/second_file.py\nEach file has a few import statements - nothing fancy.\nExtracting the required packages\nIn order to roll up the environment for the package, we first need to know what\nthe package requires. We will collect all the dependencies and create an environment file.\nGet direct dependencies\nThe first step is to collect dependencies. We'll do this using\ndepfinder. It can be installed into your\nenvironment:  conda install -c conda-forge depfinder\nThis will be as simple as calling depfinder on our test_package directory.\nWe add the -y command to return yaml format.\ndepfinder -y ./test_package\nThis command returns a YAML formatted list with our dependencies. We are interested\nin the required dependencies, which are the external package requirements.\nrequired:\n- numpy\n- pandas\n- scikit-learn\n- scipy\n- ipython\n\n\n\nCreate a temporary environment\nNow we have a list of the direct dependencies but what about all the sub-dependencies?\nTo capture these, we'll create a temporary environment.\nCopy the yaml formatted dependencies into an environment file named environment.yml.\nname: my_env\nchannels:\n  - conda-forge\ndependencies:\n  - python>=3.7\n  - numpy\n  - pandas\n  - scikit-learn\n  - scipy\n  - ipython\n  - conda-pack\n\n\n\nNotice that we've added two extra packages to our environment.yml.\nIn this example, we'll set a minimum python version to include in the package.\nWe could also have explicitly set the Python version. You may notice that we\nhave also added an additional package called called conda-pack. This will be used\nfor wrapping up the environment for distribution - more on that later.\nCreate a conda environment from this yaml that will include all of the necessary\ndependencies.\nconda env create -f environment.yml\nActivate the temporary conda env:\nconda activate my_env\nWrap up the environment into a tarball\nAt this point, we're ready to wrap up our environment into a single tarball.\nTo do this, we'll use a package called conda-pack. Conda-pack is going to help us\nwrap up our exact environment, including python itself. This means that the target machine\nis not required to have python installed for this environment to be utilized. Much of what\nfollows is taken directly from the conda-pack docs.\nPack environment my_env into out_name.tar.gz\nconda pack -n my_env -o my_env.tar.gz\nUnpacking the environment on the target machine\nAt this point you will have a portable tarball that you can send to a different\nmachine. Note that the tarball you've created must only be used on target machines\nwith the same operating system.\nNow we'll go over how to unpack the tarball on the target machine and utilize this\nenvironment.\nUnpack environment into directory my_env:\n$ mkdir -p my_env\n$ tar -xzf my_env.tar.gz -C my_env\nWe could stop here and start using the python environment directly. Note that most\nPython libraries will work fine, but things that require prefix cleanups (since\nwe've built it in one directory and moved to another) will fail.\n$ ./my_env/bin/python\nAlternatively we could activate the environment. This adds my_env/bin to your path\n$ source my_env/bin/activate\nAnd then run python from the activated environment\n(my_env) $ python\nCleanup prefixes from inside the active environment.\nNote that this command can also be run without activating the environment\nas long as some version of python is already installed on the machine.\n(my_env) $ conda-unpack\nAt this point the environment is exactly as if you installed it here\nusing conda directly. All scripts should be fully functional, e.g.:\n(my_env) $ ipython --version\nWhen you're done, you may deactivate the environment to remove it from your path\n(my_env) $ source my_env/bin/deactivate\nConclusion\nWe've successfully collected the Python package requirements for a set of Python files.\nWe've created the environment to run those files and wrapped that environment into a\ntarball. Finally, we distributed the tarballed environment onto a different machine and\nwere immediately able to utilize an identical copy of Python environment from the\noriginal machine.",
      "tags": "Conda,Conda-Pack,Depfinder,Python",
      "url": "https://labs.quansight.org/blog/2020/06/portable-python-env/"
    },
    {
      "title": "Ibis: an idiomatic flavor of SQL for Python programmers",
      "text": "Ibis is a mature open-source project that has been in development for about 5 years; it currently has about 1350 stars on Github. It provides an interface to SQL for Python programmers and bridges the gap between remote storage & execution systems. These features provide authors the ability to:\n\nwrite backend-independent SQL expressions in Python);\naccess different database connections (eg. SQLite, OmniSci, Pandas); and\nconfirm visually their SQL queries with directed acyclic graphs (DAGs).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIbis is an alternative approach using databases that relies on Python rather than SQL experience. Typically, users have to learn an entirely new syntax or flavor of SQL to perform simple tasks. Now, those familiar with Python can avoid a new learning curve by using Ibis for composing and executing database queries using familiar Python syntaxes (i.e., similar to Pandas and Dask). Ibis assists in formation of SQL expressions by providing visual feedback about each Python object.\nThis post focuses on writing SQL expressions in Python and how to compose queries visually using Ibis. We'll demonstrate this with a SQLite database\u2014in particular, Sean Lahman\u2019s baseball database.\n\n\n\n\n\n\n\nConnecting to a database\u00b6To get started, we\u2019ll need to establish a database connection. Ibis makes it easy to create connections of different types. Let's go ahead and do this now with the function ibis.sqlite.connect (in this instance, the database used is a SQLite database):\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \n%matplotlib inline\nimport ibis\nimport pathlib, requests\n\ndb_path = pathlib.Path.cwd() / 'lahmansbaseballdb.sqlite'\n\nif not db_path.exists():          # Downloads database if necessary\n    with open(db_path, 'wb') as f:\n        URL = 'https://github.com/WebucatorTraining/lahman-baseball-mysql/raw/master/lahmansbaseballdb.sqlite'\n        req = requests.get(URL)\n        f.write(req.content)\n\nclient = ibis.sqlite.connect(db_path.name) # Opens SQLite database connection\n\n\n    \n\n\n\n\n\n\n\nThe client object represents our connection to the database. It is essential to use the appropriate Ibis connection\u2014SQLite in this case constructed through the ibis.sqlite namespace\u2014for the particular database.\nThis baseball database has 29 distinct tables; we can see by running the following code:\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \ntables = client.list_tables()\nprint(f'This database has {len(tables)} tables.')\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nThis database has 29 tables.\n\n\n\n\n\n\n\n\n\n\n\nSelecting and visualizing tables\u00b6\n\n\n\n\n\n\nDisplaying the list tables, gives the names of all the tables which include, among others, tables with identifiers\n\n{python}\n[...'appearances'...'halloffame', 'homegames', 'leagues', 'managers',...]\nLet's use the database connection to extract & examine dataframe representations of the halloffame and appearances tables from the baseball database. To do this, we can invoke the table method associated with the client object called with the appropriate names.\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \nhalloffame = client.table('halloffame', database='base')\nappearances = client.table('appearances', database='base')\n\n\n    \n\n\n\n\n\n\n\nAt the moment, the objects objects halloffame and appearances just constructed don\u2019t hold any data; instead, the objects are expressions of type TableExpr that represent putative operations applied to the data. The data itself is inert wherever it's actually located\u2014in this case, within the SQLite database. We can verify this by examining their types or by using assertions like this:\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \nprint(f'The object appearances has type {type(appearances).__name__}.')\nassert isinstance(halloffame, ibis.expr.types.TableExpr), 'Wrong type for halloffame'\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nThe object appearances has type TableExpr.\n\n\n\n\n\n\n\n\n\n\n\nWe can examine the contents of these Ibis table expressions using the TableExpr.limit or the TableExpr.head method (similar to the Pandas DataFrame.head method). That is, we can define an object sample that represents a sub-table comprising the first few rows of the halloffame table:\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \nsample = halloffame.head()\nprint(f'The object sample is of type {type(sample).__name__}')\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nThe object sample is of type TableExpr\n\n\n\n\n\n\n\n\n\n\n\nRemember, the object sample is a TableExpr object representing some SQL query to extracts a sub-table from a larger table. We can view the actual SQL query corresponding to sample by compiling it with the compile method and converting the result to a string:\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \nstr(sample.compile())\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[6]:\n\n\n\n\n\n'SELECT t0.\"ID\", t0.\"playerID\", t0.yearid, t0.\"votedBy\", t0.ballots, t0.needed, t0.votes, t0.inducted, t0.category, t0.needed_note \\nFROM base.halloffame AS t0\\n LIMIT ? OFFSET ?'\n\n\n\n\n\n\n\n\n\n\n\nAnother useful feature of Ibis is its ability to represent an SQL query as a DAG (Directed Acyclic Graph). For instance, evaluating the object sample at the interactive command prompt yields a visualization of a sequence of database operations:\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \nsample  # This produces the image below in a suitably enabled shell\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[7]:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis image of a DAG is produced using Graphviz; those familiar with Dask may have used a similar helpful feature to assemble task graphs.\nFinally, the actual sub-table corresponding to the expression sample can be extracted using the execute method (similar to compute in Dask). The result returned by executing the expression sample is a tidy Pandas DataFrame object.\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \nresult = sample.execute()\nprint(f'The type of result is {type(result).__name__}')\nresult    # Leading 5 rows of halloffame table)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nThe type of result is DataFrame\n\n\n\n\n\n\n    Out[8]:\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n  \n    \n      \n      ID\n      playerID\n      yearid\n      votedBy\n      ballots\n      needed\n      votes\n      inducted\n      category\n      needed_note\n    \n  \n  \n    \n      0\n      1\n      cobbty01\n      1936\n      BBWAA\n      226\n      170\n      222\n      Y\n      Player\n      None\n    \n    \n      1\n      2\n      ruthba01\n      1936\n      BBWAA\n      226\n      170\n      215\n      Y\n      Player\n      None\n    \n    \n      2\n      3\n      wagneho01\n      1936\n      BBWAA\n      226\n      170\n      215\n      Y\n      Player\n      None\n    \n    \n      3\n      4\n      mathech01\n      1936\n      BBWAA\n      226\n      170\n      205\n      Y\n      Player\n      None\n    \n    \n      4\n      5\n      johnswa01\n      1936\n      BBWAA\n      226\n      170\n      189\n      Y\n      Player\n      None\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nA similar extraction of the leading five rows from the appearances table (in one line)\ngives the following table with 23 columns:\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \nappearances.head().execute()  # Leading 5 rows of appearances table)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[9]:\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n  \n    \n      \n      ID\n      yearID\n      teamID\n      team_ID\n      lgID\n      playerID\n      G_all\n      GS\n      G_batting\n      G_defense\n      ...\n      G_2b\n      G_3b\n      G_ss\n      G_lf\n      G_cf\n      G_rf\n      G_of\n      G_dh\n      G_ph\n      G_pr\n    \n  \n  \n    \n      0\n      1\n      1871\n      TRO\n      8\n      NA\n      abercda01\n      1\n      1\n      1\n      1\n      ...\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      2\n      1871\n      RC1\n      7\n      NA\n      addybo01\n      25\n      25\n      25\n      25\n      ...\n      22\n      0\n      3\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      3\n      1871\n      CL1\n      3\n      NA\n      allisar01\n      29\n      29\n      29\n      29\n      ...\n      2\n      0\n      0\n      0\n      29\n      0\n      29\n      0\n      0\n      0\n    \n    \n      3\n      4\n      1871\n      WS3\n      9\n      NA\n      allisdo01\n      27\n      27\n      27\n      27\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      5\n      1871\n      RC1\n      7\n      NA\n      ansonca01\n      25\n      25\n      25\n      25\n      ...\n      2\n      20\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n    \n  \n\n5 rows \u00d7 23 columns\n\n\n\n\n\n\n\n\n\n\n\n\nFiltering and selecting data\u00b6\n\n\n\n\n\n\nAs mentioned earlier, Ibis uses familiar Pandas syntax to build SQL queries. As an example, let's look at the various kinds of entries in the category column from the halloffame table. A nice way to do this is to extract the relevant column with attribute access and apply the value_counts method. Remember, an invokation of execute is needed to realize the actual expression.\n\n\n\n\n\n\nIn\u00a0[10]:\n\n    \nhalloffame.category.value_counts().execute()\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[10]:\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n  \n    \n      \n      category\n      count\n    \n  \n  \n    \n      0\n      Manager\n      74\n    \n    \n      1\n      Pioneer/Executive\n      41\n    \n    \n      2\n      Player\n      4066\n    \n    \n      3\n      Umpire\n      10\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nThere are four different types of entries in this column, most of which are Players. To illustrate filtering and selection, we'll create a expression condition of boolean values corresponding to rows from the halloffame table in which the category column has the value Player. The boolean values represented by condition can be extracted from the table halloffame using brackets. The final result is bound to the identifier players.\n\n\n\n\n\n\nIn\u00a0[11]:\n\n    \ncondition = halloffame.category == 'Player'\nplayers = halloffame[condition]\n\n\n    \n\n\n\n\n\n\nIn\u00a0[12]:\n\n    \nplayers.execute() # take a look at this table\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[12]:\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n  \n    \n      \n      ID\n      playerID\n      yearid\n      votedBy\n      ballots\n      needed\n      votes\n      inducted\n      category\n      needed_note\n    \n  \n  \n    \n      0\n      1\n      cobbty01\n      1936\n      BBWAA\n      226.0\n      170.0\n      222.0\n      Y\n      Player\n      None\n    \n    \n      1\n      2\n      ruthba01\n      1936\n      BBWAA\n      226.0\n      170.0\n      215.0\n      Y\n      Player\n      None\n    \n    \n      2\n      3\n      wagneho01\n      1936\n      BBWAA\n      226.0\n      170.0\n      215.0\n      Y\n      Player\n      None\n    \n    \n      3\n      4\n      mathech01\n      1936\n      BBWAA\n      226.0\n      170.0\n      205.0\n      Y\n      Player\n      None\n    \n    \n      4\n      5\n      johnswa01\n      1936\n      BBWAA\n      226.0\n      170.0\n      189.0\n      Y\n      Player\n      None\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      4061\n      4187\n      lidgebr01\n      2018\n      BBWAA\n      422.0\n      317.0\n      0.0\n      N\n      Player\n      None\n    \n    \n      4062\n      4188\n      millwke01\n      2018\n      BBWAA\n      422.0\n      317.0\n      0.0\n      N\n      Player\n      None\n    \n    \n      4063\n      4189\n      zambrca01\n      2018\n      BBWAA\n      422.0\n      317.0\n      0.0\n      N\n      Player\n      None\n    \n    \n      4064\n      4190\n      morrija02\n      2018\n      Veterans\n      NaN\n      NaN\n      NaN\n      Y\n      Player\n      None\n    \n    \n      4065\n      4191\n      trammal01\n      2018\n      Veterans\n      NaN\n      NaN\n      NaN\n      Y\n      Player\n      None\n    \n  \n\n4066 rows \u00d7 10 columns\n\n\n\n\n\n\n\n\n\n\n\n\nJoining Ibis tables\u00b6\n\n\n\n\n\n\nIf we want a single view of the halloffame players and their appearances, we need to join) the tables halloffame and appearances. To do this, we\u2019ll perform an inner join based on the playerID columns of our players & appearances tables.\n\n\n\n\n\n\nIn\u00a0[13]:\n\n    \ncondition = players.playerID == appearances.playerID\n\n\n    \n\n\n\n\n\n\n\nWe notice that both the players and the appearances tables each have a column labelled ID. This column needs to be excluded from appearances; otherwise the overlapping columns will corrupt the computed join. Specifically, we want to filter out the ID and playerID columns from the appearances table. One strategy to do this is to use a list comprehension.\n\n\n\n\n\n\nIn\u00a0[14]:\n\n    \ncolumns = [col for col in appearances.columns if col not in ('playerID', 'ID')]\n\n\n    \n\n\n\n\n\n\n\nNow, we use the TableExpr.join method to compute an inner join of the players table and the filtered appearances table; the result is bound to the identifier unmaterialized.\n\n\n\n\n\n\nIn\u00a0[15]:\n\n    \nunmaterialized = players.join(appearances[columns], condition)\n\n\n    \n\n\n\n\n\n\n\nMaterializing the join\u00b6\n\n\n\n\n\n\nWe used the identifier unmaterialized just above to emphasize that the resulting expression is not a materialized view (that would be required to build new expressions). Without a materialized view, Ibis raises an exception (as demonstrated here).\n\n\n\n\n\n\nIn\u00a0[16]:\n\n    \ntry:\n    unmaterialized.distinct()\nexcept Exception as e:\n    print('Unable to execute \"unmaterialized.distinct()\"')\n    print(repr(e))\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nUnable to execute \"unmaterialized.distinct()\"\nIbisError('Table operation is not yet materialized')\n\n\n\n\n\n\n\n\n\n\n\nThe distinct method in the preceding code behaves like the Pandas DataFrame.drop_duplicates method, i.e., it drops duplicated rows. We can obtain such a materialized view to circumvent the exception above using the expression's materialize method.\n\n\n\n\n\n\nIn\u00a0[17]:\n\n    \njoin = unmaterialized.materialize().distinct()\n\n\n    \n\n\n\n\n\n\n\nThe code above completes the join and binds the resulting expression to the materialized object join; here is a sample of the leading five rows of our joined data (notice the result has 31 columns).\n\n\n\n\n\n\nIn\u00a0[18]:\n\n    \njoin.head().execute()\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[18]:\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n  \n    \n      \n      ID\n      playerID\n      yearid\n      votedBy\n      ballots\n      needed\n      votes\n      inducted\n      category\n      needed_note\n      ...\n      G_2b\n      G_3b\n      G_ss\n      G_lf\n      G_cf\n      G_rf\n      G_of\n      G_dh\n      G_ph\n      G_pr\n    \n  \n  \n    \n      0\n      2861\n      aaronha01\n      1982\n      BBWAA\n      415\n      312\n      406\n      Y\n      Player\n      None\n      ...\n      16\n      0\n      15\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      1\n      3744\n      abbotji01\n      2005\n      BBWAA\n      516\n      387\n      13\n      N\n      Player\n      None\n      ...\n      16\n      0\n      15\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2\n      147\n      adamsba01\n      1937\n      BBWAA\n      201\n      151\n      8\n      N\n      Player\n      None\n      ...\n      16\n      0\n      15\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      3\n      260\n      adamsba01\n      1938\n      BBWAA\n      262\n      197\n      11\n      N\n      Player\n      None\n      ...\n      16\n      0\n      15\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      4\n      385\n      adamsba01\n      1939\n      BBWAA\n      274\n      206\n      11\n      N\n      Player\n      None\n      ...\n      16\n      0\n      15\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n5 rows \u00d7 31 columns\n\n\n\n\n\n\n\n\n\n\n\n\nIbis supports other join strategies as methods of the class TableExpr. The following list comprehension shows us what they are.\n\n\n\n\n\n\nIn\u00a0[19]:\n\n    \n[method_name for method_name in dir(players) if 'join' in method_name]\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[19]:\n\n\n\n\n\n['anti_join',\n 'any_inner_join',\n 'any_left_join',\n 'asof_join',\n 'cross_join',\n 'inner_join',\n 'join',\n 'left_join',\n 'outer_join',\n 'semi_join']\n\n\n\n\n\n\n\n\n\n\n\nExecuting an expression\u00b6\n\n\n\n\n\n\nWe'll now expand the expression join as a Pandas DataFrame object. We'll use this DataFrame to answer the following question:\n\nHow many pitchers have been inducted into the hall of fame?\nSome of the \"hitters\" have also been \"pitchers\", so we\u2019ll need to filter out rows corresponding to those appearances from the table join. That is, to identify a specific player as a \"pitcher\", we\u2019ll choose those players who played mostly as pitchers; in particular, we\u2019ll take 100 games as an arbitrary threshold between pitchers and non-pitchers. The column G_p from the table join represents the numbers of games a player played as a pitcher; the desired filtering expression, then, is as follows:\n\n\n\n\n\n\nIn\u00a0[20]:\n\n    \npitchers = join[join.G_p > 100]\n\n\n    \n\n\n\n\n\n\n\nNext, we group the pitchers table based on a specific pair of columns (stored as a list cols) and then count them annually using a groupby with a count aggregation.\n\n\n\n\n\n\nIn\u00a0[21]:\n\n    \ncols = [pitchers.inducted, pitchers.yearID]\ngrouped_pitchers = pitchers.groupby(cols).count()\n\n\n    \n\n\n\n\n\n\n\nThe expression grouped_pitchers is still an Ibis TableExpr; as we've seen several times now, it can be realized as a Pandas DataFrame using the execute method. The resulting DataFrame's index can be set as a multi-index using the inducted and yearID columns.\n\n\n\n\n\n\nIn\u00a0[22]:\n\n    \ndf = grouped_pitchers.execute().set_index('inducted yearID'.split())\ndf\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[22]:\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n  \n    \n      \n      \n      count\n    \n    \n      inducted\n      yearID\n      \n    \n  \n  \n    \n      N\n      1936\n      105\n    \n    \n      1937\n      106\n    \n    \n      1938\n      114\n    \n    \n      1939\n      99\n    \n    \n      1942\n      67\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      Y\n      2014\n      3\n    \n    \n      2015\n      4\n    \n    \n      2016\n      2\n    \n    \n      2017\n      3\n    \n    \n      2018\n      6\n    \n  \n\n150 rows \u00d7 1 columns\n\n\n\n\n\n\n\n\n\n\n\n\nThe dataframe df has counts of the number of pitchers who were (inducted index 'Y') and were not (inducted index 'N') inducted into the baseball Hall of Fame in a given year. We'll pull in all the relevant counts of inductees into a dataframe count_inducted_pitchers. Notice the use of the Pandas DataFrame.fillna method to assign 0s in rows appropriately (i.e., reflecting that no pitchers were inducted into the Hall of Fame in those years).\n\n\n\n\n\n\nIn\u00a0[23]:\n\n    \ncount_inducted_pitchers = df.loc['Y'].fillna(0).rename({'count':'Inducted pitchers'})\ncount_inducted_pitchers\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[23]:\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n  \n    \n      \n      count\n    \n    \n      yearID\n      \n    \n  \n  \n    \n      1936\n      5\n    \n    \n      1937\n      3\n    \n    \n      1938\n      1\n    \n    \n      1939\n      7\n    \n    \n      1942\n      1\n    \n    \n      ...\n      ...\n    \n    \n      2014\n      3\n    \n    \n      2015\n      4\n    \n    \n      2016\n      2\n    \n    \n      2017\n      3\n    \n    \n      2018\n      6\n    \n  \n\n76 rows \u00d7 1 columns\n\n\n\n\n\n\n\n\n\n\n\n\nThe Pandas DataFrame & Series classes have a convenient plotting interface. We'll use a dictionary options to specify keyword arguments to tidy the final invokation of plot.bar.\n\n\n\n\n\n\nIn\u00a0[24]:\n\n    \noptions = dict(figsize=(15, 5), grid=True, legend=None)\ncount_inducted_pitchers.plot.bar(**options);\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat next?\u00b6That's it! In future posts, we\u2019ll explore other backends and visualize more Ibis objects. If you\u2019d like to contribute to Ibis, please take a look at\nIbis contributing guide and OpenTeams.",
      "tags": "",
      "url": "https://labs.quansight.org/blog/2020/06/ibis-an-idiomatic-flavor-of-sql-for-python-programmers/"
    },
    {
      "title": "Highlights of the Ibis 1.3 release",
      "text": "Ibis 1.3 was just released, after 8 months of development work, with 104 new commits from 16 unique contributors. What is new? In this blog post we will discuss some important features in this new version!\nFirst, if you are new to the Ibis framework world, you can check this blog post I wrote last year, with some introductory information about it.\nSome highlighted features of this new version are:\n\nAddition of a PySpark backend\nImprovement of geospatial support\nAddition of JSON, JSONB and UUID data types\nInitial support for Python 3.8 added and support for Python 3.5 dropped\nAdded new backends and geospatial methods to the documentation\nRenamed the mapd backend to omniscidb\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis blog post is divided into different sections:\n\nOmniSciDB\nPostgreSQL\nPySpark\nGeospatial support\nPython versions support\n\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \nimport ibis\nimport pandas as pd\n\n\n    \n\n\n\n\n\n\n\nOmniSciDB\u00b6The mapd backend is now named omniscidb!\nAn important feature of omniscidb is that now you can define if the connection is IPC (Inter-Process Communication), and you can also specify the GPU device ID you want to use (if you have a NVIDIA card, supported by cudf).\nIPC is used to provide shared data support between processes. OmniSciDB uses Apache Arrow to provide IPC support.\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \ncon_omni = ibis.omniscidb.connect(\n    host='localhost', \n    port='6274',\n    user='admin',\n    password='HyperInteractive',\n    database='ibis_testing',\n    ipc=False,\n    gpu_device=None\n)\ncon_omni.list_tables()\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[2]:\n\n\n\n\n\n['diamonds', 'batting', 'awards_players', 'functional_alltypes', 'geo']\n\n\n\n\n\n\n\n\n\n\n\nAlso you can now specify ipc or gpu_device directly to the execute method:\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \nt = con_omni.table('functional_alltypes')\nexpr = t[['id', 'bool_col']].head(5)\ndf = expr.execute(ipc=False, gpu_device=None)\ndf\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[3]:\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n  \n    \n      \n      id\n      bool_col\n    \n  \n  \n    \n      0\n      6690\n      True\n    \n    \n      1\n      6691\n      False\n    \n    \n      2\n      6692\n      True\n    \n    \n      3\n      6693\n      False\n    \n    \n      4\n      6694\n      True\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can imagine, the result of df is a pandas.DataFrame\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \ntype(df)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[4]:\n\n\n\n\n\npandas.core.frame.DataFrame\n\n\n\n\n\n\n\n\n\n\n\nBut if you are using gpu_device the result would be a cudf :)\n\n\n\n\n\n\n\n\nNote: when IPC=True is used, the code needs to be executed on the same machine where the database is running\nNote: when gpu_device is used, 1) it uses IPC (see the note above) and 2) it needs a NVIDIA card supported by cudf.\n\nAnother interesting feature is that now omniscidb also supports shapefiles (input) and geopandas (output)!\nCheck out the Geospatial support section below to see more details!\nAlso the new version adds translations for more window operations for the omniscidb backend, such as: \nDenseRank, RowNumber, MinRank, Count, PercentRank/CumeDist.\nFor more information about window operations, check the \nWindow functions\ndocumentation section.\n\n\n\n\n\n\n\nPostgreSQL\u00b6Some of the highlighted features for the PostgreSQL backend are new data types included, such as:\nJSON, JSONB and UUID.\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \nfrom uuid import uuid4 \nuuid_value = ibis.literal(uuid4(), type='uuid')\nuuid_value == ibis.literal(uuid4(), type='uuid')\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[5]:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \nimport json\njson_value = ibis.literal(json.dumps({\"id\": 1}), type='json')\njson_value\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[6]:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \njsonb_value = ibis.literal(json.dumps({\"id\": 1}).encode('utf8'), type='jsonb')\njsonb_value\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[7]:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother important new features on PostgreSQL backend is the support of new geospatial operations, such as\n\nGeometryType\nGeometryN\nIsValid\nLineLocatePoint\nLineMerge\nLineSubstring\nOrderingEquals\nUnion\n\nAlso, now it has support for two geospatial data types: MULTIPOINT and MULTILINESTRING.\nCheck out the Geospatial support section below to see some usage examples of geospatial operations!\n\n\n\n\n\n\n\nPySpark\u00b6This new version also includes support for a new backend: PySpark!\nLet's do the first steps with this new backend starting with a Spark session creation.\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \nimport os\n\nimport pyspark\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.types as pt\nfrom pathlib import Path\n\n# spark session and pyspark connection\nspark_session = SparkSession.builder.getOrCreate()\ncon_pyspark = ibis.pyspark.connect(session=spark_session)\n\n\n    \n\n\n\n\n\n\n\nWe can use spark or pandas for reading from CSV file. In this example, we will use pandas.\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \ndata_directory = Path(\n    os.path.join(\n        os.path.dirname(ibis.__path__[0]),\n        'ci',\n        'ibis-testing-data'\n    )\n)\n\npd_df_alltypes = pd.read_csv(data_directory / 'functional_alltypes.csv')\npd_df_alltypes.info()\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7300 entries, 0 to 7299\nData columns (total 15 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   index            7300 non-null   int64  \n 1   Unnamed: 0       7300 non-null   int64  \n 2   id               7300 non-null   int64  \n 3   bool_col         7300 non-null   int64  \n 4   tinyint_col      7300 non-null   int64  \n 5   smallint_col     7300 non-null   int64  \n 6   int_col          7300 non-null   int64  \n 7   bigint_col       7300 non-null   int64  \n 8   float_col        7300 non-null   float64\n 9   double_col       7300 non-null   float64\n 10  date_string_col  7300 non-null   object \n 11  string_col       7300 non-null   int64  \n 12  timestamp_col    7300 non-null   object \n 13  year             7300 non-null   int64  \n 14  month            7300 non-null   int64  \ndtypes: float64(2), int64(11), object(2)\nmemory usage: 855.6+ KB\n\n\n\n\n\n\n\n\n\n\n\nNow, we can create a Spark DataFrame and we will create a temporary view from this data frame. Also we should enforce the desired types for each column.\n\n\n\n\n\n\nIn\u00a0[10]:\n\n    \ndef pyspark_cast(df, col_types):\n    for col, dtype in col_types.items():\n        df = df.withColumn(col, df[col].cast(dtype))\n    return df\n\nps_df_alltypes = spark_session.createDataFrame(pd_df_alltypes)\n\nps_df_alltypes = pyspark_cast(\n    ps_df_alltypes, {\n        'index': 'integer',\n        'Unnamed: 0': 'integer',\n        'id': 'integer',\n        'bool_col': 'boolean',\n        'tinyint_col': 'byte',\n        'smallint_col': 'short',\n        'int_col': 'integer',\n        'bigint_col': 'long',\n        'float_col': 'float',\n        'double_col': 'double',\n        'date_string_col': 'string',\n        'string_col': 'string',\n        'timestamp_col': 'timestamp',\n        'year': 'integer',\n        'month': 'integer'\n    }\n)\n\n# use ``SparkSession`` to create a table\nps_df_alltypes.createOrReplaceTempView('functional_alltypes')\ncon_pyspark.list_tables()\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[10]:\n\n\n\n\n\n['functional_alltypes']\n\n\n\n\n\n\n\n\n\n\n\nCheck if all columns were created with the desired data type:\n\n\n\n\n\n\nIn\u00a0[11]:\n\n    \nt = con_pyspark.table('functional_alltypes')\nt\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[11]:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifferent than a SQL backend, that returns a SQL statement, the returned \nobject from the PySpark compile method is a PySpark DataFrame:\n\n\n\n\n\n\nIn\u00a0[12]:\n\n    \nexpr = t.head()\nexpr_comp = expr.compile()\ntype(expr_comp)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[12]:\n\n\n\n\n\npyspark.sql.dataframe.DataFrame\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[13]:\n\n    \nexpr_comp\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[13]:\n\n\n\n\n\nDataFrame[index: int, Unnamed: 0: int, id: int, bool_col: boolean, tinyint_col: tinyint, smallint_col: smallint, int_col: int, bigint_col: bigint, float_col: float, double_col: double, date_string_col: string, string_col: string, timestamp_col: timestamp, year: int, month: int]\n\n\n\n\n\n\n\n\n\n\n\nTo convert the compiled expression to a Pandas DataFrame, you can use the toPandas method.\nThe result should be the same as that returned by the execute method.\n\n\n\n\n\n\nIn\u00a0[14]:\n\n    \nassert all(expr.execute() == expr_comp.toPandas())\n\n\n    \n\n\n\n\n\n\nIn\u00a0[15]:\n\n    \nexpr.execute()\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[15]:\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n  \n    \n      \n      index\n      Unnamed: 0\n      id\n      bool_col\n      tinyint_col\n      smallint_col\n      int_col\n      bigint_col\n      float_col\n      double_col\n      date_string_col\n      string_col\n      timestamp_col\n      year\n      month\n    \n  \n  \n    \n      0\n      0\n      0\n      6690\n      True\n      0\n      0\n      0\n      0\n      0.0\n      0.0\n      11/01/10\n      0\n      2010-11-01 00:00:00.000\n      2010\n      11\n    \n    \n      1\n      1\n      1\n      6691\n      False\n      1\n      1\n      1\n      10\n      1.1\n      10.1\n      11/01/10\n      1\n      2010-11-01 00:01:00.000\n      2010\n      11\n    \n    \n      2\n      2\n      2\n      6692\n      True\n      2\n      2\n      2\n      20\n      2.2\n      20.2\n      11/01/10\n      2\n      2010-11-01 00:02:00.100\n      2010\n      11\n    \n    \n      3\n      3\n      3\n      6693\n      False\n      3\n      3\n      3\n      30\n      3.3\n      30.3\n      11/01/10\n      3\n      2010-11-01 00:03:00.300\n      2010\n      11\n    \n    \n      4\n      4\n      4\n      6694\n      True\n      4\n      4\n      4\n      40\n      4.4\n      40.4\n      11/01/10\n      4\n      2010-11-01 00:04:00.600\n      2010\n      11\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nTo finish this section, we can play a little bit with some aggregation operations.\n\n\n\n\n\n\nIn\u00a0[16]:\n\n    \nexpr = t\nexpr = expr.groupby('string_col').aggregate(\n    int_col_mean=t.int_col.mean(),\n    int_col_sum=t.int_col.sum(),\n    int_col_count=t.int_col.count(),\n)\nexpr.execute()\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[16]:\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n  \n    \n      \n      string_col\n      int_col_count\n      int_col_mean\n      int_col_sum\n    \n  \n  \n    \n      0\n      7\n      730\n      7.0\n      5110\n    \n    \n      1\n      3\n      730\n      3.0\n      2190\n    \n    \n      2\n      8\n      730\n      8.0\n      5840\n    \n    \n      3\n      0\n      730\n      0.0\n      0\n    \n    \n      4\n      5\n      730\n      5.0\n      3650\n    \n    \n      5\n      6\n      730\n      6.0\n      4380\n    \n    \n      6\n      9\n      730\n      9.0\n      6570\n    \n    \n      7\n      1\n      730\n      1.0\n      730\n    \n    \n      8\n      4\n      730\n      4.0\n      2920\n    \n    \n      9\n      2\n      730\n      2.0\n      1460\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck out the PySpark Ibis backend \nAPI documentation \nand the tutorials\nfor more details.\n\n\n\n\n\n\n\nGeospatial support\u00b6Currently, ibis.omniscidb and ibis.postgres are the only Ibis backends that support geospatial features.\nIn this section we will check some geospatial features using the PostgreSQL backend.\n\n\n\n\n\n\nIn\u00a0[17]:\n\n    \ncon_psql = ibis.postgres.connect(\n    host='localhost',\n    port=5432,\n    user='postgres',\n    password='postgres',\n    database='ibis_testing'\n)\ncon_psql.list_tables()\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[17]:\n\n\n\n\n\n['array_types',\n 'awards_players',\n 'batting',\n 'diamonds',\n 'films',\n 'functional_alltypes',\n 'geo',\n 'geography_columns',\n 'geometry_columns',\n 'intervals',\n 'not_supported_intervals',\n 'raster_columns',\n 'raster_overviews',\n 'spatial_ref_sys',\n 'tzone']\n\n\n\n\n\n\n\n\n\n\n\nTwo important features are that it support shape objects (input) and geopandas dataframe (output)!\nSo, let's import shapely to create a simple shape point and polygon.\n\n\n\n\n\n\nIn\u00a0[18]:\n\n    \nimport shapely\n\nshp_point = shapely.geometry.Point((20, 10))\nshp_point\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[18]:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[19]:\n\n    \nshp_polygon_1 = shapely.geometry.Polygon([(20, 10), (40, 30), (40, 20), (20, 10)])\nshp_polygon_1\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[19]:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, let's create a Ibis table expression to manipulate a \"geo\" table:\n\n\n\n\n\n\nIn\u00a0[20]:\n\n    \nt_geo = con_psql.table('geo')\ndf_geo = t_geo.execute()\ndf_geo\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[20]:\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n  \n    \n      \n      id\n      geo_point\n      geo_linestring\n      geo_polygon\n      geo_multipolygon\n    \n  \n  \n    \n      0\n      1\n      POINT (0.00000 0.00000)\n      LINESTRING (0 0, 1 1)\n      POLYGON ((30 10, 40 40, 20 40, 10 20, 30 10))\n      (POLYGON ((30 20, 45 40, 10 40, 30 20)), POLYG...\n    \n    \n      1\n      2\n      POINT (1.00000 1.00000)\n      LINESTRING (1 1, 2 2)\n      POLYGON ((35 10, 45 45, 15 40, 10 20, 35 10), ...\n      (POLYGON ((40 40, 20 45, 45 30, 40 40)), POLYG...\n    \n    \n      2\n      3\n      POINT (2.00000 2.00000)\n      LINESTRING (2 2, 3 3)\n      POLYGON ((2 2, 3 3, 4 4, 5 5, 5 2, 2 2))\n      (POLYGON ((2 2, 3 3, 4 4, 5 5, 5 2, 2 2)))\n    \n    \n      3\n      4\n      POINT (3.00000 3.00000)\n      LINESTRING (3 3, 4 4)\n      POLYGON ((3 3, 4 4, 5 5, 6 6, 6 3, 3 3))\n      (POLYGON ((3 3, 4 4, 5 5, 6 6, 6 3, 3 3)))\n    \n    \n      4\n      5\n      POINT (4.00000 4.00000)\n      LINESTRING (4 4, 5 5)\n      POLYGON ((4 4, 5 5, 6 6, 7 7, 7 4, 4 4))\n      (POLYGON ((4 4, 5 5, 6 6, 7 7, 7 4, 4 4)))\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd the type of df_geo is ...  a geopandas dataframe!\n\n\n\n\n\n\nIn\u00a0[21]:\n\n    \ntype(df_geo)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[21]:\n\n\n\n\n\ngeopandas.geodataframe.GeoDataFrame\n\n\n\n\n\n\n\n\n\n\n\nSo you can take the advantage of GeoPandas features too!\n\n\n\n\n\n\nIn\u00a0[22]:\n\n    \ndf_geo.set_geometry('geo_multipolygon').head(1).plot();\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, let's check if there are any geo_multipolygon's that contain the shape point we just created.\n\n\n\n\n\n\nIn\u00a0[23]:\n\n    \nt_geo[\n    t_geo.geo_multipolygon, \n    t_geo['geo_multipolygon'].contains(shp_point).name('contains_point_1')\n].execute()\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[23]:\n\n\n\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\n\n  \n    \n      \n      geo_multipolygon\n      contains_point_1\n    \n  \n  \n    \n      0\n      MULTIPOLYGON (((30.00000 20.00000, 45.00000 40...\n      True\n    \n    \n      1\n      MULTIPOLYGON (((40.00000 40.00000, 20.00000 45...\n      True\n    \n    \n      2\n      MULTIPOLYGON (((2.00000 2.00000, 3.00000 3.000...\n      False\n    \n    \n      3\n      MULTIPOLYGON (((3.00000 3.00000, 4.00000 4.000...\n      False\n    \n    \n      4\n      MULTIPOLYGON (((4.00000 4.00000, 5.00000 5.000...\n      False\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nSo, as expected, just the first two multipolygons contain a point with coordinates (20, 10).\nFor more examples of Geospatial Analysis with Ibis, \ncheck this nice tutorial\nwritten by Ian Rose!\n\n\n\n\n\n\n\nPython versions support\u00b6Ibis 1.3 added initial support for Python 3.8 and dropped support for Python 3.5.\nNote: currently, the OmniSciDB and \nPySpark backends\nare not supported on Python 3.8.\n\n\n\n\n\n\n\n\nFinal words\u00b6Do you want to play more with Ibis framework?\nYou can install it from PyPI:\n\npython -m pip install --upgrade ibis-framework==1.3.0\nOr from conda-forge:\n\nconda install ibis-framework=1.3.0 -c conda-forge\nCheck out some interesting tutorials to help you to start on Ibis: https://docs.ibis-project.org/tutorial.html. If you are coming from the SQL world, maybe Ibis for SQL Programmers documentation section will be helpful. Have fun!",
      "tags": "Ibis",
      "url": "https://labs.quansight.org/blog/2020/05/ibis-1.3-release/"
    },
    {
      "title": "Thanking the people behind Spyder 4",
      "text": "After more than three years in development and more than 5000 commits from 60 authors around the world, Spyder 4 finally saw the light on December 5, 2019!\nI decided to wait until now to write a blogpost about it because shortly after the initial release, we found several critical performance issues and some regressions with respect to Spyder 3, most of which are fixed now in version 4.1.2, released on April 3rd 2020.\n\n\nThis new release comes with a lengthy list of user-requested features aimed at providing an enhanced development experience at the level of top general-purpose editors and IDEs, while strengthening Spyder's specialized focus on scientific programming in Python.\nThe interested reader can take a look at some of them in previous blog posts, and in detail in our Changelog.\nHowever, this post is not meant to describe those improvements, but to acknowledge all people that contributed to making Spyder 4 possible.\nSpyder 4 is the first version developed with financial support from multiple companies and organizations, as well as donations by the international user community.\nHowever, as a project, we couldn't have been able to reach the level of maturity needed to receive and handle that support without the pivotal opportunities Travis Oliphant, former CEO of Anaconda and current leader of Quansight, gave me to work in Scientific Python.\nThanks to him, I became part of Anaconda Inc. in 2015; I was able to hire a small small team to improve Spyder within Anaconda in late 2016; and I was hired by Quansight to work solely on Spyder in 2018.\nAs with other projects in our ecosystem, such as Bokeh, Dask and Numba, Spyder benefited immensely from Travis' trust in the role these efforts and ours could play in the future.\nHe certainly believed in the vision their maintainers worked so hard to make a reality, even if their beginnings were humble and their chances of survival uncertain.\nTherefore, my first big acknowledgment is to Travis: thanks for giving us a chance!\nI also want to thank our community for its continued support.\nAs I've witnessed during my years as Spyder's lead developer, many newbies and veterans alike keep choosing Spyder as their primary tool for scientific programming in Python.\nIt's really you, the members of this fantastic community, which keep Spyder relevant in a highly competitive field by using and contributing back to it.\nFurthermore, when the future looked grim, after my team and I were let go from Anaconda at the end of 2017 (not by Travis' decision), a lot of users came to our rescue by making donations through our Open Collective page.\nThat, and a NumFOCUS development grant we received the next year, filled us with confidence and allowed us to continue with Spyder's development in 2018, even after losing part of our team in the process.\nLast year it was also a pleasant surprise to learn that several companies were interested in seeing Spyder prosper and thrive.\nThrough Quansight Labs and its Community Work Order concept, we were able to sign contracts with two of them: TDK-Micronas and Kite.\nTheir support was critical to finish Spyder 4 because it allowed me to hire most of my old Anaconda team back, plus two new additions, to work full-time on the project.\nTherefore, I can't thank them enough for showing up just at the right time!\nAnd finally, even when I am often seen as the public face of Spyder, due to my presence in our issue tracker and Stack Overflow, it's really the Spyder team that is in charge of implementing new features and fixing most bugs.\nSo my last round of acknowledgments goes to them.\nI was fortunate enough to convince some of the most talented Colombian software developers to work for the project, and to attract the interest of several other equally accomplished developers from around the world.\nAll of them did a hell of a job in this release!\nJean-S\u00e9bastien Gosselin contributed our new Plots pane in its entirety; Quentin Peter did a complete re-architecting of our IPython console, which enabled numerous improvements to our debugger; Jitse Niesen added auto-saving functionality to our editor; Brian Olsen contributed the initial implementation of the runcell command; and CAM Gerlach made significant improvements to our documentation.\nGonzalo Pe\u00f1a-Castellanos helped us to greatly improve the user experience of code completion and linting in the editor, implemented most of the enhancements to the Files pane, and refactored and improved our configuration system; Edgar Margffoy single-handedly created a client to support the same protocol used by VSCode to provide completion and linting for lots of programming languages, added code snippet completions and vastly improved code folding in the editor; Daniel Althviz developed the necessary infrastructure to install and use Kite smoothly within Spyder, and added the new object viewer to the Variable Explorer.\nFinally, our junior developers, Stephannie Jimenez and Juanita Gomez, although still finding their way around our complex codebase, managed to make important contributions, such as improving the icons we use per file type in Files (Juanita), and allowing users to run code in an external system terminal on macOS (Stephannie).\nI hope you all enjoy the results of this massive effort!\nAnd happy Spydering!!!",
      "tags": "Labs,Spyder",
      "url": "https://labs.quansight.org/blog/2020/04/thanking-the-people-behind-spyder-4/"
    },
    {
      "title": "Introducing ndindex, a Python library for manipulating indices of ndarrays",
      "text": "One of the most important features of NumPy arrays is their indexing\nsemantics. By \"indexing\" I mean anything that happens inside square brackets,\nfor example, a[4::-1, 0, ..., [0, 1], np.newaxis]. NumPy's index semantics\nare very expressive and powerful, and this is one of the reasons the library\nis so popular.\nIndex objects can be represented and manipulated directly. For example, the\nabove index is (slice(4, None, -1), 0, Ellipsis, [0, 1], None). If you are\nany author of a library that tries to replicate NumPy array semantics, you\nwill have to work with these objects. However, they are often difficult to\nwork with:\n\n\nThe different types that are valid indices for NumPy arrays do not have a\nuniform API. Most of the types are also standard Python types, such as\ntuple, list, int, and None, which are usually unrelated to indexing.\n\n\nThose objects that are specific to indexes, such as slice and Ellipsis\ndo not make any assumptions about their underlying semantics. For example,\nPython lets you create slice(None, None, 0) or slice(0, 0.5) even though\na[::0] and a[0:0.5] would be always be an IndexError on a NumPy array.\n\n\nSome index objects, such as slice, list, and ndarray are not hashable.\n\n\nNumPy itself does not offer much in the way of helper functions to work with\nthese objects.\n\n\nThese limitations may be annoying, but are easy enough to live with. The real\nchallenge when working with indices comes when you try to manipulate them.\nSlices in particular are challenging to work with because the rich meaning of\nslice semantics. Writing formulas for even very simple things is a real\nchallenge with slices. slice(start, stop, step) (corresponding to\na[start:stop:step]) has fundamentally different meaning depending on whether\nstart,stop, or step are negative, nonnegative, or None. As an example,\ntake a[4:-2:-2], where a is a one-dimensional array. This slices every\nother element from the third element to the second from the last. What will\nthe shape of this sliced array be? The answer is (0,) if the original shape\nis less than 1 or greater than 5, and (1,) otherwise.\nCode that manipulates slices will tend to have a lot of if/else chains for\nthese different cases. And due to 0-based indexing, half-open semantics,\nwraparound behavior, clipping, and step logic, the formulas are often quite\ndifficult to write down.\n\nndindex\nThis is where ndindex comes in. ndindex is a new library that provides high\nlevel objects representing the various objects that can index NumPy arrays.\nThese objects automatically canonicalize under the assumption of NumPy\nindexing semantics, and can be manipulated with a uniform API. All ndindex\ntypes have a .args that can be used to access the arguments used to create\nthe object, and they are all hashable.\n>>> from ndindex import Slice, Integer, Tuple\n>>> Slice(0, 3)\nSlice(0, 3, 1)\n>>> idx = Tuple(Slice(0, 10), Integer(0))\n>>> idx.args\n(Slice(0, 10, 1), Integer(0))\n>>> [i.args for i in idx.args]\n[(0, 10, 1), (0,)]\n\nThe goal of ndindex is to give 100% correct semantics as defined by NumPy's\nndarray. This means that ndindex will not make a transformation on an index\nobject unless it is correct for all possible input array shapes. The only\nexception to this rule is that ndindex assumes that any given index will not\nraise IndexError (for instance, from an out of bounds integer index or from\ntoo few dimensions). For those operations where the array shape is known,\nthere is a reduce method to reduce an index to a simpler index that is\nequivalent for the given shape.\nFeatures\nndindex is still a work in progress. The following things are currently\nimplemented:\n\n\nSlice, Integer, and Tuple\n\n\nConstructing a class puts it into canonical form. For example\n>>> from ndindex import Slice\n>>> Slice(None, 12)\nSlice(0, 12, 1)\n\n\n\nObject arguments can be accessed with idx.args\n>>> Slice(1, 3).args\n(1, 3, 1)\n\n\n\nAll ndindex objects are hashable and can be used as dictionary keys.\n\n\nA real index object can be accessed with idx.raw. Use this to use an\nndindex to index an array.\n>>> s = Slice(0, 2)\n>>> from numpy import arange\n>>> arange(4)[s.raw]\narray([0, 1])\n\n\n\nlen() computes the maximum length of an index over a given axis.\n>>> len(Slice(2, 10, 3))\n3\n>>> len(arange(10)[2:10:3])\n3\n\n\n\nidx.reduce(shape) reduces an index to an equivalent index over an array\nwith the given shape.\n>>> Slice(2, -1).reduce((10,))\nSlice(2, 9, 1)\n>>> arange(10)[2:-1]\narray([2, 3, 4, 5, 6, 7, 8])\n>>> arange(10)[2:9:1]\narray([2, 3, 4, 5, 6, 7, 8])\n\n\n\nThe following things are not yet implemented, but are planned.\n\n\nidx.newshape(shape) returns the shape of a[idx], assuming a has shape\nshape.\n\n\nellipsis, Newaxis, IntegerArray, and BooleanArray types, so that all\ntypes of indexing are supported.\n\n\ni1[i2] will create a new ndindex i3 (when possible) so that\na[i1][i2] == a[i3].\n\n\nsplit(i0, [i1, i2, ...]) will return a list of indices [j1, j2, ...]\nsuch that a[i0] = concat(a[i1][j1], a[i2][j2], ...)\n\n\ni1 + i2 will produce a single index so that a[i1 + i2] gives all the\nelements of a[i1] and a[i2].\n\n\nSupport NEP 21 outer indexing and vectorized\nindexin.\n\n\nAnd more. If there is something you would like to see this library be able to\ndo, please open an issue. Pull\nrequests are welcome as well.\nTesting and correctness\nThe most important priority for a library like this is correctness. Index\nmanipulations, and especially slice manipulations, are complicated to code\ncorrectly, and the code for them typically involves dozens of different\nbranches for different cases and formulas that can be difficult to figure out.\nIn order to assure correctness, all operations are tested extensively against\nNumPy itself to ensure they give the same results. The basic idea is to take\nthe pure Python index and the ndindex(index).raw, or in the case of a\ntransformation, the before and after raw index, and index a numpy.arange\nwith them (the input array itself doesn't matter, so long as its values are\ndistinct). If they do not give the same output array, or do not both produce\nthe same error (like an IndexError), the code is not correct. For example,\nthe reduce method can be verified by checking that a[idx.raw] and\na[idx.reduce(a.shape).raw] produce the same sub-arrays for all possible\ninput arrays a and ndindex objects idx.\nThere are two primary types of tests that ndindex employs to verify this:\n\n\nExhaustive tests. These test every possible value in some range. For\nexample, Slice tests test all possible start, stop, and step values\nin the range [-10, 10], as well as None, on numpy.arange(n) for n in\nthe range [0, 10]. This is the best type of test, because it checks every\npossible case. Unfortunately, it is often impossible to do full exhaustive\ntesting due to combinatorial explosion.\nFor example, here is the exhaustive test for Slice.reduce:\ndef _iterslice(start_range=(-10, 10), stop_range=(-10, 10), step_range=(-10, 10)):\n  for start in chain(range(*start_range), [None]):\n      for stop in chain(range(*stop_range), [None]):\n          for step in chain(range(*step_range), [None]):\n              yield (start, stop, step)\n\ndef test_slice_reduce_exhaustive():\n  for n in range(10):\n      a = arange(n)\n      for start, stop, step in _iterslice():\n          try:\n              s = Slice(start, stop, step)\n          except ValueError:\n              continue\n\n          check_same(a, s.raw, func=lambda x: x.reduce((n,)))\n\n          reduced = s.reduce((n,))\n          assert reduced.start >= 0\n          # We cannot require stop > 0 because if stop = None and step < 0, the\n          # only equivalent stop that includes 0 is negative.\n          assert reduced.stop != None\n          assert len(reduced) == len(a[reduced.raw]), (s, n)\n\ncheck_same is a helper\nfunction\nthat ensures that two indices give either the exact same subarray or raise\nthe exact same exception. The test checks all a[start:stop:step] where\na is an array with shape from 0 to 10, and start, stop, and step\nrange from -10 to 10 or None. We also test some basic invariants, such\nas that Slice.reduce always returns a slice with non-None arguments and\nthat the start is nonnegative, and that the length of the slice is\nminimized for the given shape.\nThis test takes about 4 seconds to run, and is about at the limit of what is\npossible with exhaustive testing. Other objects, in particular Tuple, have\nso many possible combinations that a similar exhaustive test for them would\ntake billions of years to complete.\n\n\nHypothesis tests.\nHypothesis is a\nlibrary that can intelligently check a combinatorial search space of inputs.\nThis requires writing Hypothesis strategies that can generate all the\nrelevant types of indices. All ndindex tests have Hypothesis tests, even if\nthey are also tested exhaustively.\nThe Hypothesis test for the above test looks like this\nfrom hypothesis import assume\nfrom hypothesis.strategies import integers, composite, none, one_of, lists\n\n# hypothesis.strategies.tuples only generates tuples of a fixed size\n@composite\ndef tuples(draw, elements, *, min_size=0, max_size=None, unique_by=None,\n           unique=False):\n    return tuple(draw(lists(elements, min_size=min_size, max_size=max_size,\n                            unique_by=unique_by, unique=unique)))\n\n# Valid shapes for numpy arrays. Filter out shapes that would fill memory.\nshapes = tuples(integers(0, 10)).filter(lambda shape: prod([i for i in shape if i]) < 100000)\n\n@composite\ndef slices(draw, start=ints(), stop=ints(), step=ints()):\n    return slice(\n        draw(one_of(none(), start)),\n        draw(one_of(none(), stop)),\n        draw(one_of(none(), step)),\n    )\n\n@given(slices(), shapes)\ndef test_slice_reduce_hypothesis(s, shape):\n    a = arange(prod(shape)).reshape(shape)\n    try:\n        s = Slice(s)\n    except ValueError:\n        assume(False)\n\n    check_same(a, s.raw, func=lambda x: x.reduce(shape))\n\n    try:\n        reduced = s.reduce(shape)\n    except IndexError:\n        # shape == ()\n        return\n    assert reduced.start >= 0\n    # We cannot require stop > 0 because if stop = None and step < 0, the\n    # only equivalent stop that includes 0 is negative.\n    assert reduced.stop != None\n    assert len(reduced) == len(a[reduced.raw]), (s, shape)\n\nIn order to tell Hypothesis how to search the example space, we must define\nsome functions to tell it how to draw example objects of a given type, in\nthis case, slices and shape parameters for NumPy arrays. These strategies,\nas they are called, can be reused for multiple tests. Hypothesis then\nautomatically and intelligently draws examples from the sample space to try\nto find one that fails the test. You can think of Hypothesis as a fuzzer, or\nas an \"automated QA engineer\". It tries to pick examples that are most\nlikely to hit corner cases or different branch conditions.\n\n\nWhy bother with Hypothesis if the same thing is already tested exhaustively?\nThe main reason is that Hypothesis is much better at producing human-readable\nfailure examples. When an exhaustive test fails, the failure will always be\nfrom the first set of inputs in the loop that produces a failure. Hypothesis\non the other hand attempts to \"shrink\" the failure input to smallest input\nthat still fails. For example, a failing exhaustive slice test might give\nSlice(-10, -9, -10) as a the failing example, but Hypothesis would shrink it\nto Slice(-2, -1, -1).\nAnother reason for the duplication is that Hypothesis can sometimes test a\nslightly expanded test space without any additional consequences. For example,\nthe above Hypothesis tests all types of array shapes, whereas the exhaustive\ntest tests only 1-dimensional shapes. This doesn't affect things because\nHypothesis will always shrink large shapes to a 1-dimensional shape in the\ncase of a failure, and it has the benefit of ensuring the code works correctly\nfor larger shapes (it should always slice over the first index, or in the case\nof an empty shape raise IndexError).\nTry it out\nYou can install ndindex with pip or from conda-forge\nconda install -c conda-forge ndindex\n\nThe documentation can be found here,\nand the development is on GitHub.\nPlease try the library out and\nreport any issues you have, or\nthings you would like to see implemented. We are also looking for people who\nare interested in using the library and for people who are interested in\ncontributing to it.",
      "tags": "ndindex,NumPy",
      "url": "https://labs.quansight.org/blog/2020/04/introducing-ndindex-a-python-library-for-manipulating-indices-of-ndarrays/"
    },
    {
      "title": "PyTorch TensorIterator Internals",
      "text": "The history section of this post is still relevant, but TensorIterator's\ninterface has changed significantly. For an update on the new API, please check\nout this new blog\npost.\nPyTorch is one of the leading frameworks for deep learning. Its core data\nstructure is Tensor, a multi-dimensional array implementation with many\nadvanced features like auto-differentiation. PyTorch is a massive\ncodebase (approx. a million lines of\nC++, Python and CUDA code), and having a method for iterating over tensors in a\nvery efficient manner that is independent of data type, dimension, striding and\nhardware is a critical feature that can lead to a very massive simplification\nof the codebase and make distributed development much faster and smoother. The\nTensorIterator\nC++ class within PyTorch is a complex yet useful class that is used for\niterating over the elements of a tensor over any dimension and implicitly\nparallelizing various operations in a device independent manner.\nIt does this through a C++ API that is independent of type and device of the\ntensor, freeing the programmer of having to worry about the datatype or device\nwhen writing iteration logic for PyTorch tensors. For those coming from the\nNumPy universe, NpyIter is a close cousin of TensorIterator.\nThis post is a deep dive into how TensorIterator works, and is an essential\npart of learning to contribute to the PyTorch codebase since iterations over\ntensors in the C++ codebase are extremely commonplace. This post is aimed at\nsomeone who wants to contribute to PyTorch, and you should at least be familiar\nwith some of the basic terminologies of the PyTorch codebase that can be found\nin Edward Yang's excellent blog post\non PyTorch internals.  Although TensorIterator can be used for both CPUs and\naccelerators, this post has been written keeping in mind usage on the CPU.\nAlthough there can be some dissimilarities between the two, the overall\nconcepts are the same.\n\n\nHistory of TensorIterator\nTH iterators\nTensorIterator was devised to simplify the implementation of PyTorch's tensor\noperations over the TH implementation. TH uses preprocessor macros to write\ntype-independent loops over tensors, instead of C++ templates. For example,\nconsider this simple TH loop for computing the product of all the numbers in\na particular dimension (find the code\nhere):\nTH_TENSOR_DIM_APPLY2(scalar_t, t, scalar_t, r_, dimension,\n    accreal prod = 1;\n    int64_t i;\n    for(i = 0; i < t_size; i++)\n        prod *= t_data[i*t_stride];\n    *r__data = (scalar_t)prod;\n);\n\n\n\nThe above loop works by following a particular convention for the naming of the\ntypes and variables. You specify the input type and output type of your tensors in the first\nand third arguments. scalar_t is a type that can generically be used for denoting a PyTorch\nscalar type such as float, double, long etc. Internally, PyTorch uses the scalar_t\nfor compiling the file multiple times for different definitions of scalar_t (as in for different\ndata types like float, int, etc.). The input tensor and output tensors are\nspecified in the second and fourth arguments (in this case t and r_), and the dimension that\nwe want to iterate over is specified as the fifth argument (dimension).\nWe then follow these arguments with the main body of the iterator (which is accepted as the sixth\nargument into the macro), and denote the data, stride and size of the particular tensor dimension\nby using variables that are suffixed by _data, _stride and _size respectively after the\nvariable name that represents the tensor inside the iterator body. For example, the size of the\ninput tensor is denoted as t_size in the above example and the pointer to the data of the output\ntensor is denoted as r__data. The accreal in the second line is custom type that specifies\na real number that is an accumulator (in this case for accumulating the product).\nInternally, the TH_TENSOR_DIM_APPLY2 macro is expanded for generating various dispatch calls\ndepending on the type of the tensor that needs to be iterated over. The implementation of\nTH_TENSOR_DIM_APPLY2 can be found here.\nLimitations of TH iterators\nApart from the obvious complication that arises due to maintaining a codebase that is so dependent\non such insanely complex macro expansions, TH iterators have some fundamental shortcomings. For\none thing, they cannot be used for writing iterators in a device independent manner - you will\nneed separate iterators for CPU and CUDA. Also, parallelization does not happen implicitly\ninside the iterator, you need to write the parallel looping logic yourself. Moreover, at a deeper\nlevel TH iterators do not collapse the dimensions of the tensor (as we'll see later in this\npost) therefore leading to looping that might not be as cache-optimized as possible.\nThese limitations led to the creation of TensorIterator, which is used by the\nATen tensor implementation for overcoming some of the shortcomings of the previous TH\niterators.\nBasics of TensorIterator\nA TensorIterator can be created using the default constructor. You must then add the tensors\nthat you want as inputs or outputs. A good example can be found from the TensorIterator::binary_op()\nmethod that\nallows you to create TensorIterator objects for performing point-wise binary operations\nbetween two tensors. The important parts look like so:\nauto iter = TensorIterator();\n\niter.add_output(out);\niter.add_input(a);\niter.add_input(b);\n\niter.build();\n\n\n\nAs you can see, you add a tensor called out as the output tensors and a and b as the\ninput tensors. Calling build is then mandatory for creating the object and letting\nthe class perform other optimizations like collapsing dimensions.\nPerforming iterations\nBroadly, iterations using TensorIterator can be classified as point-wise iterations\nor reduction iterations. This plays a fundamental role in how iterations using TensorIterator\nare parallelized - point-wise iterations can be freely parallelized along any dimension\nand grain size while reduction operations have to be either parallelized along dimensions\nthat you're not iterating over or by performing bisect and reduce operations along the\ndimension being iterated. Parallelization can also happen using vectorized operations.\nIteration details\nThe simplest iteration operation can be performed using the\nfor_each\nfunction. This function has two overloads: one takes a function object which iterates over a\nsingle dimension (loop_t); the other takes a function object which iterates over two\ndimensions simultaneously (loop2d_t). Find their definitions here. The former can iterate over a loop\nof a single dimension whereas the latter can do so over two dimensions. The simplest\nway of using for_each is to pass it a lambda of type loop_t (or loop2d_t).\nA code snippet using it this way would look like so:\nauto iter = TensorIterator();\niter.add_output(out);\niter.add_input(a);\niter.dont_resize_outputs(); // call if out is allocated.\niter.dont_compute_common_dtype(); // call if inputs/outputs are of a different type.\niter.build();\n\nauto loop = [&](char **data, const int64_t* strides, int64_t n) {\n    auto * out_data_bytes = data[0];\n    auto * in_data_bytes = data[1];\n\n    // assume float data type for this example.\n    for (int i = 0; i < n; i++) {\n      *reinterpret_cast<float*>(out_data_bytes) +=\n        *reinterpret_cast<float*>(in_data_bytes);\n\n      out_data_bytes += strides[0];\n      in_data_bytes += strides[1];\n    }\n}\n\niter.for_each(loop);\n\n\n\nIn the above example, the char** data gives a pointer to the data within the\ntensor in the same order that you specify when you build the iterator. Note\nthat in order to make the implementation agnostic of any particular data type, you\nwill always receive the pointer typecast to char (think of it as a bunch of bytes).\nThe second argument is int64_t* strides which is an array containing the strides of\neach tensor in the dimension that you're iterating over. We can add this stride to the\npointer received in order to reach the next element in the tensor. The last argument is\nint64_t n which is the size of the dimension being iterated over.\nfor_each implicitly parallelizes the operation by executing loop in parallel\nif the number of iterations is more than the value of internal::GRAIN_SIZE, which is a value\nthat is determined as the 'right amount' of data to iterate over in order to gain a significant\nspeedup using multi-threaded execution. If you want to explicitly specify that your\noperation must run in serial, then use the serial_for_each loop.\nUsing kernels for iterations\nFrequently we want to create a kernel that applies a simple point-wise function onto entire tensors.\nTensorIterator\nprovides various such generic kernels that can be used for iterating over the elements\nof a tensor without having to worry about the stride, data type of the operands or details\nof the parallelism.\nFor example, say we want to build a function that performs the point-wise addition\nof two tensors and stores the result in a third tensor, we can use the cpu_kernel\nfunction. Note that in this example we assume a tensor of float but you can\nuse the AT_DISPATCH_ALL_TYPES_AND2 macro.\nTensorIterator iter;\niter.add_input(a_tensor);\niter.add_input(b_tensor);\niter.add_output(c_tensor);\niter.build();\ncpu_kernel(iter, [] (float a, float b) -> float {\n  return a + b;\n});\n\n\n\nWriting the kernel in this way ensures that the value returned by the lambda passed to\ncpu_kernel will populate the corresponding place in the target output tensor.\nSetting tensor iteration dimensions\nThe value of the sizes and strides will determine which dimension of the tensor you will iterate over.\nTensorIterator performs optimizations to make sure that at least\nmost of the iterations happen on contiguos data to take advantage of hierarchical cache-based\nmemory architectures (think dimension coalescing and reordering for maximum data locality).\nNow a multi-dimensional tensor will have multiple stride values depending on the dimension\nyou want to iterate over, so TensorIterator will directly compute the strides that\nget passed into the loop by\nby itself within the build() function. How exactly it computes the dimension\nto iterate over is something that should be properly understood in order to use TensorIterator\neffectively.\nIf you're performing a reduction operation (see the sum code in ReduceOps.cpp),\nTensorIterator will figure out the dimensions that will be reduced depending\non the shape of the input and output tensor, which determines how the input will be broadcast\nover the output. If you're\nperforming a simple pointwise operation between two tensors (like a addcmul from\nPointwiseOps.cpp)\nthe iteration will happen over the entire tensor, without providing a choice of the dimension.\nThis will allow TensorIterator to freely parallelize the computation, without guarantees of\nthe order of execution (since it does not matter anyway).\nFor something like a cumulative sum operation, where you want be able to choose the dimension\nto reduce but iterate over multiple non-reduced dimensions (possibly in parallel), you\nmust first re-stride the tensors, and then use these tensors\nfor creating a TensorIterator. In order to understand how this bit works, lets go over\nthe code for the kernel that executes the cumsum function.\nThe important bits of this function are like so:\nauto self_sizes = ensure_nonempty_vec(self.sizes().vec());\nself_sizes[dim] = 1;\n\nauto result_restrided = restride_dim(result, dim, self_sizes);\nauto self_restrided = restride_dim(self, dim, self_sizes);\n\nauto iter = TensorIterator();\niter.dont_compute_common_dtype();\niter.dont_resize_outputs();\niter.add_output(result_restrided);\niter.add_input(self_restrided);\niter.build();\n\n\n\nYou can see that we first change the size of the tensors to 1 on the\nreduction dimension so that the dimension collapsing logic inside\nTensorIterator#build will know which dimension to skip.\nSetting the dimension in this way is akin to telling TensorIterator\nto skip the dimension. We then restride the tensors using restride_dim and\nthen use the restrided tensors for building the TensorIterator. You can\nset any size for inputs/outputs, then TensorIterator with check whether it\ncan come up with a common broadcasted size\nConclusion\nThis post was a very short introduction to what TensorIterator is actually\ncapable of. If you want to learn more about how it works and what goes into\nthings like collapsing the tensor size for optimizing memory access, a good\nplace to start would be the build() function in\nTensorIterator.cpp.\nAlso have a look at this wiki page\nfrom the PyTorch team on using TensorIterator.",
      "tags": "C++,PyTorch",
      "url": "https://labs.quansight.org/blog/2020/04/pytorch-tensoriterator-internals/"
    },
    {
      "title": "Documentation as a way to build Community",
      "text": "As a long time user and participant in open source communities, I've always known that documentation is far from being a solved problem. At least, that's the impression we get from many developers: \"writing docs is boring\"; \"it's a chore, nobody likes to do it\". I have come to realize I'm one of those rare people who likes to write both code and documentation. \nNobody will argue against documentation. It is clear that for an open-source software project, documentation is the public face of the project. The docs influence how people interact with the software and with the community. It sets the tone about inclusiveness, how people communicate and what users and contributors can do. Looking at the results of a \u201cNumPy Tutorial\u201d search on any search engine also gives an idea of the demand for this kind of content - it is possible to find documentation about how to read the NumPy documentation!\nI've started working at Quansight in January, and I have started doing work related to the NumPy CZI Grant. As a former professor in mathematics, this seemed like an interesting project both because of its potential impact on the NumPy (and larger) community and because of its relevance to me, as I love writing educational material and documentation. Having official high-level documentation written using up-to-date content and techniques will certainly mean more users (and developers/contributors) are involved in the NumPy community.\nSo, if everybody agrees on its importance, why is it so hard to write good documentation?\n\n\nWhy do we lack documentation?\nIn a recent article about documentation for open source data analytics libraries, Geiger et al. point out that \"In a 2017 GitHub survey of OSS contributors, 93% reported that incomplete or outdated documentation is a pervasive problem but 60% of contributors say they rarely or never contribute to documentation (Zlotnick et al. 2017).\" At the same time, still in the words of Geiger et al., \"Many interviewees who regularly contribute documentation to such projects stated that they did not feel like they received same levels of positive community feedback for documentation work as they did for adding new features or fixing bugs.\" The authors in this paper go as far as saying that documentation is the invisible work behind these projects. \nIt doesn't help that many of these projects have been around for some time and have developed in decentralized, community-based ways. This is both positive and negative since it leaves the choice of the type of contribution to the new contributor. Most of the time, people who arrive want to make a big impact, and they perceive implementing a new feature or solving a pending bug, not writing documentation, as the way to do that.\nComing from an academic background, the same kind of dynamics seems to apply. Writing a paper or doing an experiment is far more appealing than writing a textbook or developing high-quality educational materials. But educational materials can have a huge impact and effectively bring people into the community. For me, realizing this was a turning point on the way I looked at documentation. \nWho writes the docs?\nIf we look at proprietary or company-backed software projects, often professional technical writers are working on the docs. Having access to these professionals to do the documentation can make a huge difference. However, even then there can be problems. In her excellent talk \"Who Writes the Docs?\", Beth Aitman says People who work on docs often don't feel like their work is valued (...) Being in a position where your value is questioned is pretty horrible.\nAs I got more involved in the open source world, I realized that the people writing docs were not only invisible but were sometimes actively discouraged. There is even a differentiation in naming such contributions; have you ever heard of a \"core docs developer\"? Rich Bowen says There's common wisdom in the open source world: Everybody knows that the documentation is awful, that nobody wants to write it, and that this is just the way things are. But the truth is that there are lots of people who want to write the docs. We just make it too hard for them to participate. So they write articles on Stack Overflow, on their blogs, and third-party forums. Although this can be good, it's also a great way for worst-practice solutions to bloom and gain momentum. Embracing these people and making them part of the official documentation effort for your project has many advantages.\nEven when the community is welcoming, documentation is often seen as a \"good first issue\", meaning that the docs end up being written by the least experienced contributors in the community. This can have its advantages, as it may give voice to the users who are experiencing difficulties, improving the communication between the projects and its community. However, it may transfer the responsibility of one of the most crucial aspects of any project to novice users, who have neither the knowledge or the experience to make decisions about it.\nWhat can we do about it?\nSo if expert users and developers are too busy to write docs, or just don't want to, how can we address this problem? \nAs much as the culture is changing, and many people are talking about the importance of documentation, we still have technical debt related to projects that have been around for a while. This is not something we can overlook: having good documentation is the difference between being successful or not in solving our users' (and clients') problems. Furthermore, it is also crucial if we aim to improve reproducibility and transparency issues in science and data analytics.\nIn this sense, grants like the one NumPy just received are extremely important to create momentum around documentation and reshape the community into one that values those contributions. \nShort and long-term goals\nAs part of our work related to the CZI grant, we have submitted a NumPy Enhancement Proposal (NEP) that proposes a restructure the NumPy documentation, to make it more organized and discoverable for beginners and experienced users. \nIn practical terms, we propose reorganizing the docs into the four categories mentioned in Daniele Procida's article \"What nobody tells you about documentation\", namely Tutorials, How-Tos, Reference Guide and Explanations. We believe that this will have several consequences:\n\nImproving on the quality and discoverability of the documentation as a whole;\nShowing a clearer difference between documentation aimed at different users (novices vs. experts, for example)\nGiving users more opportunities to contribute, generating content that can be shared directly on NumPy's official documentation\nBuilding a documentation team as a first-class team in the project, which helps create an explicit role as documentation creator. This helps people better identify how they can contribute beyond code.\nDiversifying our contributor base, allowing people from different levels of expertise and different life experiences to contribute. This is also extremely important so that we have a better understanding of our community and can be accessible, unbiased and welcoming to all people.\n\nIn the long term, having a process set up that can onboard new contributors and make sure they have the tools and environment they need to contribute can significantly improve the quality of our projects and broaden our contributors/maintainers base.\nConclusion\nDocumentation is much more than a complement to code. It is education, it is community building, and it is how we can make sure the project is healthy and sustainable.",
      "tags": "Labs,NumPy",
      "url": "https://labs.quansight.org/blog/2020/03/documentation-as-a-way-to-build-community/"
    },
    {
      "title": "Meet Our",
      "text": "Tania Allard\n            \n                \n            \n              \n                \n              \n                \n                  \n                    JupyterHub\n                  \n                  \n                    NumFOCUS DISC\n                  \n                \n        \n      \n      \n          \n            Ralf Gommers\n            \n                \n            \n              \n                \n              \n                \n                  \n                    NumPy\n                  \n                  \n                    SciPy\n                  \n                  \n                    data-apis.org\n                  \n                \n        \n      \n      \n          \n            Juanita Gomez\n            \n                \n            \n              \n                \n              \n                \n                  \n                    Spyder\n                  \n                \n        \n      \n      \n          \n            Carlos Cordoba\n            \n                \n            \n              \n                \n              \n                \n                  \n                    Spyder\n                  \n                \n        \n      \n      \n          \n            Matti Picus\n            \n                \n            \n              \n                \n              \n                \n                  \n                    NumPy\n                  \n                  \n                    PyPy\n                  \n                \n        \n      \n      \n          \n            Melissa Mendon\u00e7a\n            \n                \n            \n              \n                \n              \n                \n                  \n                    NumPy\n                  \n                \n        \n      \n      \n          \n            Pearu Peterson\n            \n                \n            \n              \n                \n              \n                \n                  \n                    Numba\n                  \n                  \n                    f2py\n                  \n                  \n                    rbc\n                  \n                  \n                    XND\n                  \n                \n        \n      \n      \n          \n            Isabela Presedo-Floyd\n            \n                \n            \n              \n                \n              \n                \n                  \n                    JupyterLab\n                  \n                  \n                    Spyder\n                  \n                \n        \n      \n      \n          \n            Tony Fast\n            \n                \n            \n              \n                \n              \n                \n                  \n                    Holoviz\n                  \n                  \n                    Jupyter\n                  \n                \n        \n      \n      \n          \n            Stephannie Jim\u00e9nez\n            \n                \n            \n              \n                \n              \n                \n                  \n                    Spyder\n                  \n                \n        \n      \n      \n          \n            Victor Fomin\n            \n                \n            \n              \n                \n              \n                \n                  \n                    PyTorch Ignite\n                  \n                \n        \n      \n      \n          \n            Marc Garcia\n            \n                \n            \n              \n                \n              \n                \n                  \n                    Ibis\n                  \n                  \n                    pandas\n                  \n                \n        \n      \n      \n          \n            Mars Lee\n            \n                \n            \n              \n                \n              \n                \n                  \n                    NumPy\n                  \n                \n        \n      \n      \n          \n            Guilherme Leobas\n            \n                \n            \n              \n                \n              \n                \n                  \n                    Numba\n                  \n                  \n                    XND\n                  \n                  \n                    PyData/Sparse\n                  \n                \n        \n      \n      \n          \n            Edgar Margffoy\n            \n                \n            \n              \n                \n              \n                \n                  \n                    Spyder\n                  \n                \n        \n      \n      \n          \n            Aaron Meurer\n            \n                \n            \n              \n                \n              \n                \n                  \n                    Numba\n                  \n                  \n                    SymPy\n                  \n                  \n                    ndindex\n                  \n                \n        \n      \n      \n          \n            Ivan Ogasawara\n            \n                \n            \n              \n                \n              \n                \n                  \n                    Ibis\n                  \n                \n        \n      \n      \n          \n            Gonzalo Pe\u00f1a-Castellanos\n            \n                \n            \n              \n                \n              \n                \n                  \n                    JupyterLab\n                  \n                  \n                    Spyder\n                  \n                \n        \n      \n      \n          \n            Matthias Bussonnier\n            \n                \n            \n              \n                \n              \n                \n                  \n                    Jupyter\n                  \n                  \n                    JupyterHub\n                  \n                  \n                    IPython\n                  \n                \n        \n      \n      \n          \n            Peter Bell\n            \n                \n            \n              \n                \n              \n                \n                  \n                    SciPy\n                  \n                  \n                    uarray\n                  \n                \n        \n      \n      \n          \n            Kim Pevey\n            \n                \n            \n              \n                \n              \n                \n                  \n                    Holoviz\n                  \n                \n        \n      \n      \n          \n            Athan Reines\n            \n                \n            \n              \n                \n              \n                \n                  \n                    JupyterLab\n                  \n                  \n                    stdlib.js\n                  \n                \n        \n      \n      \n          \n            Eric Charles\n            \n                \n            \n              \n                \n              \n                \n                  \n                    Jupyter\n                  \n                \n        \n      \n      \n          \n            Daniel Althviz\n            \n                \n            \n              \n                \n              \n                \n                  \n                    Spyder\n                  \n                \n        \n      \n      \n          \n            Pam Wadhwa\n            \n                \n            \n              \n                \n              \n                \n                  \n                    Holoviz\n                  \n                \n        \n      \n      \n          \n            Sameer Deshmukh\n            \n                \n            \n              \n                \n              \n                \n                  \n                    XND\n                  \n                \n        \n      \n      \n          \n            Chris Ostrouchov\n            \n                \n            \n              \n                \n              \n                \n                  \n                    python-api-inspect\n                  \n                \n        \n      \n      \n          \n            Anirrudh Krishnan\n            \n                \n            \n              \n                \n              \n                \n                  \n                    mtypes\n                  \n                \n        \n      \n      \n          \n            Gregory Lee\n            \n                \n            \n              \n                \n              \n                \n                  \n                    scikit-image\n                  \n                  \n                    SciPy\n                  \n                  \n                    PyWavelets\n                  \n                  \n                    CuPy\n                  \n                  \n                    PyFFTW\n                  \n                \n        \n      \n      \n          \n            Jaime Rodriguez Guerra\n            \n                \n            \n              \n                \n              \n                \n                  \n                    conda-forge\n                  \n                \n        \n      \n      \n          \n            Thomas J. Fan\n            \n                \n            \n              \n                \n              \n                \n                  \n                    scikit-learn\n                  \n                \n        \n      \n      \n          \n            Gagandeep Singh\n            \n                \n            \n              \n                \n              \n                \n                  \n                    SymPy\n                  \n                  \n                    LFortran\n                  \n                  \n                    NumPy",
      "tags": "",
      "url": "https://labs.quansight.org/team/"
    },
    {
      "title": "uarray: GSoC Participation",
      "text": "I'm pleased to announce that uarray is participating in GSoC '20 as a sub-organization under the umbrella of the Python Software Foundation. Our ideas page is up here, go take a look and see if you (or someone you know) is interested in participating, either as a student or as a mentor.\nPrasun Anand and Peter Bell and myself will be mentoring, and we plan to take a maximum of two students, unless more community mentors show up.\nThere have been quite a few pull requests already to qualify from prospective students, some even going as far as to begin the work described in the idea they plan to work on.\nWe're quite excited by the number of students who have shown an interest in participating, and we look forward to seeing excellent applications! What's more exciting, though, are some of the first contributions from people not currently at Quansight, in the true spirit of open-source software!",
      "tags": "Big data,NumPy,SciPy,Tensors,uarray",
      "url": "https://labs.quansight.org/blog/2020/03/uarray-gsoc-participation/"
    },
    {
      "title": "Planned architectural work for PyData/Sparse",
      "text": "What have we been doing so far? \ud83e\udd14\nResearch \ud83d\udcda\nA lot of behind the scenes work has been taking place on PyData/Sparse. Not so much in terms of code, more in terms of research and community/team building. I've more-or-less decided to use the structure and the research behind the Tensor Algebra Compiler, the work of Fredrik Kjolstad and his collaborators at MIT. \ud83d\ude47\ud83c\udffb\u200d\u2642\ufe0f To this end, I've read/watched the following talks and papers:\n\n\n\nFredrik Kjolstad, Shoaib Kamil, Stephen Chou, David Lugato, and Saman Amarasinghe. 2017. The tensor algebra compiler. Proc. ACM Program. Lang. 1, OOPSLA, Article 77 (October 2017), 29 pages. DOI:https://doi.org/10.1145/3133901\nFredrik Kjolstad, Peter Ahrens, Shoaib Kamil, and Saman Amarasinghe. 2018. Sparse Tensor Algebra Optimizations with Workspaces. https://arxiv.org/abs/1802.10574\nChou, Stephen, Fredrik Kjolstad, and Saman Amarasinghe. \u201cFormat Abstraction for Sparse Tensor Algebra Compilers.\u201d Proceedings of the ACM on Programming Languages 2.OOPSLA (2018): 1\u201330. Crossref. Web. https://arxiv.org/abs/1804.10112\nRyan Senanayake, Fredrik Kjolstad, Changwan Hong, Shoaib Kamil, and Saman Amarasinghe. 2019. A Unified Iteration Space Transformation Framework for Sparse and Dense Tensor Algebra https://arxiv.org/abs/2001.00532\nStephen Chou, Fredrik Kjolstad, Saman Amarasinghe. Automatic Generation of Efficient Sparse Tensor Format Conversion Routines. 2019. Automatic Generation of Efficient Sparse Tensor Format Conversion Routines https://arxiv.org/abs/2001.02609\nThe Sparse Tensor Algebra Compiler https://www.youtube.com/watch?v=0OP8WjFyU-Q\nFormat Abstraction for Sparse Tensor Algebra Compilers https://www.youtube.com/watch?v=sQOq3Ci4tB0\nThe Tensor Algebra Compiler https://www.youtube.com/watch?v=yAtG64qV2nM\n\nA bit heavy, so don't feel obliged to go through them. \ud83d\ude09\nResources \ud83d\udc65\nI've also had conversations with Ralf Gommers about getting someone experienced in Computer Science on board, as a lot of this work is very heavy on Computer Science.\nStrategy \ud83e\uddbe\nThe original TACO compiler requires a compiler at runtime, which isn't ideal for many users. However, what's nice is that we have Numba as a Python package. One, instead of emitting C code, can emit Python AST to be transpiled to LLVM by Numba, and then to machine code. I settled on this after researching Cppyy, which also requires a compiler, and pybind11, which wouldn't work as TACO itself is built to require a compiler.\nThe above warrants some explanation as to why exactly we're following this pattern. See, TACO is based on the fact that many popular matrix formats can be created using just a few per-dimension formats. The advantage behind this is that one can create highly efficient\ncode from just a few building blocks (albeit some hard-to-understand ones) for a lot of different formats. The downside is, one needs to do some code generation (the original TACO emits C code). In Python-land, one could emit source code or AST, with the latter being easier to debug and with guaranteed syntatical correctness. This is the reason I decided to go with AST.\nAPI Changes \ud83d\udc68\ud83c\udffb\u200d\ud83d\udcbb\nI'd also like to invite anyone who's interested into the discussion about API changes. The discussion can be found in this issue, but essentially, we're planning on moving to a lazy model for an asymptotically better runtime performance. We decided not to go and break backwards compatibility and essentially decided to have a separate submodule for this kind of work, then calling .compute() similar to Dask at the end.\nSo what does this mean for the user? \ud83d\ude15\nWhy, support for a lot more operations across a lot more formats, really. \ud83d\ude04 And don't forget the performance. \ud83d\ude80 With the downside being lazy operations a-la Dask.",
      "tags": "Big data,NumPy,SciPy,Sparse,Tensors",
      "url": "https://labs.quansight.org/blog/2020/02/whats-next-for-pydatasparse/"
    },
    {
      "title": "My Unexpected Dive into Open-Source Python",
      "text": "Header illustration by author, Mars Lee\nI'm very happy to announce that I have joined Quansight as a front-end developer and designer! It was a happy coincidence how I joined- the intersection of my skills and the open source community's expanded vision.\n\n\nI met Ralf Gommers, the director of Quansight Labs, at the PyData Conference in New York City last year after giving a Lightning Talk. However, as cool and confident as this may sound, I sure didn't start off that way.\nAt that point, it's been a few months since I graduated from a coding bootcamp. I was feeling down in the job-search funk. I hadn't even done much in Python, since my focus was in Javascript.\n\nWhen I first heard about PyData and its Diversity Scholar program, I didn't think either applied to me. I didn't feel like I knew enough Python nor deserve a place to learn more about it. I'm always eager to learn something new, but PyData felt too out of my reach.\nThankfully, my friend who was a volunteer at PyData really encouraged me to apply. I gave it a shot. I was rejected at first - I didn't quite make the cut. I was disappointed and accepted it.\nBut a second round of funding came in from partner organizations. Now, they could bring in more Diversity Scholars. I was elated to have the chance! Their trust in me gave me the confidence to go. It totally changed my point of view from self-doubt to eagerness.\nOnce I was at PyData, I signed up for the Lightning Talks, knowing that it would be a great way for me to talk about my personal project, the 'Codesprite Explains' comics.\nThe 'Codesprite' comics break down code concepts into fun and visual ways. It was inspired by my desire to turn my boring written notes into something more personal.\n \nFirst two pages of my personal project, 'Codesprite Explains'\nAt my Lightning Talk, I was confident. No longer did I doubt that I could be here- to take up space and stand up tall. I introduced myself as a Diversity Scholar. I was unabashedly unashamed to be new to Python and eager to learn. I delighted in showing off my artistic skills that give me so much joy and that aren\u2019t common in this community. I laughed with the crowd at my cheesy puns.\nWhen I finished my talk, I realized I came a long way from where I started. So when Ralf congratulated me, I was already feeling good. When he asked if I was interested in joining Quansight, that was the most unexpected-yet-awesomest cherry on top!\n\nThe author giving a Lightning Talk at PyData NYC\nIn my next blog post, I'll talk about my first impressions of Quansight and what made me say 'yes'.\nBefore ending this post, I would like to recap a few things.\nThere were some initial barriers that prevented me from joining the open-source space. Without knowing people involved in the community, such as my friend volunteering there, I wouldn't have known about PyData. Without the financial assistance of the Diversity Scholar program, I wouldn't have been able to afford a ticket as an unemployed graduate. A lack of technical knowledge itself isn't prohibitive, but when combined with the above factors, stops many other capable people from entering this space.\nI was able to overcome these barriers with a combination of external and internal factors. The Diversity Scholars program putting trust into newcomers like me, my growing confidence and a welcoming community.\nI look forward to contributing to the open-source community!\nSpecial Thanks to:\n\nPyData's Diversity Scholars program and its NYC volunteer organizers Hanhan and Vinay.\nMy new friends that welcomed me at PyData NYC - W4rner, Bhargav, Agata and Lev at their Natural Language Role-Playing workshop, hosted by agileEducation.\nNumFOCUS for organizing PyData and its funding partner organizations. \nNicole from NumFOCUS for sitting next to me during my first Pandas lesson and accepting my feedback on further improving the Diversity Scholars program. \nMy friend, Rinchen, for encouraging me to apply and volunteering at PyData NYC.\n\nView more of Mars's work on Medium and Twitter",
      "tags": "Labs,PyData",
      "url": "https://labs.quansight.org/blog/2020/02/my-unexpected-dive-into-open-source-python/"
    },
    {
      "title": "See Our",
      "text": "Projects we are actively working on\n\n\n\n    \n      NumPy\n      \n        \n            \n                \n            \n        \n        \n            NumPy is the fundamental package for scientific computing with Python. It contains among other things: a powerful N-dimensional array object, sophisticated (broadcasting) functions, tools for integrating C/C++ and Fortran code, and useful linear algebra, Fourier transform, and random number capabilities.\n        \n      \n    \n    \n    \n      JupyterLab\n      \n        \n            \n                \n            \n        \n        \n            JupyterLab is a next-generation web-based user interface for Project Jupyter. JupyterLab enables you to work with documents and activities such as Jupyter notebooks, text editors, terminals, and custom components in a flexible, integrated, and extensible manner.\n        \n      \n    \n    \n    \n      conda-forge\n      \n        \n            \n                \n            \n        \n        \n            conda-forge is a community effort that provides conda packages for a wide range of software.\n        \n      \n    \n    \n    \n      Dask\n      \n        \n            \n                \n            \n        \n        \n            Dask natively scales Python! Dask provides advanced parallelism for analytics, enabling performance at scale for the tools you love.\n        \n      \n    \n    \n    \n      Spyder IDE\n      \n        \n            \n                \n            \n        \n        \n            Spyder is a powerful scientific environment written in Python, for Python, and designed by and for scientists, engineers and data analysts. It offers a unique combination of the advanced editing, analysis, debugging, and profiling functionality of a comprehensive development tool with the data exploration, interactive execution, deep inspection, and beautiful visualization capabilities of a scientific package.\n        \n      \n    \n    \n    \n      SciPy\n      \n        \n            \n                \n            \n        \n        \n            The SciPy library is one of the core packages that make up the SciPy stack. It provides many user-friendly and efficient numerical routines, such as routines for numerical integration, interpolation, optimization, linear algebra, and statistics.\n        \n      \n    \n    \n    \n      Zarr\n      \n        \n            \n                \n            \n        \n        \n            Zarr is a Python package providing an implementation of chunked, compressed, N-dimensional arrays.\n        \n      \n    \n    \n    \n      JupyterHub\n      \n        \n            \n                \n            \n        \n        \n            JupyterHub brings the power of notebooks to groups of users. It gives users access to computational environments and resources without burdening the users with installation and maintenance tasks. Users - including students, researchers, and data scientists - can get their work done in their own workspaces on shared resources which can be managed efficiently by system administrators.\n        \n      \n    \n    \n    \n      Numba\n      \n        \n            \n                \n            \n        \n        \n            Numba makes Python code fast! Numba is an open source JIT compiler that translates a subset of Python and NumPy code into fast machine code.\n        \n      \n    \n    \n    \n      SymPy\n      \n        \n            \n                \n            \n        \n        \n            SymPy is a Python library for symbolic mathematics. It aims to become a full-featured computer algebra system (CAS) while keeping the code as simple as possible in order to be comprehensible and easily extensible. SymPy is written entirely in Python.\n        \n      \n    \n    \n    \n      PyData Sparse\n      \n        \n            \n                \n            \n        \n        \n            Sparse arrays, or arrays that are mostly empty or filled with zeros, are common in many scientific applications. To save space we often avoid storing these arrays in traditional dense formats, and instead choose different data structures. Our choice of data structure can significantly affect our storage and computational costs when working with these arrays. Sparse implements sparse arrays of arbitrary dimension on top of numpy and scipy.sparse. It generalizes the scipy.sparse.coo_matrix and scipy.sparse.dok_matrix layouts, but extends beyond just rows and columns to an arbitrary number of dimensions.\n        \n      \n    \n    \n    \n      PyTorch Ignite\n      \n        \n            \n                \n            \n        \n        \n            Ignite is a high-level library to help with training and evaluating neural networks in PyTorch flexibly and transparently.\n        \n      \n    \n    \n    \n      Ibis\n      \n        \n            \n                \n            \n        \n        \n            Ibis is a toolbox to bridge the gap between local Python environments (like pandas and scikit-learn) and remote storage and execution systems like Hadoop components (like HDFS, Impala, Hive, Spark) and SQL databases (Postgres, etc.). Its goal is to simplify analytical workflows and make you more productive.\n        \n      \n    \n    \n    \n      Uarray\n      \n        \n            \n                \n            \n        \n        \n            uarray is a backend system for Python that allows you to separately define an API, along with backends that contain separate implementations of that API.\n        \n      \n    \n    \n    \n      metadsl\n      \n        \n            \n                \n            \n        \n        \n            Domain Specific Languages Embedded in Python  metadsl inserts a layer between calling a function and computing its result, so that we can build up a bunch of calls, transform them, and then execute them all at once.\n        \n      \n    \n    \n    \n      XND\n      \n        \n            \n                \n            \n        \n        \n            We are building XND to recreate the foundations of NumPy as a number of smaller libraries, combining the lessons learned in the past twenty years of array computing in Python with the needs of newer applications. This is not a replacement of NumPy. Eventually, NumPy could use XND as could Pandas, Dask, and other libraries.\n        \n      \n    \n    \n    \n      HoloViz\n      \n        \n            \n                \n            \n        \n        \n            HoloViz is a coordinated effort to make browser-based data visualization in Python easier to use, easier to learn, and more powerful.\n        \n      \n    \n    \n    \n      ndindex\n      \n        \n            \n                \n            \n        \n        \n            ndindex is a library that allows representing and manipulating objects that can be valid indices to numpy arrays, i.e., slices, integers, ellipses, None, integer and boolean arrays, and tuples thereof.\n        \n      \n    \n    \n\n\nProjects we aim to support\n\n\n\n    \n      pandas\n      \n        \n            \n                \n            \n        \n        \n            pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n        \n      \n    \n    \n    \n      xarray\n      \n        \n            \n                \n            \n        \n        \n            xarray (formerly xray) is an open source project and Python package that makes working with labelled multi-dimensional arrays simple, efficient, and fun!\n        \n      \n    \n    \n    \n      matplotlib\n      \n        \n            \n                \n            \n        \n        \n            Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. Matplotlib can be used in Python scripts, the Python and IPython shells, the Jupyter notebook, web application servers, and four graphical user interface toolkits.\n        \n      \n    \n    \n    \n      bokeh\n      \n        \n            \n                \n            \n        \n        \n            Bokeh is an interactive visualization library for modern web browsers. It provides elegant, concise construction of versatile graphics, and affords high-performance interactivity over large or streaming datasets. Bokeh can help anyone who would like to quickly and easily make interactive plots, dashboards, and data applications.\n        \n      \n    \n    \n    \n      scikit-learn\n      \n        \n            \n                \n            \n        \n        \n            Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities.\n        \n      \n    \n    \n\n   See more projects and topics here",
      "tags": "",
      "url": "https://labs.quansight.org/projects/"
    },
    {
      "title": "Creating the ultimate terminal experience in Spyder 4 with Spyder-Terminal",
      "text": "The Spyder-Terminal project is revitalized! The new 0.3.0 version adds numerous features that improves the user experience, and enhances compatibility with the latest Spyder 4 release, in part thanks to the improvements made in the xterm.js project.\n\n\nUpgrade to ES6/JSX syntax\nFirst, we were able to update all the old JavaScript files to use ES6/JSX syntax and the tests for the client terminal. This change simplified the code base and maintenance and allows us to easily extend the project to new functionalities that the xterm.js API offers. In order to compile this code and run it inside Spyder, we migrated our deployment to Webpack.\nMultiple shells per operating system\nIn the new release, you now have the ability to configure which shell to use in the terminal. On Linux and UNIX systems, bash, sh, ksh, zsh, csh, pwsh, tcsh, screen, tmux, dash and rbash are supported, while cmd and powershell are the available options on Windows. To select your preferred command processor, simply choose it from the menu in the Terminal pane of Spyder's preferences and restart the IDE.\n\nThis is a great feature because it allows the user to determine their shell interpreter among the ones that are installed in their systems. In this way, Spyder-Terminal can be configured with any of the existing shells as long as it is available on their machine.\nTheme support and new UI options\nAnother big change in the new version is built-in support for all Spyder's light and dark themes. When you change your Spyder theme or display options, the Terminal automatically adapt its UI, colors and fonts accordingly.\n\nBuilding on the look and feel of the plugin, we also added configurable options for the terminal sounds and the cursor style. In this way, you can choose whether the terminal uses a bell sound and can select from one of three cursors.\n\nShortcut configuration\nLastly, the shortcuts on the terminal are configurable within the keyboard shortcuts in Spyder 4. The terminal's shortcuts for Copy, Paste, Clean and New terminal are now no longer hardcoded.\n\nWe're excited for you to try out Spyder-Terminal 0.3.0 and experiment with the new features available; we'd like to thank NumFOCUS and Quansight for providing the funding and support that made all this work possible. Check back soon for more Spyder updates, and until then, happy coding!",
      "tags": "Labs,Spyder",
      "url": "https://labs.quansight.org/blog/2020/02/creating-the-ultimate-terminal-experience-in-Spyder-4-with-Spyder-Terminal/"
    },
    {
      "title": "Learn About",
      "text": "Mission\nThe mission of Quansight Labs is to sustain and grow community-driven open\nsource projects and ecosystems, with a focus on the core of the PyData stack\nand on tools and digital infrastructure for data science, ML/AI, and\nscientific computing.\n\n\nVision\nThe majority of our efforts are aimed at maintaining and evolving existing\nopen source projects that are of critical importance to the PyData stack -\nprojects like NumPy, Jupyter, Dask, SciPy, Pandas, and conda-forge. We aim to\ngive talented maintainers time for necessary regular maintenance as well as\nfor tackling hard problems that need weeks or months of dedicated time.\nOur innovation activities are aimed at filling critical gaps in the PyData ecosystem.\nWe recognize that successful software projects require many skills and\nactivities beyond software development: community building, technical writing,\ngraphical design, UI/UX design, marketing, fundraising, and more. Most\ncommunity-driven open source projects lack these skills. We employ writers,\ndesigners, community managers, and others with an interest in open source - with\nthe aim of growing the diversity of skills present in open source communities,\nand making projects more healthy and mature.\n\n\nOur model\nWe created and continue to refine an innovative hybrid employment model.\nWe give open source maintainers a significant amount of time to work on\ncommunity-driven open source projects, while also spending part of their time\non consulting work with Quansight's commercial clients. Benefits include\nlearning opportunities for our team, bringing back concrete needs and ideas\nto open source projects, and the flexibility to scale up our contributions to\nthe open source projects we support quickly when we receive direct financial\nsupport for them.\nOur funding comes from multiple sources:\n\npart of the Quansight consulting revenue generated by our team members flows back to Quansight Labs\nwe work with industry partners interested in supporting particular projects or our mission\nwe created Community Work Orders\nto allow companies to sponsor features they need - in a way that guarantees\nthe open source projects themselves stay in charge.\ndirect funding for open source projects through grants\n\n\n\nContact Us\nKey contacts:\n\nTania Allard, Co-Director of Quansight Labs: tania.allard@quansight.com\nRalf Gommers, Co-Director of Quansight Labs: rgommers@quansight.com\nTravis Oliphant, CEO of Quansight: travis@quansight.com\nfor commercial inquiries: sales@quansight.com",
      "tags": "",
      "url": "https://labs.quansight.org/about/"
    },
    {
      "title": "Quansight",
      "text": "Quansight Labs is a public-benefit division of Quansight created to provide a\nhome for a \u201cPyData Core Team\u201d which consists of developers, community\nmanagers, designers, and writers who create and maintain open-source\ntechnology around all aspects of scientific and data science workflows.\n\nSustaining the future of Open Source\nProjects like NumPy, SciPy, Pandas, Jupyter, Matplotlib, scikit-learn, Dask,\nConda, and many others serve as a foundation for data science workflows and a\nlaunchpad for AI. We have had and continue to have the privilege of\nparticipating in these communities and understand personally the need for\npeople to be funded to work on these projects. We also understand the needs\nof companies that use these projects, and aim to connect communities and\ncompanies to guarantee the health and accelerate development of the projects\nand technology that tens of millions of users rely on.\nSee our Mission, Vision and Model for more.\n\n\nSponsors",
      "tags": "",
      "url": "https://labs.quansight.org/"
    },
    {
      "title": "metadsl PyData talk",
      "text": "metadsl PyData talk\nPyData NYC just ended and I thought it would be good to collect my thoughts on metadsl based on the many conversations I had there surrounding it. This is a rather long post, so if you are just looking for some code here is a Binder link for my talk. Also, here is the talk I gave a month or so later on the same topic in Austin:\n\n\nWhat is metadsl?\nclass Number(metadsl.Expression):\n   @metadsl.expression\n   def __add__(self, other: Number) -> Number:\n       ...\n\n   @metadsl.expression\n   @classmethod\n   def from_int(cls, i: int) -> Number:\n       ...\n\n\n@metadsl.rule\ndef add_zero(y: Number):\n   yield Number.from_int(0) + y, y\n   yield y + Number.from_int(0), y\n\n\n\n\n\nIt's a system for creating DSLs (domain specific languages) in Python so that you can separate your API from its implementation. I wrote a previous blog post showing how to create a system like metadsl from the ground up in Python. The premise is that we are looking to a future (and present) where Python is used to build up a computation which is compiled and executed with other systems. Examples of this today are systems like Tensorflow, Numba, and Ibis.\nThere are currently many different approaches to writing deeply embedded DSLs in Python. metadsls main differentiator is that it piggybacks on Python's existing static typing community. All code written in metadsl should be statically analyzable with MyPy and its ilk. We define the language using Python's existing mechanisms of class and functions with type annotations. We also support writing rewrite rules, using regular Python code that is also statically analyzable. So instead of defining a separate API for describing your nodes in your graph, their signatures, and how to replace them with other nodes, you just write Python functions and expressions.\nWhy would we limit ourselves to essentially a restricted subset of Python's flexibility when it has such great support for flexible metaprogramming and magic? It's so that users of the library can keep the same abstractions that they know from Python. It ties it closer conceptually to the language itself and pushes any type level improvements to happen at the typing/mypy level. So that metadsl moves in accordance with the language itself. It also means we are more restricted in what we can support, but sometimes bounds are helpful to limit scope and possible options.\nTLDR: metadsl is a framework to write domain specific languages in Python that piggybacks on the standard type annotation semantics.\nHow did your talk go?\nThis was my first time to present metadsl, although really a year earlier Travis and I co-presented on a previous incarnation of the project. I imagine at some point the recording will be published, but until then you can run through my talk by playing with this notebook on Binder. I appreciated the opportunity to try to place this project in some historical context. I think once I got to the actual code example though, I could have done a better job actually explaining how the system worked and what its limitations are. But it did give me leaping off point to have many great conversations with folks afterwards, which I will summarize below.\nWhat are the remaining technical hurdles?\nMany! :D\nDebugging\nI was very glad to bump into Martin Hirzel again. He presented a great poster at PLDI a year and a half ago, outlining some issues with using a variety of probabilistic programming libraries in Python. Each, for example, has their own abstractions for things like arrays or variables, that users have to re-learn as they switch between them. Users also don't get the same sort of debugging or tracebacks when using these libraries, as they do with regular Python, which leads to a worse user experience.\nBut he was here at PyData presenting a new system, called Lale, which allows you to declaratively specify your machine learning pipeline and leave out hyperparameters which you don't know. Then you can feed this to an automatic meta learning system like TPOT which will optimize these. Watch for him on a future Open Source Directions webinar presenting this work!\nWe talked about how we might add better debugging to metadsl. Ideally, we would be able to track back errors to the line where the user entered the expression. So even if they compile to say LLVM, and the code throws some error, it would be good to point out that error to where the user originally wrote the code. To do this, we would have to track the file, line, and column for every expression creation and propagate those along as we do replacements. We should look to Swift for some guidance here, because I know they have spent some effort making sure their compiler preserves source location as it compiles.\nMathematical soundness\nAnother person asked a good question after the presentation, \"What if two rules match? Which gets chosen?\" At the moment it's a manual process, where you define groups of rules that all should fire at the same time, so should be confluent and then put those groups in series. Like a number of compiler passes. However, we can't prove that two rules won't both match the same objects, leading to indeterminism, or that there are enough rules defined so that the result will actually be replaced, leading to an error. It would be nice to be able to say these things statically.\nFor example, here is an optimization rule that should execute before, say, compiling to LLVM rules:\n@metadsl.rule\ndef add_zero(y: Number):\n    \"\"\"\n    Replace y + 0 with y\n    \"\"\"\n    yield Number.from_int(0) + y, y\n    yield y + Number.from_int(0), y\n\n\n\nThere is lots of existing research in the pattern matching and lambda calculus community on being able to answer these questions. However, to get to those systems it would be good to first make our pattern matching system more restricted. Currently, we can define \"pure\" rules, which have strictly structural matching, and \"unpure\" rules which execute arbitrary Python functions to determine the replacement. The unpure rules will be very hard to reason about mathematically, unless we decide to model all of Python, which seems like a bad idea. So if we can move more, if not all, of our system to pure replacements then we can have more luck analyzing them. I have a couple of ideas here:\n\nOne use case for unpure functions is to check if a certain python value has some property. Like checking if a integer is 0 to execute some optimization. Even if we don't wanna be able to prove anything about the actual details of these properties, we could model them as pre-checks for the substitution based on deterministic functions on the values.\nAnother is to wrap/unwrap from values in the host language of Python. We need this at the edges of the system, for example to unwrap tuples or replace based on which Python type is found. I don't think we can avoid this part. However, by making sure we limit it to the edges, we could still possibly prove things about the inside of the system.\nWe also embed a simply typed lambda calculus into the language itself and implement beta reduction by replacing variables in the subtree. Beta reduction is impure, because we are doing a recursive find and replace on the subtree. Creation, from a Python function, is impure not only because it calls a Python callable, but also because it generates a \"Variable\" with a unique ID to save as the param for the function.\nIn order to compile a functional data flow expression to an imperative control flow graph we need to order our expressions. More details can be found in \"Optimizing compilation with the Value State Dependence Graph\" paper. It's hard for me at the moment to imagine this kind of transformation in a pure pattern replacement.\n\nFor these last two, we should talk to folks from the term rewriting and lambda calculus communities. In particular, the \"International Conference on Formal Structures for Computation and Deduction\" seems like an appropriate venue to look into:\n\nFSCD covers all aspects of formal structures for computation and deduction from theoretical foundations to applications. Building on two communities, RTA (Rewriting Techniques and Applications) and TLCA (Typed Lambda Calculi and Applications), FSCD embraces their core topics and broadens their scope to closely related areas in logics, models of computation (e.g. quantum computing, probabilistic computing, homotopy type theory), semantics and verification in new challenging areas (e.g. blockchain protocols or deep learning algorithms).\n\nThere is also a long history of approaches in trying to compile simply typed lambda calculus using term rewriting systems.\nPython Control Flow\nIt was really nice to meet Joe Jevnik for the first time. He has a ton of experience hacking on Python to make things faster and was one of the original contributors to the Blaze project at Continuum.\nCurrently, metadsl creates expressions just by executing Python functions. This is nice and simple, but it doesn't let us map Python control flow, like if, for, and list comprehensions, to a functional form.  Joe said we could modify the CPython interpreter to be able to transform these things (codetransformer). I like this method more than AST or bytecode parsing since it should work without access to the source and we don't need to put our computation in a function.\nTiark Rompf and some folks at Google have been exploring a similar space, in their \"The 800 Pound Python in the Machine Learning Room\" paper and snek-LMS repo:\n\nThis is a great area of future work, exploring how we can map Python control flow to metadsl expressions to get closer to the expressiveness of things like Numba or Tensorflow. It's much nicer for users to be able to write if: .... else: ... instead of a functional if_(true_clause, false_clause).\nWhere should this be used?\nThis was the second time talking to Mati Picus about this work. He works on NumPy as well as PyPy so has an interesting perspective on how tools like this could make it easier to create alternative runtimes for Python. One major difference between this work and PyPy is that we are limiting ourselves here to creating and compiling domain specific languages in Python, instead of all of Python. Not only does this mean we can deal with a restricted object model, but we can also co-design our libraries to work with this system. PyPy's job is much harder, having to interpret the full complexity of Python and the creative ways existing libraries use it.\nMLIR\nHe also was wondering about the relationship with MLIR, a new project from Chris Lattner and Google to provide a meta-IR to describe translations between things like the Tensorflow Graph and LLVM. I was really excited to see this announced last year at the Compilers for Machine Learning conference, because it made me think that maybe this whole \"lets share infrastructure to describe our IRs even if we know we need multiple ones\" isn't totally crazy. I see metadsl as another take on the same idea, but focusing on Python first friendliness whereas with MLIR you use C++ (at least this is what I have seen so far).\nI do want  to investigate it though and see what are the main differences. One key question I have is, \"Can you make a simply typed lambda calculus dialect in MLIR?\" metadsl was designed to allow this to facilitate compiling things like the Mathematics of Arrays which represents arrays as functions from indices to values.\nIt would be nice to see if we could convert between MLIR dialects and metadsl. And long term, if we could open a dialogue there and come together into one system in the end, that would be ideal!\nI have already started experimenting with translating metadsl types into JSON with the typez project. That's useful because then we could possibly generate these typez descriptions from MLIR dialects and then from there auto-generate Python code. Or vice versa.\nI think for now we should focus on doing something useful with metadsl first, to really stress test it, then we can come back and see how to bridge that gap.\nDataframes\nWhich brings me to the most practically relevant conversations... Dataframes!\nI was excited to reconnect with Marc Garcia, after meeting him a few years ago at SciPy. Congratulations to him for being awarded the Project Sustainability award at the NumFOCUS Summit the weekend before PyData NYC! He ran a docs sprint for Pandas that drew 500 people and is mentoring 26 women new to contributing to Pandas.\nWe talked about how we could think about targeting different backends and optimizing Pandas. His notion is that they are already exploring this for small chunks of computation. Like adding a backend flag to some larger operations to run them through different systems. They just implemented this for plotting as well. He said to move Pandas anywhere, it has to be incremental, that a full rewrite would be less likely to succeed.\nI was also thrilled to meet Jeff Reback for the first time. Jeff heroically helps manage the Pandas and Ibis projects, answering your issues and merging your pull requests. If you haven't heard of Ibis yet, it's a lazy Pandas project that can target different backends, like SQL databases. At Quansight, Ivan Ogasawara has implemented a backend for the OmniSci GPU database, so that you can run queries over millions of rows very quickly from Python with a familiar API. This seems like a nice project to see if metadsl has legs.\nWe should take a look at C# LINQ for inspiration here, to see how they had to update their type system to support it. We will have to also upstream some of these changes to Python's core typing.\nLong term implications\nBy describing our APIs with metadsl we transform our code into structured data.\nReproducible Science\nThen, if we save this data, we are able to later analyze it semantically. For example, we can determine what data files were used to produce a certain visualization in a notebook if we output the computation graph alongside the result. This dovetails with work we have been doing in JupyterLab to support viewing linked data with Paco Nathan, the NYU Coleridge Initiative, and CalPoly.\n.\nI sadly missed the talk by Evan Patterson on \"Semantic modeling of data science code\" also at PyData that seemed to be going down this same route, but instead by creating separate schemas for the API code. The difference here is that if libraries adopt metadsl then we get a defined ontology for free.\nStatistical Code Optimizations\nBy creating an extensible rewrite and compilation system we make it possible to experiment with new types of whole-program optimizations on existing codebases. We are starting to see an increase in machine learning in the compilation process itself (Tensor Comprehensions, TVM, \"Machine Learning in Compiler Optimization\"). As our hardware and software stacks get more complicated it becomes harder to reason about optimization strictly from first principles. So we can use tools from statistics, like Bayesian Optimization, to optimize our compilation.\nI went to a talk last week by David Blei, a pioneer in the probabilistic models and causal inference space. Causal inference lets us try to understand the effect of actions based on previous data from the world:\n\nI would love to experiment with these tools to help users compile fast code across different architectures, but first we need to create space in the eocsystem to prototype them and get them into users' hands.\nWanna get involved?\nThis project is at an early state and would benefit from collaboration. If you want to get involved, please open an issue on the repo or reach out to me directly. As a Python ecosystem I hope we can come together to work towards solutions in this space.",
      "tags": "Labs,metadsl",
      "url": "https://labs.quansight.org/blog/2019/12/metadsl-talk/"
    },
    {
      "title": "Variable Explorer improvements in Spyder 4",
      "text": "Spyder 4 will be released very soon with lots of interesting new features that you'll want to check out, reflecting years of effort by the team to improve the user experience. In this post, we will be talking about the improvements made to the Variable Explorer.\nThese include the brand new Object Explorer for inspecting arbitrary Python variables, full support for MultiIndex dataframes with multiple dimensions, and the ability to filter and search for variables by name and type, and much more.\nIt is important to mention that several of the above improvements were made possible through integrating the work of two other projects. Code from gtabview was used to implement the multi-dimensional Pandas indexes, while objbrowser was the foundation of the new Object Explorer.\n\n\nNew viewer for arbitrary Python objects\nFor Spyder 4 we added a long-requested feature: full support for inspecting any kind of Python object through the Variable Explorer. For many years, Spyder has been able to view and edit a small subset of Python variables: NumPy arrays, Pandas DataFrames and Series, and builtin collections (lists, dictionaries and tuples). Other objects were displayed as dictionaries of their attributes, inspecting any of which required showing a new table. This made it rather cumbersome to use this functionality, and was the reason arbitrary Python objects were hidden by default from the Variable Explorer view.\n\nFor the forthcoming Spyder release, we've integrated the excellent objbrowser project by Pepijn Kenter (@titusjan), which provides a tree-like view of Python objects, to offer a much simpler and more user-friendly way to inspect them.\n\nAs can be seen above, this viewer will also allow users to browse extra metadata about the inspected object, such as its documentation, source code and the file that holds it.\nIt is very important to note that this work was accomplished thanks to the generosity of Pepijn, who kindly changed the license of objbrowser to allow us to integrate it with Spyder.\nTo expose this new functionality, we decided to set the option to hide arbitrary Python objects in the Variable Explorer to disabled by default, and introduced a new one called Exclude callables and modules. With this enabled by default, Spyder will now display a much larger fraction of objects that can be inspected, while still excluding most \"uninteresting\" variables.\n\nFinally, we added a context-menu action to open any object using the new Object Explorer even if they already have a builtin viewer (DataFrames, arrays, etc), allowing for deeper inspection of the inner workings of these datatypes.\n\nMulti-index support in the dataframe viewer\nOne of the first features we added to the Variable Explorer in Spyder 4 was MultiIndex support in its DataFrame inspector, including for multi-level and multi-dimensional indices. Spyder 3 had basic support for such, but it was very rudimentary, making inspecting such DataFrames a less than user-friendly experience.\n\nFor Spyder 4, we took advantage of the work done by Scott Hansen (@firecat53) and Yuri D'Elia (@wavexx) in their gtabview project, particularly its improved management of column and table headings, which allows the new version of Spyder to display the index shown above in a much nicer way.\n\nFuzzy filtering of variables\nSpyder 4 also includes the ability to filter the variables shown down to only those of interest. This employs fuzzy matching between the text entered in the search field and the name and type of all available variables.\nTo access this functionality, click the search icon in the Variable Explorer toolbar, or press Ctrl+F (Cmd-F on macOS) when the Variable Explorer has focus.\n\nTo remove the current filter, simply click the search icon again, or press Esc or Ctrl+F (Cmd-F) while the Variable Explorer is focused.\nRefresh while code is running\nWe added back the ability to refresh the Variable Explorer while code is running in the console. This feature was dropped in Spyder 3.2, when we removed the old and unmaintained Python console. However, this functionality will return in Spyder 4, thanks to the fantastic work done by Quentin Peter (@impact27) to completely re-architect the way Spyder talks to the Jupyter kernels that run the code in our IPython console, integrating support for Jupyter Comms.\n\nTo trigger a refresh, simply click the reload button on the Variable Explorer toolbar, or press the shortcut Ctrl+R (Cmd-R) when it has focus.\nFull support for sets\nIn Spyder 3, the Variable Explorer could only show builtin Python sets as arbitrary objects, making it very difficult for users to browse and interact with them. In Spyder 4, you can now view sets just like lists, as well as perform various operations on them.\n\nUI enhancements and more\nFinally, beyond the headline features, we've added numerous smaller improvements to make the Variable Explorer easier and more efficient to use. These include support for custom index names, better and more efficient automatic resizing of column widths, support for displaying Pandas Indices, tooltips for truncated column headers, and more.\nSpyder's Variable Explorer is what many of you consider to be one of its standout features, so we can't wait for you all to get your hands on the even better version in Spyder 4. Thanks again to Quansight, our generous community donors, and as always all of you! Spyder 4.0.0 final is planned to be released within approximately one more week, but if you'd like to test it out immediately, follow the instructions on our GitHub to install the pre-release version (which won't touch your existing Spyder install or settings). As always, happy Spydering!",
      "tags": "Labs,Spyder",
      "url": "https://labs.quansight.org/blog/2019/11/variable-explorer-improvements-in-Spyder-4/"
    },
    {
      "title": "A new grant for NumPy and OpenBLAS!",
      "text": "I'm very pleased to announce that NumPy and OpenBLAS just received a $195,000 grant from\nthe Chan Zuckerberg Initiative, through its\nEssential Open Source Software for Science\n(EOSS) program! This is good news for both projects, and I'm particularly excited about\nthe types of activities we'll be undertaking, what this will mean in terms of growing\nthe community, and to be part of the first round of funded projects of this visionary program.\nThe program\nThe press release\ngives a high level overview of the program, and the\ngrantee website lists the 32 successful applications.\nOther projects that got funded include SciPy and Matplotlib (it's the very first\nsignificant funding for both projects!), Pandas, Zarr, scikit-image, JupyterHub, and\nBioconda - we're in good company!\nNicholas Sofroniew and Dario Taborelli, two of the people driving the EOSS program, wrote\na blog post that's well worth reading about the motivations for starting this program and\nthe 42 projects that applied and got funded:\nThe Invisible Foundations of Biomedicine.\n\n\nSo what will we be doing?\nFor NumPy, we will be working on:\n\nNumPy\u2019s governance and organizational structure,\nthe numpy.org website,\nhigh-level documentation aimed at new users and contributors, and\ncommunity outreach and mentoring of new team members.\n\nThis is driven by both the recognition that we need to better serve NumPy's\nlarge number of beginner to intermediate level users, and that NumPy\u2019s\nsustainability depends on growing its core team and contributor community -\nand in particular in areas other than code maintenance.\nThe OpenBLAS work will focus on addressing sets of key technical issues, in\nparticular thread-safety, AVX-512 and thread-local storage (TLS) issues. In\naddition algorithmic improvements will be made in ReLAPACK (Recursive\nLAPACK), which OpenBLAS is the main user of.\nMore details on our planned activities and deliverables can be found in\nthe full proposal.\nThere were a couple of other motivations for focusing on governance, website,\ndocumentation and community building activities. First, it was allowed -\nwhich is already quite unusual. Most potential funders, whether they be\ninstitutional science funders or companies, usually want to see a proposal\nfull of shiny new features. Often it's explicitly disallowed to propose\nmaintenance or community building. The people at the Chan Zuckerberg\nInstitute that put this program together clearly understand how scientific\nopen source software projects work and what their needs are, and they allowed\nproposing any type of work that makes sense for a project. Simple, but\nradical!\nSecond, I thought quite hard about what the best ways are of spending this\namount of funding effectively. Of the grant, $140,000 goes to NumPy - a lot\ncan be done with this, but it's also good to realize that it's about 10% of\nthe amount of funding that BIDS received for NumPy. And we'd like to have\nmore than 10% of the impact in the long term! In my\nSciPy'19 talk\nI attempted to quantify the impact of those BIDS grants. The vast majority of\nthose funds were used for (very much necessary) technical work, and it\nincreased the velocity of the projects by ~25-30%, and in addition it enabled\nintegrating some larger changes, like the numpy.random redesign. It seems\nclear to me though that adding another 10% of similar activities, while valuable,\nwon't be transformative. On the other hand, focusing all our time\non growing the team and better serving new users and contributors may be -\nwe're aiming to enable more sustained contributions from more people, and in\nareas like high-level documentation that are now under-developed.\nOf the activities that we proposed back in July, some are already underway.\nWe have a small team working on redesigning the website, and through the\nGoogle Season of Docs program we are working with a professional tech writer,\nAnne Bonner, on a new beginner-friendly tutorial. This is great, in\nparticular because our proposal only got funded at the level of 80% of what\nwe asked for. So the deliverables that are in the proposal but we descoped\nare likely to materialize anyway.\nThe significance and current state of OpenBLAS\nWhile NumPy has its struggles with finding maintainers, at least every user\nknows what it is and (most of the time) appreciates it. OpenBLAS is in a harder\nposition - as a library for accelerated linear algebra it's fundamentally important\nfor NumPy (as well as for SciPy, Julia and R), but it's not visible to end users.\nThis is a main reason for why it has far fewer maintainers and contributors.\nIn fact, its bus factor is exactly one - over the last two years Martin\nKroeker, the main OpenBLAS maintainer, has >10x more commits than the next\nmost active contributors. For a project that's so important to all of\nscientific computing, that's a worry. I'm quite happy that I was able to\ncollaborate with Martin on this grant proposal, and that he can now dedicate\nsome more time to OpenBLAS development.\nIf you're still fuzzy on what OpenBLAS is and does, imagine what would happen if\nnp.dot and every other linear algebra function ran 20x slower. Or, say,\nyour scikit-learn model would take 20x longer to run. There are alternatives\nto OpenBLAS, but not many. Intel MKL is proprietary, so while it's a good\noption for (for example) the NumPy shipped by Anaconda, MKL is not an option\nfor many redistributors (including NumPy itself, for its PyPI wheels and\nconda-forge packages). BLIS is a newer library that provides accelerated BLAS\nfunctions, but its companion libFlame for accelerated LAPACK is far from\nready for mainstream use. And the venerable ATLAS is stagnant, not as performant,\nand suffered from the single-maintainer issue as well. So you may not know it,\nbut you're very likely relying on OpenBLAS!\nOpenBLAS is fast, but it's also relatively unstable. NumPy releases are\noften impacted by issues in an OpenBLAS function, and we need to be very\ncareful about which OpenBLAS version to use in the wheels we ship. This grant\nwill tackle some of the more important types of issues that we have run into\nwith NumPy. By extension, this will also help all other major users of OpenBLAS.\nGrant management - the money flow explained\nI think it's quite important to be transparant with the community about how a grant\nis managed and how we plan to spend the funds. The full $195,000 will go to NumFOCUS,\nthe fiscal sponsor of NumPy, with myself as the PI responsible for it. Of\nthat grant, $55,000 is reserved for the OpenBLAS work - NumFOCUS will\ncontract Martin for the bulk of that (there's some overhead NumFOCUS charges,\nand we reserve some funds for being able to hire a student intern for\nOpenBLAS work). The other $140,000 is for NumPy, and will be used for a\nsubcontract with Quansight to fund my time as well as hire a tech writer and web\ndesigner at Quansight Labs. This is quite exciting to me - I'd love to see more\nwriters, web developers, graphic designers and community managers become part of\ncommunity open source projects, so this role at Labs is the first of hopefully many.\nNext steps\nToday was announcement day, the real work starts now. First, I'll be figuring out\nthe logistics of the grant so we can start the work in time. Then on November 22-23\nthere's a NumPy sprint at BIDS where we will have 8-10 members of the NumPy team in\na room. That's a great opportunity to discuss many of the topics that we will be\nworking on - the NumPy roadmap, governance, community building, and website & documentation\nwork are all on the agenda. The official start date of the grant is December 1st.\nStay tuned - there's a lot more to come!",
      "tags": "community,funding,Labs,NumPy,OpenBLAS",
      "url": "https://labs.quansight.org/blog/2019/11/numpy-openblas-CZI-grant/"
    },
    {
      "title": "File management improvements in Spyder4",
      "text": "Version 4.0 of Spyder\u2014a powerful Python IDE designed for scientists, engineers and data analysts\u2014is almost ready! It has been in the making for well over two years, and it contains lots of interesting new features. We will focus on the Files pane in this post, where we've made several improvements to the interface and file management tools. \nSimplified interface\nIn order to simplify the Files pane's interface, the columns corresponding to size and kind are hidden by default. To change which columns are shown, use the top-right pane menu or right-click the header directly.\n\n\n\nCustom file associations\nFirst, we added the ability to associate different external applications with specific file extensions they can open. Under the File associations tab of the Files preferences pane, you can add file types and set the external program used to open each of them by default.\n\nOnce you've set this up, files will automatically launch in the associated application when opened from the Files pane in Spyder. Additionally, when you right-click a file you will find an Open with... option that will show the application associated with this extension.\n\nSingle click open option\nWe've added a new option to open files and directories with a single-click instead of a double-click, to suit different user preferences. To enable this option, go to the General tab of the Files preferences pane and check the option Single click to open files. With this setting, only a single click is needed to open a file in the Files pane either externally or in Spyder.\n\nBear in mind that changing this configuration option will also affect the behaviour of the Project Explorer pane.\nOpen files externally\nWe added the option to open files with the operating system's default program for the file type to the Files pane's context menu. To use this feature, right-click any file in the Files pane and click Open externally.\n\nMoreover, it is now possible to select more than one file on which to perform a context menu action, thanks to the Files pane's new multi-select functionality. For this, press Ctrl (Cmd on macOS) or Shift while clicking on the files to select, just as you would in your operating system's file manager (Windows Explorer, macOS Finder, etc). As you'd expect, Ctrl (Cmd) selects individual files one by one, while Shift selects all the files between the one you click and the one previously selected.\nOnce you've selected multiple files, it is now possible to execute many of the actions available in the context menu on all the selected files, including delete, rename, move, copy and open externally.\n\nAbsolute and relative path handling\nSpyder 4 now allows you to easily copy the absolute or relative paths of one or more files in the Files pane and pasting them as well-formatted text anywhere else in Spyder. Just right-clicking any file or files in the Files or Project panes and select the desired option.\n\nBeyond just copying a single path, when pasting more than one Spyder will intelligently format it into a comma-separated collection of quoted strings, one path per line, that can be pasted directly into a list with no additional formatting needed.\n\nThe Copy Absolute Path option gets us the complete path of each selected file, starting from the drive root.\n\nAlternatively, Copy Relative Path gets us the path of the file relative to the current working (displayed in the top right of the Spyder window).\n\nFile type icons\nFinally, files in the Files pane are now displayed with icons depending on their type. There are custom icons for the source files of a variety of common programming languages including C C++, C Sharp, Java, Python, R, and Swift; for media types like JPEG, MP3, and M4A; and for other common extensions such as .txt and .tex. This allows you to identify file types with just a glance and select the appropriate application accordingly.\n\nWith these new features and UI options in Spyder's Files pane, we hope you will enjoy the improved file management experience coming in Spyder 4! We would like to thank Quansight, our donors, and most of all our dedicated users and contributors from around the world for making these features possible. The final version will be released very soon, but if you'd like to try it out now and share your feedback, follow the instructions on our Github. Happy Spydering!",
      "tags": "Labs,Spyder",
      "url": "https://labs.quansight.org/blog/2019/11/File-management-improvements-in-Spyder4/"
    },
    {
      "title": "uarray: Attempting to move the ecosystem forward",
      "text": "There comes a time in every project where most technological hurdles have been surpassed, and its adoption is a social problem. I believe uarray and unumpy had reached such a state, a month ago.\nI then proceeded, along with Ralf Gommers and Peter Bell to write NumPy Enhancement Proposal 31 or NEP-31. This generated a lot of excellent feedback on the structure and the nuances of the proposal, which you can read both on the pull request and on the mailing list discussion, which led to a lot of restructuring in the contents and the structure of the NEP, but very little in the actual proposal. I take full responsibility for this: I have a bad tendency to assume everyone knows what I'm thinking. Thankfully, I'm not alone in this: It's a known psychological phenomenon.\n\n\nOf course, social problems can take a long time to resolve one way or another, regardless of the proponents. And I consider this a good thing: it's better not to be stuck with an API decision that may bite you a few years down the line, especially with a project with API compatibility guarantees and number of dependents as NumPy.\nI must confess I felt discouraged at some points in the uarray journey: However, realising my flaws will make me perform better in the future.\nAlthough my main focus at this point in my career isn't uarray (not that I don't want it to be, only that social problems don't take very much of your time), it isn't a small library at the back of my head by any means.\nWith that out of the way, I'd like to present some improvements the Quansight team has made to uarray over the course of the past several months:\nProgress on Integration\nuarray \u2764\ufe0f scipy.fft\nuarray is now the official override mechanism for scipy.fft. This is a huge win.\nNEP-31 merged with draft status\nAlready discussed above, but NEP-31 is available on the NumPy website in draft form.\nIn core uarray\nC++ Implementation Merged\nThe C++ implementation of the uarray protocol is now merged. uarray is now lightning fast: About as fast as __array_function__.\nGlobal backend as the \"only\" backend\nThe set_global_backend gained an only= kwarg. You can read what this means in the docs, but basically: In the absence of any local backends; no other backend will be tried at all.\nCross-platform CI\nWe now have cross-platform CI for Windows, macOS and Linux.\nBuilding wheels on CI\n... so we don't have to. \ud83d\ude01\nBackends can fail fast\n... by throwing a ua.BackendNotImplementedError\nNew website \ud83e\udd73\nhttps://uarray.org/, and https://unumpy.uarray.org/. 'Nuff said.\nTest scipy.fft in CI\n... so we don't accidentally break it.\nRe-entrant context managers\n... so you can cache contexts and use them without worrying.\nThen we cache for you\n... so you don't have to.\nRich comparison of backends\n... so you can specify (via __eq__) which backends to skip\nA bit of performance here and there\nBackend systems need to be fast. \ud83d\ude09\nIn unumpy\nMore coverage of the NumPy API, mainly. \ud83c\udfc3\u200d\nudiff\nThere's another part of the uarray ecosystem now: udiff. It can perform auto differentiation of any unumpy array.",
      "tags": "",
      "url": "https://labs.quansight.org/blog/2019/11/uarray-attempting-to-move-the-ecosystem-forward/"
    },
    {
      "title": "Quansight Labs Work Update for September, 2019",
      "text": "As of November, 2018, I have been working at\nQuansight. Quansight is a new startup founded by\nthe same people who started Anaconda, which aims to connect companies and open\nsource communities, and offers consulting, training, support and mentoring\nservices. I work under the heading of Quansight\nLabs. Quansight Labs is a public-benefit\ndivision of Quansight. It provides a home for a \"PyData Core Team\" which\nconsists of developers, community managers, designers, and documentation\nwriters who build open-source technology and grow open-source communities\naround all aspects of the AI and Data Science workflow.\nMy work at Quansight is split between doing open source consulting for various\ncompanies, and working on SymPy.\nSymPy, for those who do not know, is a\nsymbolic mathematics library written in pure Python. I am the lead maintainer\nof SymPy.\nIn this post, I will detail some of the open source work that I have done\nrecently, both as part of my open source consulting, and as part of my work on\nSymPy for Quansight Labs.\nBounds Checking in Numba\nAs part of work on a client project, I have been working on contributing code\nto the numba project. Numba is a just-in-time\ncompiler for Python. It lets you write native Python code and with the use of\na simple @jit decorator, the code will be automatically sped up using LLVM.\nThis can result in code that is up to 1000x faster in some cases:\n\n\nIn [1]: import numba\n\nIn [2]: import numpy\n\nIn [3]: def test(x):\n   ...:     A = 0\n   ...:     for i in range(len(x)):\n   ...:         A += i*x[i]\n   ...:     return A\n   ...:\n\nIn [4]: @numba.njit\n   ...: def test_jit(x):\n   ...:     A = 0\n   ...:     for i in range(len(x)):\n   ...:         A += i*x[i]\n   ...:     return A\n   ...:\n\nIn [5]: x = numpy.arange(1000)\n\nIn [6]: %timeit test(x)\n249 \u00b5s \u00b1 5.77 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nIn [7]: %timeit test_jit(x)\n336 ns \u00b1 0.638 ns per loop (mean \u00b1 std. dev. of 7 runs, 1000000 loops each)\n\nIn [8]: 249/.336\nOut[8]: 741.0714285714286\n\n\n\nNumba only works for a subset of Python code, and primarily targets code that\nuses NumPy arrays.\nNumba, with the help of LLVM, achieves this level of performance through many\noptimizations. One thing that it does to improve performance is to remove all\nbounds checking from array indexing. This means that if an array index is out\nof bounds, instead of receiving an IndexError, you will get garbage, or\npossibly a segmentation fault.\n>>> import numpy as np\n>>> from numba import njit\n>>> def outtabounds(x):\n...     A = 0\n...     for i in range(1000):\n...         A += x[i]\n...     return A\n>>> x = np.arange(100)\n>>> outtabounds(x) # pure Python/NumPy behavior\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 4, in outtabounds\nIndexError: index 100 is out of bounds for axis 0 with size 100\n>>> njit(outtabounds)(x) # the default numba behavior\n-8557904790533229732\n\n\n\nIn numba pull request #4432, I am\nworking on adding a flag to @njit that will enable bounds checks for array\nindexing. This will remain disabled by default for performance purposes. But\nyou will be able to enable it by passing boundscheck=True to @njit, or by\nsetting the NUMBA_BOUNDSCHECK=1 environment variable. This will make it\neasier to detect out of bounds issues like the one above. It will work like\n>>> @njit(boundscheck=True)\n... def outtabounds(x):\n...     A = 0\n...     for i in range(1000):\n...         A += x[i]\n...     return A\n>>> x = np.arange(100)\n>>> outtabounds(x) # numba behavior in my pull request #4432\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nIndexError: index is out of bounds\n\n\n\nThe pull request is still in progress, and many things such as the quality of\nthe error message reporting will need to be improved. This should make\ndebugging issues easier for people who write numba code once it is merged.\nremovestar\nremovestar is a new tool I wrote to\nautomatically replace import * in Python modules with explicit imports.\nFor those who don't know, Python's import statement supports so-called\n\"wildcard\" or \"star\" imports, like\nfrom sympy import *\n\n\n\nThis will import every public name from the sympy module into the current\nnamespace. This is often useful because it saves on typing every name that is\nused in the import line. This is especially useful when working interactively,\nwhere you just want to import every name and minimize typing.\nHowever, doing from module import * is generally frowned upon in Python. It is\nconsidered acceptable when working interactively at a python prompt, or in\n__init__.py files (removestar skips __init__.py files by default).\nSome reasons why import * is bad:\n\nIt hides which names are actually imported.\nIt is difficult both for human readers and static analyzers such as\n  pyflakes to tell where a given name comes from when import * is used. For\n  example, pyflakes cannot detect unused names (for instance, from typos) in\n  the presence of import *.\nIf there are multiple import * statements, it may not be clear which names\n  come from which module. In some cases, both modules may have a given name,\n  but only the second import will end up being used. This can break people's\n  intuition that the order of imports in a Python file generally does not\n  matter.\nimport * often imports more names than you would expect. Unless the module\n  you import defines __all__ or carefully dels unused names at the module\n  level, import * will import every public (doesn't start with an\n  underscore) name defined in the module file. This can often include things\n  like standard library imports or loop variables defined at the top-level of\n  the file. For imports from modules (from __init__.py), from module import\n  * will include every submodule defined in that module. Using __all__ in\n  modules and __init__.py files is also good practice, as these things are\n  also often confusing even for interactive use where import * is\n  acceptable.\nIn Python 3, import * is syntactically not allowed inside of a function\n  definition.\n\nHere are some official Python references stating not to use import * in\nfiles:\n\nThe official Python\n  FAQ:\n\n\nIn general, don\u2019t use from modulename import *. Doing so clutters the\nimporter\u2019s namespace, and makes it much harder for linters to detect\nundefined names.\n\n\nPEP 8 (the official\n  Python style guide):\n\n\nWildcard imports (from <module> import *) should be avoided, as they\nmake it unclear which names are present in the namespace, confusing both\nreaders and many automated tools.\n\nUnfortunately, if you come across a file in the wild that uses import *, it\ncan be hard to fix it, because you need to find every name in the file that is\nimported from the * and manually add an import for it. Removestar makes this\neasy by finding which names come from * imports and replacing the import\nlines in the file automatically.\nAs an example, suppose you have a module mymod like\nmymod/\n  | __init__.py\n  | a.py\n  | b.py\n\n\n\nwith\n# mymod/a.py\nfrom .b import *\n\ndef func(x):\n    return x + y\n\n\n\nand\n# mymod/b.py\nx = 1\ny = 2\n\n\n\nThen removestar works like:\n$ removestar -i mymod/\n$ cat mymod/a.py\n# mymod/a.py\nfrom .b import y\n\ndef func(x):\n    return x + y\n\n\n\nThe -i flag causes it to edit a.py in-place. Without it, it would just\nprint a diff to the terminal.\nFor implicit star imports and explicit star imports from the same module,\nremovestar works statically, making use of\npyflakes. This means none of the code is\nactually executed. For external imports, it is not possible to work statically\nas external imports may include C extension modules, so in that case, it\nimports the names dynamically.\nremovestar can be installed with pip or conda:\npip install removestar\n\n\n\nor if you use conda\nconda install -c conda-forge removestar\n\n\n\nsphinx-math-dollar\nIn SymPy, we make heavy use of LaTeX math in our documentation. For example,\nin our special functions\ndocumentation,\nmost special functions are defined using a LaTeX formula, like \n(from https://docs.sympy.org/dev/modules/functions/special.html#sympy.functions.special.bessel.besselj)\nHowever, the source for this math in the docstring of the function uses RST\nsyntax:\nclass besselj(BesselBase):\n    \"\"\"\n    Bessel function of the first kind.\n\n    The Bessel `J` function of order `\\nu` is defined to be the function\n    satisfying Bessel's differential equation\n\n    .. math ::\n        z2 \\frac{\\mathrm{d}2 w}{\\mathrm{d}z2}\n        + z \\frac{\\mathrm{d}w}{\\mathrm{d}z} + (z2 - \\nu2) w = 0,\n\n    with Laurent expansion\n\n    .. math ::\n        J_\\nu(z) = z\\nu \\left(\\frac{1}{\\Gamma(\\nu + 1) 2\\nu} + O(z2) \\right),\n\n    if :math:`\\nu` is not a negative integer. If :math:`\\nu=-n \\in \\mathbb{Z}_{<0}`\n    *is* a negative integer, then the definition is\n\n    .. math ::\n        J_{-n}(z) = (-1)n J_n(z).\n\n\n\nFurthermore, in SymPy's documentation we have configured it so that text\nbetween `single backticks` is rendered as math. This was originally done for\nconvenience, as the alternative way is to write :math:`\\nu` every\ntime you want to use inline math. But this has lead to many people being\nconfused, as they are used to Markdown where `single backticks` produce\ncode.\nA better way to write this would be if we could delimit math with dollar\nsigns, like $\\nu$. This is how things are done in LaTeX documents, as well\nas in things like the Jupyter notebook.\nWith the new sphinx-math-dollar\nSphinx extension, this is now possible. Writing $\\nu$ produces $\\nu$, and\nthe above docstring can now be written as\nclass besselj(BesselBase):\n    \"\"\"\n    Bessel function of the first kind.\n\n    The Bessel $J$ function of order $\\nu$ is defined to be the function\n    satisfying Bessel's differential equation\n\n    .. math ::\n        z2 \\frac{\\mathrm{d}2 w}{\\mathrm{d}z2}\n        + z \\frac{\\mathrm{d}w}{\\mathrm{d}z} + (z2 - \\nu2) w = 0,\n\n    with Laurent expansion\n\n    .. math ::\n        J_\\nu(z) = z\\nu \\left(\\frac{1}{\\Gamma(\\nu + 1) 2\\nu} + O(z2) \\right),\n\n    if $\\nu$ is not a negative integer. If $\\nu=-n \\in \\mathbb{Z}_{<0}$\n    *is* a negative integer, then the definition is\n\n    .. math ::\n        J_{-n}(z) = (-1)n J_n(z).\n\n\n\nWe also plan to add support for $$double dollars$$ for display math so that ..\nmath :: is no longer needed either .\nFor end users, the documentation on docs.sympy.org\nwill continue to render exactly the same, but for developers, it is much\neasier to read and write.\nThis extension can be easily used in any Sphinx project. Simply install it\nwith pip or conda:\npip install sphinx-math-dollar\n\n\n\nor\nconda install -c conda-forge sphinx-math-dollar\n\n\n\nThen enable it in your conf.py:\nextensions = ['sphinx_math_dollar', 'sphinx.ext.mathjax']\n\n\n\nGoogle Season of Docs\nThe above work on sphinx-math-dollar is part of work I have been doing to\nimprove the tooling around SymPy's documentation. This has been to assist our\ntechnical writer Lauren Glattly, who is working with SymPy for the next three\nmonths as part of the new Google Season of\nDocs program. Lauren's project\nis to improve the consistency of our docstrings in SymPy. She has already\nidentified many key ways our docstring documentation can be improved, and is\ncurrently working on a style guide for writing docstrings. Some of the issues\nthat Lauren has identified require improved tooling around the way the HTML\ndocumentation is built to fix. So some other SymPy developers and I have been\nworking on improving this, so that she can focus on the technical writing\naspects of our documentation.\nLauren has created a draft style guide for documentation at\nhttps://github.com/sympy/sympy/wiki/SymPy-Documentation-Style-Guide. Please\ntake a moment to look at it and if you have any feedback on it, email\nme or write to the SymPy mailing list.",
      "tags": "",
      "url": "https://labs.quansight.org/blog/2019/10/quansight-labs-work-update-for-september-2019/"
    },
    {
      "title": "Ruby wrappers for the XND project",
      "text": "Table of Contents\n\nIntroduction\nNdtypes\nUsage\nBasic initialization\nConcrete Vs. Abstract Types\nTypedefs\nUsage via The C API\n\n\nImplementation\n\n\nXnd\nBasic Usage\nData Type Support\nMissing Values\nUsage via The C API\n\n\nImplementation\n\n\nGumath\nUsage\nUsage via The C API\n\n\nImplementation\nAutomatic Kernel Generation\n\n\nConclusion and Future Work\n\n\n\nIntroduction\nLack of stable and reliable scientific computing software has been a persistent problem\nfor the Ruby community, making it hard for enthusiastic Ruby developers to use Ruby in\neverything from their web applications to their data analysis projects. One of the most important\ncomponents of any successful scientific software stack is a well maintained and flexible\narray computation library that can act as a fast and simple way of storing in-memory data\nand interfacing it with various fast and battle-tested libraries like LAPACK and BLAS.\nVarious projects have attempted to make such libraries in the past (and some are still thriving\nand maintained). Some of the notable ones are numo, nmatrix, and more recently, numruby.\nThese projects attempt to provide a simple Ruby-like API for creating and manipulating arrays\nof various types. All of them are able to easily interface with libraries like ATLAS, FFTW\nand LAPACK.\nHowever, all of the above projects fall short in two major aspects:\n\nLack of extensibility to adapt to modern use cases (read Machine Learning).\nLack of a critical mass of developers to maintain a robust and fast array library.\n\nThe first problem is mainly due to the fact that they do not support very robust type systems.\nThe available data types are limited and are hard to extend to more complex uses. Modern use cases like\nMachine Learning require a more robust type system (i.e. defining array shapes of arbitrary dimension on multiple devices), as has been demonstrated by the tensor\nimplementations of various frameworks like Tensorflow and PyTorch.\nThe second problem is due to the fact that all of the aforementioned projects are community\nefforts that are maintained part-time by developers simply out of a sense of purpose and\npassion. Sustaining such complex projects for extended periods of time without expectation\nof any support is simply unfeasible even for the most driven engineers.\nThis is where the XND project comes in. The XND project is a project for\nbuilding a common library that is able to meet the needs of the various data analysis and\nmachine learning frameworks that have had to build their own array objects and programming \nlanguages. It is built with the premise of extending arrays with new types and various \ndevice types (CPUs, GPUs etc.) without loss of performance and ease of use.\n\n\nThe XND project as a whole is a product of three C libraries : ndtypes, xnd and gumath. They\nhave been made such that they can work as standalone C libraries that can be interfaced\nwith any language binding (currently supporting Ruby and Python). Ndtypes is used for defining\nthe shape of data within memory, XND is a data container that holds that data and gumath provides\na multiple dispatch mechanism for performing computations on data held in XND containers. We will\nelaborate on each of these in the post below.\nThe XND project presents the perfect answer to Ruby's lack of a mature array computation ecosystem. \nIt is highly extensible, allows defining data types in almost any combination with a simple and\nintuitive interface, is built with performance in mind and is backed by a team consisting of \nexperts who have vast experience in this domain for the Python scientific computing stack.\nThe biggest backer of XND as of now is Quansight, and I as a part-time engineer am responsible \nfor maintaining the Ruby wrapper for XND. This post is a rather long and detailed introduction \nto the XND ruby wrapper. There will also be some\ndetails on the implementation of the wrapper and how it differs from the Python wrapper (which\nexisted before the Ruby wrapper). Read on for further details.\nAll the source code can be found in the xnd-ruby repo.\nNdtypes\nNdtypes is the library that is used for defining the shape of data.\nRun gem install ndtypes --pre for easily installing ndtypes onto your machine. It has\nbeen tested with Ruby 2.4.1 so far. The gem install will download the C sources and compile\nthem by itself.\nUsage\nBasic initialization\nThe ndtypes Ruby wrapper provides a simple interface to the ndtypes C library for creating\ncomplex data shapes with extreme simplicity. For example, for creating an array of 10 int64\ndigits, all we need to do is create an instance of the NDT class:\nt = NDT.new \"10 * int64\"\n\n\n\nNot only can you create arrays, but also very complex types, for example a nested record (xnd \nterminology for a Ruby Hash) with the values as arrays of type float32 of size 25 each:\nt = NDT.new \"{x: 25 * float32, y: {a: 25 * float64, 25 * float64}}\"\n\n\n\nConcrete Vs. Abstract Types\nNdtypes distinguishes types depending on whether they are abstract or concrete. Abstract types\ncan have symbolic values like dimension or type variables and are used for type checking. Concrete \ntypes additionally have full memory layout information like alignment and data size.\nSome operations can be only performed on abstract types.\nTypedefs\nOne can also define typedefs using the NDT#typedef function and then use them in place of\nthe original type. Here's an example of using typedefs to define a graph type:\nNDT.typedef \"node\", \"int32\"\nNDT.typedef \"cost\", \"int32\"\nNDT.typedef \"graph\", \"var * var * (node, cost)\"\n\n\n\nUsage via The C API\nMost of the C API functions of ndtypes deal with creating NDT Ruby objects or obtaining\ninternal struct data of an NDT Ruby object. The complete specification can be found\nin the ruby_ndtypes.h file. This is the file you should include if you want to use the\nC API in any of your libraries.\nImplementation\nThe Ruby wrapper is a wrapper over the libndtypes library. The NDT Ruby object is a wrapper\nover a C struct of type NdtObject that has the following definition:\ntypedef struct NdtObject {\n  const ndt_t *ndt;                   /* type */\n} NdtObject;\n\n\n\nThis simple struct stores a pointer to a struct of type const ndt_t * that is provided\nby libndtypes for representing an ndtype. The const ndt_t structs are allocated by\nvarious libndtypes functions like ndt_from_string() or ndt_alloc().\nInternally libndtypes uses a reference counting mechanism for keeping track of ndt_t allocations\nthat need to be destroyed. The reference count can be incremented using ndt_incref() or\ndecremented using ndt_decref(). Once the refcount reaches the 0 the object is automatically\ndestroyed by libndtypes. Of course, ndt_t structs allocated via calls to functions like\nndt_alloc() already come with a refcount of 1.\nXnd\nXND is the main storage library of the project. It uses types defined by ndtypes for defining\nthe shape of data and allows users to read and write data into buffers that are of the shape\nof the data passed to it by ndtypes. It is responsible for maintaining the memory consistency\nof data and has provisions for various operations such as slicing, copying and interfacing\ndata with 3rd party libraries like Apache Arrow. It also serves as a memory buffer for the\nfunctions that are defined within gumath (explained later in this post).\nSimilar to the ndtypes wrapper, the xnd Ruby wrapper can be installed with a call to\ngem install xnd --pre.\nBasic Usage\nThe xnd Ruby wrapper is extremely simple to use and provides a single class XND for the\nuser that interfaces with libxnd. In the simplest case, one can create an XND object as follows:\nx = XND.new [1,2,3,4]\n# => #<XND:47340720296980>\n#    type= 4 * int64\n#    value= [1, 2, 3, 4]\n\n\n\nSince we have not specified the data type, it will be inferred as int64 since we are supplying\nan array composed entirely of integers. This can be seen using the XND#dtype function, which\nwill return the NDT object that holds the type of this XND object:\nx.dtype\n# => #<NDTypes:47340721833280>\n#   int64 \n\n\n\nWhile XND#dtype gives the general type of the object, a more precise description of the data\ntype (including shape etc.) can be obtained using the type method:\nx.type\n# => #<NDTypes:47340721846240>\n#  4 * int64\n\n\n\nThe value within the XND object can be obtained as a Ruby Array (or Hash if it is a NDT record)\nusing the XND#value method:\nx.value\n# => [1, 2, 3, 4]\n\n\n\nWe can also perform operations for checking equality between XND objects using the == or != operators:\na = XND.new [1,2,3,4]\nx == a\n# => true\n\n\n\nA nice thing about XND is that it returns copy-free 'views' of data when you perform a slicing\noperation. So say we define a 2D tensor tensor_2d like this:\ntensor_2d = XND.new(\n  [\n    [1,2,3,4,5],\n    [1,2,3,4,5],\n    [1,2,3,4,5],\n    [1,2,3,4,5],\n    [1,2,3,4,5]\n  ]\n)\ntensor_2d.inspect\n# => #<XND:47340720946720>\n#    type= 5 * 5 * int64\n#    value= [[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]\n\n\n\nWe can obtain a slice (say the 2nd column) of the tensor using Ruby Range. Note that using INF is a shorthand for specifying the entire axis (usually denoted as 0..-1):\nvector_view = tensor_2d[INF, 2]\n# => #<XND:47340720380980>\n#    type= 5 * int64\n#    value= [3, 3, 3, 3, 3]\n\n\n\nWhen using slices, XND will always return a 'view' of the original XND object. Changes\nmade to this slice will reflect on the original XND object as well:\nvector_view[2] = 666\ntensor_2d.inspect\n# => #<XND:47340720946720>\n#    type= 5 * 5 * int64\n#    value= [[1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 666, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]\n\n\n\nHowever, the type of the view and the original object differ as they should:\nvector_view.type\n# => #<NDTypes:47340720381100>\n#   5 * int64 \ntensor_2d.type\n# => #<NDTypes:47340720939360>\n#   5 * 5 * int64\n\n\n\nIf you want a separate storage space for the view (i.e. do not want changes to the view\nto reflect on the parent object), you should use the XND#dup method and make a copy. You\ncan also allocate a data container without storing any data into it using the XND.empty\nmethod as can be seen in the following examples.\nData Type Support\nAs a result of the flexibility provided by the ndtypes type definition interface, xnd is able\nto provide type support for far more flexible data shapes than simply for arrays with fixed\ndimensions. For example, you can use records for storing Ruby Hashes and performing operations\non them:\nrequire 'xnd'\n\nx = XND.empty \"{x: complex64, y: bytes, z: string}\"\nv = { 'x' => 1+20i, 'y' => \"abc\".b, 'z' => \"any\" }\nx['x'] = v['x']\nx['y'] = v['y']\nx['z'] = v['z']\nx\n# => #<XND:47340721378580>\n#    type= {x : complex64, y : bytes, z : string}\n#    value= {\"x\"=>(1.0+20.0i), \"y\"=>\"abc\", \"z\"=>\"any\"}\n\n\n\nMissing Values\nXND also supports optional data (represented by nil). It can be created as follows:\nx = XND.empty \"2 * 4 * ?float64\"\nv = [[10.0, nil, 2.0, 100.12], [nil, nil, 6.0, 7.0]]\nx[INF] = v # assign full slice\n# => [[10.0, nil, 2.0, 100.12], [nil, nil, 6.0, 7.0]] \n\n\n\nUsage via The C API\nThe primary function of the XND Ruby C API is for creating and querying XND Ruby objects.\nThe full API can be found in the ruby_xnd.h file.\nImplementation\nThe implementation of the Ruby wrapper differs from the Python wrapper largely due to nature\nof the garbage collection algorithms employed by both these languages: Ruby uses a mark-and-sweep\nGC while Python uses a reference counted GC. Therefore, Ruby objects created within the C extension\nhave to somehow be kept 'alive' such that the GC does not deallocate them thinking that they\nhave gone out of scope and are no longer useful.\nFor this purpose we utilize a 'GC guard' structure (inspired by the implementation of \n@mrkn's matplotlib.rb gem). The GC guard is basically a global Ruby Hash that has the\nRuby object created within the C extension as a key and something random as a value. We\nuse a Hash because it provides lookups in O(1) time and we don't care about the value\nbecause we only want to save the object in some kind of a global store so that Ruby is \naware of its presence (in case of NDT we use true for the value). XND uses three different\nGC guards for various internal objects, which can be found in the gc_guard.h file.\nGumath\nWhile ndtypes and xnd allow us to define types and memory storage, gumath allows us to actually\ndo something with them. The basic idea behind gumath is that it is a library that allows defining\nfunctions for various data types stored within an XND object and allows the user to transparently\ncall them using a high level interface that uses multiple dispatch for calling the relevant function\non the appropriate type. The Ruby interface is a wrapper over the libgumath C library.\nSome functions (known as kernels) come bundled with libgumath and others can be written fairly\neasily. Similar to the xnd and ndtypes wrappers, the gumath Ruby wrapper can be installed with a call to gem install gumath --pre.\nUsage\nThe Gumath class is a top level namespace for various modules that serve as namespaces for\nfunctions that come rolled in with the libgumath C library. These modules will keep expanding\nas more interfaces are added to libgumath. The Gumath::Functions module contains various\nsuch functions that are provided by libgumath by default.\nGumath functions accept XND objects as arguments and output XND objects with the result\nof the function. An example of a simple element-wise multiply kernel is the following:\nrequire 'xnd'\nrequire 'gumath'\n\nx = XND.new [2,3,4,5,6,7,8,9], dtype: \"float64\"\ny = XND.new [1,2,3,4,5,6,7,8], dtype: \"float64\"\nz = Gumath::Functions.multiply x, y\n# => #<XND:47340721458320>\n#    type= 8 * float64\n#    value= [2.0, 6.0, 12.0, 20.0, 30.0, 42.0, 56.0, 72.0]\n\n\n\nUsage via The C API\nSince the main purpose of the gumath C API is to allow adding kernels to a Ruby module,\nit provides a single function of the prototype:\nint rb_gumath_add_functions(VALUE module, const gm_tbl_t *tbl);\n\n\n\nThe module parameter is a Ruby object, and tbl is a function table of gumath kernels.\nImplementation\nCompared to xnd and ndtypes, the gumath Ruby wrapper is much simpler\nsince its primary function is to take functions from libgumath and add them as module \nfunctions to Ruby modules.\nWhen the library is initially loaded using a call to require, the relevant libgumath kernels\nprovided by default are loaded into the Ruby interpreter by interfacing each kernel with a\nRuby object. Further details on the working of the method dispatch within Ruby can be found\nin the CONTRIBUTING file.\nThe most important part of the C implementation is the GufuncObject class which is a Ruby\nclass defined within the C API that helps interface with a single gumath function. This class\nis basically a wrapper over a C struct GufuncObject that can be found in the gufunc_object.h\nfile.\nThe struct has the following definition:\ntypedef struct {\n  const gm_tbl_t *table;          /* kernel table */\n  char *name;                     /* function name */\n  uint32_t flags;                 /* memory target */\n  VALUE identity;                 /* identity element */\n} GufuncObject;\n\n\n\nThe table pointer is the pointer to the definition of the function within libgumath that\nholds information about the function that is used by gm_apply for making the actual call to\nthe function with the data. name is a string holding the name of the function. flags\nsignify whether the function is a CPU function or a CUDA function (or for that matter any\nother device that might be added in the future). identity is a Ruby object used for identifying\nthis function. It is initially set to nil.\nAutomatic Kernel Generation\nWriting kernels can be painstaking if you're not familiar with the various functionalities\nthat libgumath provides for this purpose. Therefore we also provide a kernel generator\ncalled xndtools that allows writing gumath kernels by simply providing the function\nthat needs to wrapped. However, this functionality has not yet been tested for Ruby.\nConclusion and Future Work\nThe current state of the Ruby XND wrappers makes them suitable for the XND libraries via Ruby,\nbut what would be truly exciting would be have a more Ruby-like API that conforms to accepted\nRuby idioms and creates a truly intuitive XND Ruby interface, rather than simply a one-on-one\nmapping of functions.\nIn the future we also plan to integrate XND with various Ruby libraries like rubyplot and daru\nand expand the uses of XND even further. For example, operators like multiplication in other \nscientific languages like MATLAB simply work with operator overloading by using the * operator\nbetween objects. Similarly overriding operators on XND and calling the underlying gumath\nkernel is a work in progress.\nSimilarly, have a 'Ruby-like' API that allows better method chaining in the sense of Rails or \nrspec should free up the programmer of having to 'think' of the interfaces and make\narray computations much more intuitive for programmers. These changes will of course be implemented\nafter XND reaches a critical base of users who are willing to provide feedback and try out\nnew interfaces. We plan to integrate XND into rubyplot to achieve this goal of more usage.\nThe C API for the wrapper is also quite limiting as of now, and it would be quite a pain\nfor another Ruby gem to use XND via the C API. Therefore, improving the C API is also\nsomething that we will be seriously looking into in the future.",
      "tags": "",
      "url": "https://labs.quansight.org/blog/2019/09/ruby-wrappers-for-the-xnd-project/"
    },
    {
      "title": "Quansight Labs Dask Update",
      "text": "This post provides an update on some recent Dask-related activities the Quansight Labs team has been working on.\nDask community work order\nThrough a community work order (CWO) with the D. E. Shaw group, the Quansight Labs team has been able to dedicate developer time towards bug fixes and feature requests for Dask. This work has touched on several portions of the Dask codebase, but generally have centered around using Dask Arrays with the distributed scheduler.\n\nFor instance, performance improvement in how Dask handles large graphs were made in dask PR #4918 and distributed PR #2594, while several Dask Array bug fixes and support for asymmetric overlapping computations were included in dask PR #5256, dask PR #5151, and dask PR #4863.\nThis CWO approach allows institutions, in this case the D. E. Shaw group, to fund development in community-driven open source projects while still respecting how these projects make decisions. To learn more about funding projects through CWOs, check out this post by Ralf Gommers which discusses the CWO model in detail.\nConferences\nIn addition to working on Dask directly, we've also been engaging the broader scientific Python community through attending conferences. In particular, developers at Quansight, Anaconda, and NVIDIA organized a Dask sprint at the SciPy 2019 conference. Throughout the two-day sprint, core developers were able to help sprint attendees become more familiar with the Dask codebase, set up local development environments, and even get new pull requests from sprinters submitted, reviewed, and merged into Dask! I personally had a great time at SciPy and hope to see lots of familiar faces again next year.\nLikewise, in July I spoke about Dask at the Data-Driven Wisconsin 2019 conference. Following my talk, I had several engaging discussions with other conference attendees and it was really gratifying to see all the enthusiasm surrounding Dask. The materials presented at the talk can be found on GitHub and a live, interactive version is available on mybinder.org.\nMaintenance and development\nFinally, there's been a push for a more coordinated effort towards project maintenance and development by core Dask maintainers at Quansight, Anaconda, and NVIDIA. As part of this effort, we spend a portion of our work week on day-to-day project maintenance tasks (e.g. responding on issues, reviewing pull requests, fixing CI systems, etc.) as well as working on contributions that require significant amounts of time or expertise to implement (e.g. large-scale refactoring, adding new features, writing documentation, etc.). Today, Dask users typically get a quicker response from a core maintainer when opening an issue or pull request, in part, because of these efforts. I, and perhaps other core maintainers, hope to write more about this process in the future.",
      "tags": "Dask,Labs",
      "url": "https://labs.quansight.org/blog/2019/08/labs-dask-update/"
    },
    {
      "title": "Spyder 4.0 beta4: Kite integration is here",
      "text": "Kite is sponsoring the work discussed in this blog post, and in addition supports Spyder 4.0 development through a Quansight Labs Community Work Order.\nAs part of our next release, we are proud to announce an additional completion client for Spyder, Kite. Kite is a novel completion client that uses Machine Learning techniques to find and predict the best autocompletion for a given text. Additionally, it collects improved documentation for compiled packages, i.e., Matplotlib, NumPy, SciPy that cannot be obtained easily by using traditional code analysis packages such as Jedi.\n\n\n\nBy incorporating Kite into Spyder, we will improve and provide the ultimate autocompletion and signature retrieval experience for most of the scientific Python stack and beyond. For instance, let\u2019s take a look at the following PyTorch completion. While the Language Server Protocol server would not complete some of the functions, e.g. torch.linspace, Kite is able to return it successfully:\n\nAnother benefit of using Kite is that of personalized completions, due to incremental ML analysis performed by the analysis engine, which keeps track of the keypresses and text writing style, alongside common usage patterns, this way users will get a personalized experience that improves over time.\nWe are working in collaboration with the Kite team to improve our autocompletion experience for the upcoming 4.0 release of Spyder. We plan to have all Kite features tested and ready to be used; that includes adding on-the-fly completions and code snippets for autocompletions.\nTo start using Kite, you just need to install the Kite client from their web page. They offer client packages for Windows, Linux and MacOS that are easy to install. Once Kite is installed on the system, Spyder is able to detect it and start the Kite client automatically.\nOur new Kite client will work alongside the new Language Server Protocol and the usual fallback tokenizer for files that cannot be analyzed by either the LSP or Kite. Thus, Spyder will be able to provide an optimal autocompletion experience for Python and other languages. Besides that, it is possible to define the priority between different completion plugins to show a completion source result first over the other completion plugins.\nLast but not least, as part of this novel client, we are introducing a new completion API that enables to write third-party completion clients for Spyder that require integration with the code editor and project manager. We offer calls to perform autocompletion, hover requests, signature hints, among others without any hassle. By leveraging this API, it should be possible to add completions and documentation suitable for domain-specific applications developed using Spyder.\nThe function calls and constants of the new completion API will be added to the Spyder documentation and should be available in the next beta release of Spyder. Additionally, we will provide code snippets support for both LSP and Kite for the next release - please stay tuned!\nWe, the Spyder development team, hope that these new improvements accelerate your productivity, and we invite you to beta-test them to improve them even more. As always, we are open to your questions, issue reports and contributions on all of our communication channels and our Github repository.",
      "tags": "Labs,Spyder",
      "url": "https://labs.quansight.org/blog/2019/08/spyder-40-beta4-kite-integration-is-here/"
    },
    {
      "title": "Quansight presence at SciPy'19",
      "text": "Yesterday the SciPy'19 conference ended. It was a lot of fun, and very productive. You can really feel that there's a lot of energy in the community, and that it's growing and maturing. This post is just a quick update to summarize Quansight's presence and contributions, as well as some of the more interesting things I noticed.\nA few highlights\nThe \"Open Source Communities\" track, which had a strong emphasis on topics like burnout, diversity and sustainability, as well as the keynotes by Stuart Geiger (\"The Invisible Work of Maintaining and Sustaining Open-Source Software\") and Carol Willing (\"Jupyter: Always Open for Learning and Discovery\") showed that many more people and projects are paying more attention to and evolving their thinking on the human and organizational aspects of open source. \nI did not go to many technical talks, but did make sure to catch Matt Rocklin's talk \"Refactoring the SciPy Ecosystem for Heterogeneous Computing\". Matt clearly explained some key issues and opportunities around the state of array computing libraries in Python - I highly recommend watching this talk.\nAbigail Cabunoc Mayes' talk \"Work Open, Lead Open (#WOLO) for Sustainability\" was fascinating - it made me rethink the governance models and roles we use for our projects, and I worked on some of her concrete suggestions during the sprints.\n\n\nFinally, the number of people interested in helping grow the community and projects was a highlight. In my own talk I identified a number of things that we (the NumPy project) were not doing well or not doing at all, and for literally every single concrete item or action I mentioned I had one or more people come up to me to volunteer their time and expertise. So I'll definitely be sticking my hand up with a \"we could really use some help here\" more often in the coming months.\nContributions by Quansight people\nCarol Willing gave a fascinating keynote, Jupyter: Always Open for Learning and Discovery. \nAnthony Scopatz must have lost out on some sleep: he gave a talk, Inequality of Underrepresented Groups in Core Project Leadership, co-taught two tutorials, Xonsh - Bringing Python Data Science to your Shell and RAPIDS: Open GPU Data Science, and entertailed us hosting the lightning talks every day (together with Paul Ivanov).\nAaron Meurer taught the RAPIDS tutorial with Anthony, and co-organized a Birds-of-a-Feather (BoF) session about SymPy.\nI gave a talk, Inside NumPy: Preparing for the Next Decade and co-organized and moderated a BoF: Mechanisms and Governance Issues for Funding Open Source Software in Science (a lot of ideas to follow up on from that!).\nChris Ostrouchov, Travis Oliphant, and Ivan Ogasawara gave lightning talks (I may be forgetting someone here, it was impossible to keep track of everything that was going on). James Bourbeau and Dharhas Pothina were also present; all of us stayed for the sprints, where we did everything from helping new people make their first open source contributions to in-depth design discussions, code review, and sketching out how the organizational structure of NumPy should be transformed.\nI hope to see many of you at SciPy'20 next year!",
      "tags": "Labs,SciPy",
      "url": "https://labs.quansight.org/blog/2019/07/quansight-at-scipy2019/"
    },
    {
      "title": "Ibis: Python data analysis productivity framework",
      "text": "Ibis is a library pretty useful on data analysis tasks that provides a pandas-like API that allows operations like create filter, add columns, apply math operations etc in a lazy mode so all the operations are just registered in memory but not executed and when you want to get the result of the expression you created, Ibis compiles that and makes a request to the remote server (remote storage and execution systems like Hadoop components or SQL databases). Its goal is to simplify analytical workflows and make you more productive.\nIbis was created by Wes McKinney and is mainly maintained by Phillip Cloud and Kriszti\u00e1n Sz\u0171cs. Also, recently, I was invited to become a maintainer of the Ibis repository!\nMaybe you are thinking: \"why should I use Ibis?\". Well, if you have any of the following issues, probably you should consider using Ibis in your analytical workflow!\n\nif you need to get data from a SQL database but you don't know much about SQL ...\nif you create SQL statements manually using string and have a lot of IF's in your code that compose specific parts of your SQL code (it could be pretty hard to maintain and it will makes your code pretty ugly) ...\nif you need to handle data with a big volume ...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want to learn more about ibis consider taking a look at these tutorials:\n\nhttps://docs.ibis-project.org/tutorial.html\n\nDo you want to watch some interesting video about Ibis? Check this out:\n\nhttps://www.youtube.com/embed/8Tzh42mQjrw?start=1625\n\nNow, let's check out some work developed here at Quansight in the last months!\nDuring the last months OmniSci and Quansight were working together to add a backend to Ibis for OmniSciDB (formerly MapD Core)! In a few words, OmniSciDB is an in-memory, column store, SQL relational database designed from the ground up to run on GPUs. If you don't know yet this amazing database, I invite you to check it out.\nThe implementation of this new backend also resulted in the creation of new expressions/operators on Ibis core, such as:\n\nGeoSpatial data types and operations\nTrigonometric operations\nSome statistcal operations\n\n\n\n\n\n\n\n\nFirst, let's connect to a OmniSciDB and play with this new features!\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \n# install the dependencies if you need!\n# !conda install -y ibis-framework=1.1.0 pyarrow pymapd vega geopandas geoalchemy2 shapely matplotlib --force-reinstall\n\n\n    \n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \nimport ibis\nfrom matplotlib import pyplot as plt\n\nprint('ibis:', ibis.__version__)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nibis: 1.2.0+7.g3afa8b0d\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \n# metis.mapd.com is used in some OmniSci docs\n# but maybe you want to install your own OmniSciDB instance\n# you can take a look into installation section at \n# https://www.omnisci.com/docs/latest/\n# also you maybe want to check the omniscidb-cpu conda package\n# conda install -c conda-forge omniscidb-cpu\n# if you need any help, feel free to open an issue at\n# https://github.com/conda-forge/omniscidb-cpu-feedstock/\nomniscidb_cli = ibis.mapd.connect(\n    host='metis.mapd.com', \n    user='mapd', \n    password='HyperInteractive',\n    port=443, \n    database='mapd',\n    protocol='https'\n)\n\n\n    \n\n\n\n\n\n\n\nGeoSpatial features\u00b6You need to handle geospatial data in a esier way?\nLet's take a look inside zipcodes_2017 table!\nWell, currently omniscidb backend doesn't support geopandas output, so let's use a workaround for that! It should be implemented into omniscidb backend soon! (see: gist-code)\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \ngist_url = 'https://gist.githubusercontent.com/xmnlab/587dd1bde44850f3117a1087ed3f0f28/raw/0750400db90cf97319a91aa514648c31ad4ace45/omniscidb_geopandas_output.py'\n!wget {gist_url} -O omniscidb_geopandas_output.py\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n--2019-07-05 11:31:57--  https://gist.githubusercontent.com/xmnlab/587dd1bde44850f3117a1087ed3f0f28/raw/0750400db90cf97319a91aa514648c31ad4ace45/omniscidb_geopandas_output.py\nResolviendo gist.githubusercontent.com (gist.githubusercontent.com)... 151.101.48.133\nConectando con gist.githubusercontent.com (gist.githubusercontent.com)[151.101.48.133]:443... conectado.\nPetici\u00f3n HTTP enviada, esperando respuesta... 200 OK\nLongitud: 1874 (1,8K) [text/plain]\nGuardando como: \u201comniscidb_geopandas_output.py\u201d\n\nomniscidb_geopandas 100%[===================>]   1,83K  --.-KB/s    en 0s      \n\n2019-07-05 11:31:57 (70,1 MB/s) - \u201comniscidb_geopandas_output.py\u201d guardado [1874/1874]\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \n# workaround to use geopandas output\nfrom omniscidb_geopandas_output import enable_geopandas_output \nenable_geopandas_output(omniscidb_cli)\n\n\n    \n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \nt = omniscidb_cli.table('zipcodes_2017')\ndisplay(t)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \nprint('# rows:', t.count().execute())\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n# rows: 33144\n\n\n\n\n\n\n\n\n\n\n\nThis table has ~33k rows. For this example, let's use just the first 1k rows.\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \nexpr = t[t.omnisci_geo].head(1000)\ndf = expr.execute()\n\n\n    \n\n\n\n\n\n\n\nInstead of getting all rows from the database and get from that the first 1000 rows, Ibis will prepare a SQL statement to get just the first 1000 rows! So it reduces the memory consuming to just the data you need!\nThis is what Ibis will request to the database:\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \nprint(expr.compile())\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nSELECT \"omnisci_geo\"\nFROM zipcodes_2017\nLIMIT 1000\n\n\n\n\n\n\n\n\n\n\n\nOf course geospatial data reading as text wouldn't be useful, so let's plot the result!\nRemember: we are using geopandas here!\n\n\n\n\n\n\nIn\u00a0[10]:\n\n    \n# let's add some custom style :)\nstyle_kwds = {\n    'linewidth': 2,\n    'markersize': 2,\n    'facecolor': 'red',\n    'edgecolor': 'red'\n}\n\ndf['omnisci_geo'].iloc[::3].plot(**style_kwds)\nplt.show()\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrigonometric operations\u00b6\n\n\n\n\n\n\nCurrently the OmniSciDB backend supports the follow trigonometric operations: acos, asin, atan, atan2, cos, cot, sin, tan.\nLet's check an example using a sin operation over rowid from zipcodes_2017.\n\n\n\n\n\n\nIn\u00a0[11]:\n\n    \n# if you want to use a SQL statement try`sql` method!\nt = omniscidb_cli.sql('select rowid from zipcodes_2017')\n\nexpr = t[t.rowid, t.rowid.sin().name('rowid_sin')].sort_by('rowid').head(100)\nexpr.execute().rowid_sin.plot()\nplt.show()\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSome statistical operations\u00b6\n\n\n\n\n\n\nThe OmniSciDB Ibis backend also implements some statistical operations, such as: Correlation (corr), Standard Deviation (stddev), Variance (var) and Covariance (cov).\nLet's check a pretty simple example: if there is any correlation in this dataset between per capita income and education.\n\n\n\n\n\n\nIn\u00a0[12]:\n\n    \nt = omniscidb_cli.table('demo_vote_clean')\n# remove some conflictives fields: 'TYPE', 'NAME', 'COUNTY' field\nfields = [name for name in t.schema().names if name not in ('TYPE', 'NAME', 'COUNTY')]\nt = t[fields].distinct()\nt.PerCapitaIncome.corr(t.Education).execute()\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[12]:\n\n\n\n\n\n0.7212061029308654\n\n\n\n\n\n\n\n\n\n\n\nThe result ~0.72 means that Per Capita Income and Education has a positive correlation in this dataset.\n\n\n\n\n\n\n\nConclusions\u00b6\n\n\n\n\n\n\nIbis is a cool library that can help you in your data analysis tasks. If you already use pandas, it will be pretty easy to add Ibis in your workflow!\nSo ...\n\nAre you excited to use Ibis? Try it out now!\nHave you already used Ibis? Reach out to me, ivan.ogasawara@quansight.com, and share your experience!\nAre you interested in contributing to Ibis? Check the good first issues label on GitHub!\nDo you want to add new features and want to fund Ibis? Contact us at info@quansight.com\n\n\n\n\n\n\n\nIn\u00a0[\u00a0]:",
      "tags": "Ibis",
      "url": "https://labs.quansight.org/blog/2019/07/ibis-python-data-analysis-productivity-framework/"
    },
    {
      "title": "uarray update: API changes, overhead and comparison to __array_function__",
      "text": "uarray is a generic override framework for objects and methods in Python. Since my last uarray blogpost, there have been plenty of developments, changes to the API and improvements to the overhead of the protocol. Let\u2019s begin with a walk-through of the current feature set and API, and then move on to current developments and how it compares to __array_function__. For further details on the API and latest developments, please see the API page for uarray. The examples there are doctested, so they will always be current.\n\n\n\n\n\n\n\nMotivation\u00b6Other array objects\u00b6NumPy is a simple, rectangular, dense, and in-memory data store. This is great for some applications but isn't complete on its own. It doesn't encompass every single use-case. The following are examples of array objects available today that have different features and cater to a different kind of audience.\n\nDask is one of the most popular ones. It allows distributed and chunked computation.\nCuPy is another popular one, and allows GPU computation.\nPyData/Sparse is slowly gaining popularity, and is a sparse, in-memory data store.\nXArray includes named dimensions.\nXnd is another effort to re-write and modernise the NumPy API, and includes support for GPU arrays and ragged arrays.\nAnother effort (although with no Python wrapper, only data marshalling) is xtensor.\n\nSome of these objects can be composed. Namely, Dask both expects and exports the NumPy API, whereas XArray expects the NumPy API. This makes interesting combinations possible, such as distributed sparse or GPU arrays, or even labelled distributed sparse or CPU/GPU arrays.\nAlso, there are many other libraries (a popular one being scikit-learn) that need a back-end mechanism in order to be able to support different kinds of array objects. Finally, there is a desire to see SciPy itself gain support for other array objects.\n__array_function__ and its limitations\u00b6One of my motivations for working on uarray were the limitations of the __array_function__ protocol, defined in this proposal. The limitations are threefold:\n\nIt can only dispatch on array objects.\nConsequently, it can only dispatch on functions that accept array objects.\nIt has no mechanism for conversion and coercion.\nSince it conflates arrays and backends, only a single backend type per array object is possible.\n\nThese limitations have been partially discussed before.\nuarray \u2014 The solution?\u00b6With that out of the way, let's explore uarray, a library that hopes to resolve these issues, and even though the original motivation was NumPy and array computing, the library itself is meant to be a generic multiple-dispatch mechanism.\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \n# Enable __array_function__ for NumPy < 1.17.0\n!export NUMPY_EXPERIMENTAL_ARRAY_FUNCTION=1\n\n\n    \n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \nimport uarray as ua\nimport numpy as np\n\n\n    \n\n\n\n\n\n\n\nIn uarray, the fundamental building block is a multimethod. Multimethods have a number of nice properties, such as automatic dispatch based on backends. It is important to note here that multimethods will be written by API authors, rather than implementors. Here's how we define a multimethod in uarray:\n\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \nmy_multimethod = ua.generate_multimethod(\n    # This is the argument extractor, it also defines the signature.\n    lambda a, b=None: (),\n    # This is the reverse dispatcher, it is important for conversion/coercion.\n    # It is optional and can be set to None.\n    lambda a, kw, d: (a, kw),\n    # This is the domain, it separates the multimethods into clean parts.\n    \"ql_blogpost\",\n    # This is the default implementation. It is also optional, which means\n    # \"error if no backend is found/set\".\n    default=lambda a, b=None: (a, b)\n)\n\n\n    \n\n\n\n\n\n\n\nWe will explore the function of each of the parts of this multimethod later. Let's try calling this multimethod.\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \nmy_multimethod(1, 2)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[4]:\n\n\n\n\n\n(1, 2)\n\n\n\n\n\n\n\n\n\n\n\nAs we can see, it remains faithful to its purpose. It has a default implementation that it can execute nicely. However, the real power comes when overriding this multimethod. To do this, we must consider the concept of a backend, which is separate. This is different from the view of __array_function__, in which array objects themselves define the backend. Here, we have applied the principle of separation of concerns to separate the multimethod, the objects it operates on, as well as the backend. Note as well that backend authors provide the implementation of a given API.\nLet\u2019s see how one would define a backend in uarray.\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \nclass Backend:\n    # A backend can only override methods from its own domain\n    __ua_domain__ = \"ql_blogpost\"\n    \n    # This is the main protocol a backend must have in order to work.\n    @staticmethod\n    def __ua_function__(\n        func,  # It gets the multimethod being called,\n        args, kwargs  # And the arguments the method is called with.\n    ):\n        # Here we have the implementation\n        return func.__name__, args, kwargs\n\n\n    \n\n\n\n\n\n\n\nNow, let's go about overriding the function. Note here that set_backend will typically be used by consumers of the API.\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \nwith ua.set_backend(Backend):\n    print(my_multimethod(1, 2))\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n('<lambda>', (1, 2), {})\n\n\n\n\n\n\n\n\n\n\n\nAs you can see, the function's return value magically changed. Note that this propagates all the way down the call stack. With that, let's get into some of the more magical features of uarray, starting with the function of the argument extractor, the argument replacer, and the __ua_convert__ protocol.\n\n\n\n\n\n\n\nThe argument extractor and argument-based dispatch\u00b6The argument extractor (equivalent to a dispatcher in __array_function__) is more than just a dummy that returns an empty tuple. It can also return the arguments needed for dispatch. Let\u2019s go ahead and see this in action.\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \nimport numbers\nmy_multimethod_with_dispatch = ua.generate_multimethod(\n    # a is dispatchable, and it's supposed to be some kind of number\n    lambda a, b=None: (ua.Dispatchable(a, numbers.Number),),\n    lambda a, kw, d: (a, kw),\n    \"ql_blogpost\"\n)\n\n\n    \n\n\n\n\n\n\n\nJust to illustrate what happens when a multimethod doesn't have a default implementation, let\u2019s call this multimethod.\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \nmy_multimethod_with_dispatch(1, 2)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n---------------------------------------------------------------------------\nBackendNotImplementedError                Traceback (most recent call last)\n<ipython-input-8-14d7909bfa3d> in <module>\n----> 1 my_multimethod_with_dispatch(1, 2)\n\nBackendNotImplementedError: No selected backends had an implementation for this function.\n\n\n\n\n\n\n\n\n\n\nIt raises an error, as is expected, with a message explaining the situation. Now, let's explore dispatch.\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \nclass DispatchBackendInt:\n    # A backend can only override methods from its own domain\n    __ua_domain__ = \"ql_blogpost\"\n    \n    @staticmethod\n    def __ua_function__(func, args, kwargs):\n        return \"int\", func.__name__, args, kwargs\n    \n    @staticmethod\n    def __ua_convert__(\n        dispatchables,  # The list of dispatchables\n        coerce # Whether or not to forcibly coerce them to the required form, if possible\n    ):\n        converted = []\n        for d in dispatchables:\n            # Check if it's a number, we only support ints\n            if d.type is numbers.Number and isinstance(d.value, int):\n                converted.append(d.value)\n            else:\n                return NotImplemented\n        \n        return converted\n\nclass DispatchBackendFloat:\n    # A backend can only override methods from its own domain\n    __ua_domain__ = \"ql_blogpost\"\n    \n    @staticmethod\n    def __ua_function__(func, args, kwargs):\n        return \"float\", func.__name__, args, kwargs\n    \n    @staticmethod\n    def __ua_convert__(dispatchables, coerce):\n        converted = []\n        for d in dispatchables:\n            # This one only supports floats\n            if d.type is numbers.Number and isinstance(d.value, float):\n                converted.append(d.value)\n            else:\n                return NotImplemented\n        \n        return converted\n\n\n    \n\n\n\n\n\n\nIn\u00a0[10]:\n\n    \nwith ua.set_backend(DispatchBackendInt), ua.set_backend(DispatchBackendFloat):\n    print(my_multimethod_with_dispatch(1, 2))\n    print(my_multimethod_with_dispatch(1.0, 2))\n    print(my_multimethod_with_dispatch(\"1\", 2))\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n('int', '<lambda>', (1, 2), {})\n('float', '<lambda>', (1.0, 2), {})\n\n\n\n\n\n\n    \n\n\n\n\n---------------------------------------------------------------------------\nBackendNotImplementedError                Traceback (most recent call last)\n<ipython-input-10-cdc811a82346> in <module>\n      2     print(my_multimethod_with_dispatch(1, 2))\n      3     print(my_multimethod_with_dispatch(1.0, 2))\n----> 4     print(my_multimethod_with_dispatch(\"1\", 2))\n\nBackendNotImplementedError: No selected backends had an implementation for this function.\n\n\n\n\n\n\n\n\n\n\nAs we can see, the method dispatches fine on int and float to the correct implementation but raises an error for strings. Let's make a small modification to make even working with str possible:\n\n\n\n\n\n\nIn\u00a0[11]:\n\n    \n# The argument replacer is supposed to replace the args/kwargs\n# with the right dispatchables\ndef my_ar(args, kwargs, dispatchables):\n    def replacer(a, b=None):\n        return dispatchables, {'b': b}\n    \n    return replacer(*args, **kwargs)\n\nmy_multimethod_with_dispatch = ua.generate_multimethod(\n    lambda a, b=None: (ua.Dispatchable(a, numbers.Number),),\n    my_ar,  # We put the right thing here.\n    \"ql_blogpost\"\n)\n\n\n    \n\n\n\n\n\n\nIn\u00a0[12]:\n\n    \nclass DispatchBackendInt:\n    # A backend can only override methods from its own domain\n    __ua_domain__ = \"ql_blogpost\"\n    \n    @staticmethod\n    def __ua_function__(func, args, kwargs):\n        return \"int\", func.__name__, args, kwargs\n    \n    @staticmethod\n    def __ua_convert__(dispatchables, coerce):\n        converted = []\n        for d in dispatchables:\n            if d.type is numbers.Number:\n                if isinstance(d.value, int):\n                    converted.append(d.value)\n                # If we're allowed to coerce it\n                elif coerce and d.coercible:\n                    try:\n                        converted.append(int(d.value))\n                    # Make sure unsupported conversions are caught\n                    except TypeError:\n                        return NotImplemented\n                else:\n                    return NotImplemented\n            else:\n                # Handle the base case\n                return NotImplemented\n        \n        return converted\n\n\n    \n\n\n\n\n\n\nIn\u00a0[13]:\n\n    \nwith ua.set_backend(DispatchBackendInt, coerce=True):\n    print(my_multimethod_with_dispatch(\"1\", 2))\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n('int', '<lambda>', (1,), {'b': 2})\n\n\n\n\n\n\n\n\n\n\n\nNote that we must pass the coerce=True parameter to make this work, otherwise the method will return NotImplemented and fail. Note also that the string has been correctly converted before being passed into the function. I previously mentioned the possibility of adding a reverse dispatcher to __array_function__, but it was rejected. I have added it to uarray, but it is optional.\n\n\n\n\n\n\n\nMeta-backends: Backends which may rely on others\u00b6One other thing easily possible in uarray not easily possible in other frameworks is the possibility of meta backends. These are backends which could rely on other backends. Suppose, that a backend wraps other objects and needs to get data out, and then call other backends:\n\n\n\n\n\n\nIn\u00a0[14]:\n\n    \nclass CustomNumber(numbers.Number):\n    def __init__(self, n):\n        self.n = n\n\n\n    \n\n\n\n\n\n\nIn\u00a0[15]:\n\n    \nclass DispatchBackendCustom:\n    # A backend can only override methods from its own domain\n    __ua_domain__ = \"ql_blogpost\"\n    \n    @staticmethod\n    def __ua_function__(func, args, kwargs):\n        with ua.skip_backend(DispatchBackendCustom):\n            return (\"Custom\",) + func(*args, **kwargs)\n            \n    @staticmethod\n    def __ua_convert__(dispatchables, coerce):\n        converted = []\n        for d in dispatchables:\n            if d.type is numbers.Number and isinstance(d.value, CustomNumber):\n                # Access the internal value, no conversion going on\n                converted.append(d.value.n)\n            else:\n                # Handle the base case\n                return NotImplemented\n        \n        return converted\n\n\n    \n\n\n\n\n\n\nIn\u00a0[16]:\n\n    \nwith ua.set_backend(DispatchBackendCustom), ua.set_backend(DispatchBackendInt):\n    print(my_multimethod_with_dispatch(CustomNumber(1), 2))\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n('Custom', 'int', '<lambda>', (1,), {'b': 2})\n\n\n\n\n\n\n\n\n\n\n\nPermanent registration of backend\u00b6uarray provides permanent registration for backends, after which the backend will be automatically tried every time. This is user-facing code, and we recommend that no libraries register themselves permanently, apart from reference implementations. This can be done via the register_backend method.\n\n\n\n\n\n\n\nComparison with __array_function__\u00b6Replacement and dispatching based on different objects\u00b6In this section, let\u2019s define a function with __array_function__, its uarray equivalent and explore the limitations of __array_function__ and how uarray attempts to resolve them. You can see the NumPy enhancement proposal defining __array_function__ here. Here's a snippet from NumPy showing roughly how np.sum would be defined (current code as of writing here):\ndef _sum_dispatcher(a, axis=None, dtype=None, out=None, keepdims=None,\n                    initial=None, where=None):\n    return (a, out)\n\n\n@array_function_dispatch(_sum_dispatcher)\ndef sum(a, axis=None, dtype=None, out=None, keepdims=np._NoValue,\n        initial=np._NoValue, where=np._NoValue):\n    # Default implementation\n\nHere's the equivalent uarray code:\ndef _sum_ar(a, kw, d):\n    def replacer(a, axis=None, dtype=None, out=None, **kwargs):\n        return (d[0],), {'out': d[1], 'axis': d[3], 'dtype': d[2], **kwargs}\n\naxis_type = object()\n\n@ua.create_multimethod(_sum_ar, \"numpy\", default=np.add.reduce)\ndef sum(a, axis=None, dtype=None, out=None, keepdims=None,\n        initial=None, where=None):\n    return (\n        ua.Dispatchable(a, np.ndarray),\n        ua.Dispatchable(out, np.ndarray),\n        ua.Dispatchable(dtype, np.dtype),\n        ua.Dispatchable(axis, axis_type),\n    )\n\nThis shows the power of uarray in a simple example:\n\nNot only can you dispatch based on ndarrays, you can convert and replace them too.\nYou can dispatch based on dtype, so (for example), ndtypes can be supported.\nYou can dispatch based on axis, so XArray could take over when the axes were named.\nIt allows you to have a fallback in terms of other multimethods.\nDefault keywords are dropped automatically, so one doesn\u2019t need to worry about missing kwargs that will be added later.\n\n\n\n\n\n\n\n\nReplacement of functions without any dispatchables\u00b6One of the biggest drawbacks of __array_function__ was the need for what I call a \"protocol soup\". For example, examine this issue which shows the number of extra functions that are required simply because NumPy can only perform dispatch on functions which accept array_like objects.\nuarray solves this in two ways: The first is by allowing the use of a context manager which sets the backend. So, for example, you could do the following:\nwith ua.set_backend(dask.array):\n    x = np.array(...)  # x becomes a Dask array\n\nThe second is allowing for dispatch on more object types, so more functions can be dispatched on.\n\n\n\n\n\n\n\nComplete example: np.ones using uarray\u00b6Here is a complete example of how one would override np.ones using uarray, with backends for NumPy, Dask, and XArray as examples. I will mention some advanced features one could use but won't implement them myself. For reference, let's see how np.ones would work with __array_function__: In any codebase using np.ones, someone would first have to replace it with np.ones_like, and then dispatch using __array_function__, which is a bit of a hack, and sort of undesirable. What's more, it would need to be done throughout the codebase. Let\u2019s first begin by defining the multimethod.\n\n\n\n\n\n\nIn\u00a0[17]:\n\n    \nshape_type = object()\n\ndef ones_ar(a, kw, d):\n    def replacer(shape, dtype=None, order='C'):\n        return (d[0],), dict(dtype=d[1], order=order)\n    return replacer(*a, **kw)\n\n@ua.create_multimethod(ones_ar, domain=\"numpy\")\ndef ones(shape, dtype=None, order='C'):\n    return ua.Dispatchable(shape, shape_type), ua.Dispatchable(dtype, np.dtype)\n\n\n    \n\n\n\n\n\n\n\nNow, we'll go on to define the NumPy backend, and register it:\n\n\n\n\n\n\nIn\u00a0[18]:\n\n    \nimport collections.abc as collections\n\nclass NumpyBackend:  # Ideally, this would be the numpy module itself.\n    __ua_domain__ = \"numpy\"\n    \n    @staticmethod\n    def __ua_function__(f, a, kw):\n        return getattr(np, f.__name__)(*a, **kw)\n    \n    @staticmethod\n    def __ua_convert__(dispatchables, coerce):\n        converted = []\n        \n        for d in dispatchables:\n            if d.type is np.ndarray:\n                try:\n                    # We ONLY want to coerce if coercion is true.\n                    if not hasattr(d.value, '__array_interface__') and not (coerce and d.coercible):\n                        return NotImplemented\n                    converted.append(np.asarray(d.value))\n                except (TypeError, ValueError):\n                    return NotImplemented\n            elif d.type is shape_type:\n                if not (isinstance(d.value, numbers.Integral)\n                        or isinstance(d.value, collections.Iterable)\n                        and all(isinstance(dim, numbers.Integral) for dim in d.value)):\n                    return NotImplemented\n\n                converted.append(tuple(int(dim) for dim in d.value)\n                        if isinstance(d.value, collections.Iterable)\n                        else (int(d.value),))\n            elif d.type is np.dtype:\n                try:\n                    converted.append(np.dtype(d.value))\n                except (TypeError, ValueError):\n                    return NotImplemented\n            else:\n                # Handle the base case, pass through everything else\n                converted.append(d.value)\n        return converted\n    \nua.set_global_backend(NumpyBackend)\n\n\n    \n\n\n\n\n\n\n\nSince we have set NumPy to be a global backend, it will always work.\n\n\n\n\n\n\nIn\u00a0[19]:\n\n    \nones((3, 4))\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[19]:\n\n\n\n\n\narray([[1., 1., 1., 1.],\n       [1., 1., 1., 1.],\n       [1., 1., 1., 1.]])\n\n\n\n\n\n\n\n\n\n\n\nLet\u2019s now move forward to Dask, and how its backend would be defined:\n\n\n\n\n\n\nIn\u00a0[20]:\n\n    \nimport dask.array as da\n\nclass DaskBackend:  # Ideally, this would be the dask.array module itself.\n    __ua_domain__ = \"numpy\"\n    \n    @staticmethod\n    def __ua_function__(f, a, kw):\n        return getattr(da, f.__name__)(*a, **kw)\n    \n    @staticmethod\n    def __ua_convert__(dispatchables, coerce):\n        converted = []\n        \n        for d in dispatchables:\n            if d.type is np.ndarray:\n                try:\n                    # We ONLY want to coerce if coercion is true.\n                    if not hasattr(d.value, '__array_interface__') and not (coerce and d.coercible):\n                        return NotImplemented\n                    converted.append(da.asarray(d.value))\n                except (TypeError, ValueError):\n                    return NotImplemented\n            else:\n                # Pass through everything else, we only want to dispatch on arrays.\n                converted.append(d.value)\n        return converted\n\n\n    \n\n\n\n\n\n\nIn\u00a0[21]:\n\n    \nwith ua.set_backend(DaskBackend):\n    print(ones((3, 4)))\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\ndask.array<ones, shape=(3, 4), dtype=float64, chunksize=(3, 4)>\n\n\n\n\n\n\n\n\n\n\n\nNote how a simple context manager allows things to magically work, unlike in __array_function__ where array creation functions are not supported at all.\nHere are a few things that could be done by Dask:\n\nIt could depend on unumpy and become a true, agnostic, meta-array.\nIt could define a backend factory, which automatically inserts the chunk sizes into functions.\n\nNext, let\u2019s move on to XArray.\n\n\n\n\n\n\nIn\u00a0[22]:\n\n    \nimport xarray\n\ndef xarray_ones(shape, dtype=None):\n    data = ones(tuple(shape.values()), dtype=dtype)\n    return xarray.DataArray(data, dims=tuple(shape.keys()))\n\nclass XarrayBackend:  # Ideally, this would be the xarray module itself.\n    __ua_domain__ = \"numpy\"\n    \n    @staticmethod\n    def __ua_function__(f, a, kw):\n        with ua.skip_backend(XarrayBackend):\n            if f is ones:\n                return xarray_ones(*a, **kw)\n        \n        return NotImplemented\n    \n    @staticmethod\n    def __ua_convert__(dispatchables, coerce):\n        converted = []\n        \n        for d in dispatchables:\n            if d.type is np.ndarray:\n                try:\n                    # We ONLY want to coerce if coercion is true.\n                    if not hasattr(d.value, '__array_interface__') and not (coerce and d.coercible):\n                        return NotImplemented\n                    converted.append(xarray.Dataset(d.value))\n                except (TypeError, ValueError):\n                    return NotImplemented\n            elif d.type is shape_type:\n                if not (isinstance(d.value, dict)\n                        and all(\n                            isinstance(k, str)\n                            and isinstance(v, numbers.Integral)\n                            for k, v in d.value.items()\n                        )):\n                    return NotImplemented\n\n                converted.append({k: int(v) for k, v in d.value.items()})\n            elif d.type is np.dtype:\n                try:\n                    converted.append(np.dtype(d.value))\n                except (TypeError, ValueError):\n                    return NotImplemented\n            else:\n                # Handle the base case, pass through everything else\n                converted.append(d.value)\n        return converted\n\n\n    \n\n\n\n\n\n\nIn\u00a0[23]:\n\n    \nwith ua.set_backend(XarrayBackend):\n    print(ones({'t': 3, 'x': 5}))\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n<xarray.DataArray (t: 3, x: 5)>\narray([[1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1.]])\nDimensions without coordinates: t, x\n\n\n\n\n\n\n\n\n\n\n\nNote here how a shape as a dictionary was automatically intercepted by XArray, and creates a suitable DataArray. This would simply not be possible in __array_function__. Let's explore a more exotic case:\n\n\n\n\n\n\nIn\u00a0[24]:\n\n    \nwith ua.set_backend(DaskBackend), ua.set_backend(XarrayBackend):\n    print(ones({'t': 3, 'x': 5}))\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n<xarray.DataArray 'ones-c74937aa1cf5a1e9c9dd0ca4c337fd2c' (t: 3, x: 5)>\ndask.array<shape=(3, 5), dtype=float64, chunksize=(3, 5)>\nDimensions without coordinates: t, x\n\n\n\n\n\n\n\n\n\n\n\nAnd just like that, you can compose the two, without either having to be aware of the other!\n\n\n\n\n\n\n\nThe overhead of uarray\u00b6One of the limiting factors in the adoption of uarray has been its overhead. This is currently being brought down by Peter Bell, a GSoC student working on uarray, who is working on a C++ implementation of the protocol. As of the last version, the overhead on my machine was about 449 ns per call (or about 12.4 Python function calls) for a simple function with no parameters that returns nothing, and this is still being worked on. You can follow the updates on Peter\u2019s work here. It is also being merged into scipy.fft as part of his ongoing GSoC work.\n\n\n\n\n\n\n\nConclusion\u00b6uarray provides a strong contender for a multiple-dispatch solution for array computing, and an alternative to __array_function__. While it is more feature-complete, it also needs a bit more boilerplate and has higher overhead than __array_function__. Here's to hoping for more options in the array computing space, and to separating the API from the implementation, which always opens more pathways to future improvement.",
      "tags": "",
      "url": "https://labs.quansight.org/blog/2019/07/uarray-update-api-changes-overhead-and-comparison-to-__array_function__/"
    },
    {
      "title": "Labs update and May highlights",
      "text": "Time flies when you're having fun. Here is an update of some of the highlights\nof my second month at Quansight Labs.\nThe making of a black hole image & GitHub Sponsors\nBoth Travis and myself were invited by GitHub to attend GitHub Satellite in\nBerlin. The main reason was that Nat Friedman (GitHub CEO) decided to spend the\nfirst 20 minutes of his keynote to highlight the Event Horizon\nTelescope's black hole image and the open\nsource software that made that imaging possible. This included the scientific\nPython very prominently - NumPy, Matplotlib, Python, Cython, SciPy, AstroPy and\nother projects were highlighted. At the same time, Nat introduced new GitHub\nfeatures like \"used by\", a triaging role and new dependency graph features and\nillustrated how those worked for NumPy. These features will be very welcome\nnews to maintainers of almost any project.\n\nThe single most visible feature introduced was GitHub Sponsors:\n\nI really enjoyed meeting Devon Zuegel, Product\nManager of the Open Source Economy Team at GitHub, in person after previously\nhaving had the chance to exchange ideas with her about the funding related\nneeds of scientific Python projects and their core teams. I'm confident that\nGitHub Sponsors will evolve in a direction that's beneficial to\ncommunity-driven open source projects.\n\n\nIt was also a pleasure meeting many people I have interacted with before or\nknew only by name, from core Python and Cython developers like Matthias Klose,\nMariatta Wijaya, Stefan Behnel and Christian Heimes, to scientific Python\nmaintainers like Antony Lee (Matplotlib), Marc Garcia (Pandas) and Brigitta\nSipocz (AstroPy).  The next day I took advantage of the opportunity to visit\nMaintainerati (highly recommended!) to exchange\nideas and experiences with a completely different set of maintainers, from\ncommunities as diverse as Node/Javascript, Kubernetes and Debian.\nArray computing\nStefan Krah added support for \"flexible arrays\" to XND. These\narrays are capable of supporting GeoJSON, which is an important format for\ngeospatial data. It's highly nested and irregularly shaped, so supporting it is\nnontrivial - as far as we're aware, XND is the only array library able to do\nthis.\nHameer Abbasi participated in the NumPy sprint at BIDS, which focused on the\nredesign of the dtype system. The NumPy team also made significant progress\ntowards NumPy 1.17, which is shaping up to be the most interesting NumPy\nrelease in a long time. The numpy.fft and numpy.random modules are almost\ncompletely rewritten (faster and more accurate FFTs, and new higher-quality\nrandom number generators!), and the __array_function__ protocol will be\nenabled by default. Hameer also received good feedback on\nuarray and improved the internals as\na result, resulting in much lower overhead and a much less nested call stack.\nJupyterLab\nThe Jupyter developers at Quansight have been busy. The list of issues in the\nJupyterLab 1.0 milestone\nis shrinking rapidly. The final release is expected in June, which will mark a\nhuge achievement of the JupyterLab team.\nThere has also been significant work going into two new features that are\nplanned for release post-1.0. Quoting team lead Ryan Henning:\n\"JupyterLab's Data Registry will have its first release on July 12 and will\nmake it easier for developers to create data-centric extensions for JupyterLab.\nThe concept of \"a dataset\" will finally be a first-class entity within\nJupyterLab, which will ignite tremendous potential for data-centric interactive\nplugins.\nThe debut of JupyterLab's built-in commenting feature will be on June 28 of\nthis year! It will include the ability to comment on any file in JupyterLab,\nand even the ability to comment on individual lines of text/code within source\nfiles.\"\nSpyder\nThe Spyder team grew in May - I was very happy to welcome\nEdgar Andr\u00e9s Margffoy Tuay to Quansight. With Carlos,\nGonzalo and Edgar we now have three Spyder maintainers at Quansight Labs. It's\ngreat to see the momentum of the Spyder project increasing.\nOn 21 May the second beta of Spyder 4.0 was released - it was 8 months in the\nmaking. The most visible features are the new dark theme and the plots pane:\n\nThere's a lot more though - from Language Server Protocol integration to better\npandas.DataFrame support and dedicated Cython, SymPy and Pylab consoles.\nGonzalo's blog post\ngoes into a lot more detail on this release.\nOther PyData core projects\nAaron Meurer is co-mentoring three Google Summer of Code projects for SymPy.\nSymPy has always benefited a lot from participating in GSoC, with a large part\nof SymPy's core team having started out via the program. This year seems to be\nshaping up to be a successful one again, so this is a great way for Aaron to\ninvest his time.\nFor SciPy, I finalized the Tidelift agreement (see the\nannouncement\nfor details). A steady project income of $2500/month is very nice to have, it\ngives the SciPy team the opportunity to do things like organize a developer\nmeeting or pay for activities that are hard to get done otherwise (e.g. a\nlong overdue website redesign).\nMy largest time investment this month for NumPy and SciPy was interacting with\ntech writers who are interested in participating in the\nGoogle Season of Docs program.\nThe quality and enthusiasm of potential applicants has exceeded all my\nexpectations, and I'm really looking forward to involving talented writers in\nthe projects.\nMeasuring Python API usage\nChris Ostrouchov cracked a tough nut: coming up with a practical method to\ndetermine how heavily functions in an API are used, and in what ways. This\nallows maintainers to obtain hard data when considering questions like \"can we\ndeprecate this function?\" or \"what should we focus on in our\ndocumentation?\". Chris wrote a\ndetailed blog post\non his methodology and initial results. He continues to improve his method in\npython-api-inspect, and\nwill soon be able to give people an interface to formulate their own questions.\nThere's a lot more to come here!\nFunding - Community Work Orders\nWithin Quansight we're starting to get a much clearer vision for how Labs\nfunding will work, and in particular the concept of a Community Work Order\n(CWO). In\nthis blog post\nI articulate our current vision in some detail. It's a nuanced proposition,\nhowever we're finding that the concept appeals to companies that have an\ninterest in supporting one or more core PyData projects. It strikes a nice\nbalance between giving project core teams the freedom to set directions and\nwork on things that benefit their whole community, and still meeting specific\nneeds that companies may have.\nI'm looking forward to being able to announce new CWOs in June.",
      "tags": "Labs",
      "url": "https://labs.quansight.org/blog/2019/06/labs-update-may-highlights/"
    },
    {
      "title": "TDK-Micronas partners with Quansight to sponsor Spyder",
      "text": "TDK-Micronas is sponsoring Spyder development efforts through Quansight Labs.\nThis will enable the development of some features that have been requested by\nour users, as well as new features that will help TDK develop custom Spyder\nplugins in order to complement their Automatic Test Equipment (ATE\u2019s) in the\ndevelopment of their Application Specific Integrated Circuits (ASIC\u2019s).\nAt this point it may be useful to clarify the relationship the role of\nQuansight Labs in Spyder's development and the relationship with TDK. To quote\nRalf Gommers (director of Quansight Labs):\n\n\"We're an R&D lab for open source development of core technologies around data\nscience and scientific computing in Python. And focused on growing communities\naround those technologies. That's how I see it for Spyder as well: Quansight\nLabs enables developers to be employed to work on Spyder, and helps with\nconnecting them to developers of other projects in similar situations. Labs\nshould be an enabler to let the Spyder project, its community and individual\ndevelopers grow. And Labs provides mechanisms to attract and coordinate\nfunding. Of course the project is still independent. If there are other\nfunding sources, e.g. donations from individuals to Spyder via OpenCollective,\nall the better.\"\n\nMultiple Projects aka Workspaces\nIn its current state Spyder can only handle one active project at a time.\nAlthough in the past we had basic support for workspaces, it was never a fully\nfunctional feature, so to ease development and simplify the user experience,\nwe decided to remove it in the 3.x series.\n\n\nFor TDK-Micronas, as well as for a large part of the Spyder user base, the use\nof multiple simultaneous projects is a common pattern, so this enhancement\nwill:\n\nMove from a single to a multiple active projects concept\nChange the name Project Explorer to WorkSpace Explorer.\nCopying from one project to another is done in the 'Explorer' by means of\n  context menus.\n\nIt is worth mentioning that projects are considered an \u201cadvanced feature\u201d,\nso it will not be forced on users that just want to get some work done.\nProjects usage is, and will remain, an optional feature.\nProject Types\nSince the revamp of Spyder Projects with the 3.x series, the new dialog hinted\nat the possibility of creating different types of projects. Currently Spyder\nprovides an \"empty project\" project type.\n\nThis feature will:\n\nExpand the (current) Project Explorer API to handle Project Types.\nDefine and implement the minimal aspects that should be extensible by a\n  Project Type\n\nFile Associations\nSpyder currently supports opening files on the Project and File Explorer\nusing the (default) Operating System defined applications. We will enhance\nthis by adding Global file associations as part of the Preferences\nand Project Specific associations as part of the Project Preferences.\nThis will give users more power to select the tool or set of tools that they\nprefer to use for specific projects and tasks directly on the File and Project\nExplorer pane.\nURL Awareness\nWorking with projects requires access to documentation, located within the\ncomputer as other files or links located over the internet. We will now\ninclude integration for URI detection within the editor so users can\nCtrl+Click on these items. This will work similarly as module\ninspection works, but generalized to external and internal links.\nGit version control enhancements\nThe revision system of choice by our user base, including TDK, is Git.\nThe purpose of this enhancement is to enable basic Git integration from\nwithin the File and Project Explorer and to extend the current plugin\nAPI so context menus can be customized with specific Git features and\ncommands.\nInitial work has already started to display Git relevant information\non the status bar\n\nGit status is now displayed in the status bar (on the left)\nEditor alternative views\nThere are many circumstances where a user would like to edit a file that might\nbe rendered using a parsing tool into another type of view. One example of\nthis is Markdown, the markup language in which this blog post is written and\nthat can be rendered to HTML. At the moment the only way to achieve this within\nSpyder is via the use of a dedicated plugin that would create a new type of\nPane. An example of such a plugin is spyder-reports\n(See image below and the Reports pane). The reason why plugins are developed\nthis way is because the Editor Plugin on Spyder can only contain Code Editor\nwidgets, which prevents us and other developers to use the Editor to hold\nthese alternate views of a given file.\n\nThe new alternative views feature will remove this limitation and enable the\ninclusion of generic widgets (views) of different types of files and provide\nan API for plugin extensions.\nClosing Remarks\nI would like to thank TDK-Micronas and Quansight for the opportunity to\nwork in open source development on an awesome product such as Spyder.\nI would also like to thank the users,\ncontributors\nand core developers for helping\nmake Spyder an awesome tool!",
      "tags": "Labs,Spyder",
      "url": "https://labs.quansight.org/blog/2019/06/tdk-partners-with-quansight-labs/"
    },
    {
      "title": "metadsl: A Framework for Domain Specific Languages in Python",
      "text": "metadsl: A Framework for Domain Specific Languages in Python\u00b6Hello, my name is Saul Shanabrook and for the past year or so I have been at Quansight exploring the array computing ecosystem. This started with working on the xnd project, a set of low level primitives to help build cross platform NumPy-like APIs, and then started exploring Lenore Mullin's work on a mathematics of arrays. After spending quite a bit of time working on an integrated solution built on these concepts, I decided to step back to try to generalize and simplify the core concepts. The trickiest part was not actually compiling mathematical descriptions of array operations in Python, but figuring out how to make it useful to existing users. To do this, we need to meet users where they are at, which is with the APIs they are already familiar with, like numpy. The goal of metadsl is to make it easier to tackle parts of this problem seperately so that we can collaborate on tackling it together.\nLibraries for Scientific Computing\u00b6Much of the recent rise of Python's popularity is due to its usage for scientific computing and machine learning. This work is built on different frameworks, like Pandas, NumPy, Tensorflow, and scikit-learn. Each of these are meant to be used from Python, but have their own concepts and abstractions to learn on top of the core language, so we can look at them as Domain Specific Languages (DSLs). As the ecosystem has matured, we are now demanding more flexibility for how these languages are executed. Dask gives us a way to write Pandas or NumPy and execute it across many cores or computers, Ibis allows us to write Pandas but on a SQL database, with CuPy we can execute NumPy on our GPU, and with Numba we can optimize our NumPy expession on a CPU or GPU. These projects prove that it is possible to write optimizing compilers that target varying hardware paradigms for existing Python numeric APIs. However, this isn't straightforward and these projects success is a testament to the perserverence and ingenuity of the authors. We need to make it easy to add reusable optimizations to libraries like these, so that we can support the latest hardware and compiler optimizations from Python. metadsl is meant to be a place to come together to build a framework for DSLs in Python. It provides a way to seperate the user experience from the the specific of execution, to enable consistency and flexibility for users. In this post, I will go through an example of creating a very basic DSL. It will not use the metadsl library, but will created in the same style as metadsl to illustrate its basic principles.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA simple DSL\u00b6We will create a simple language allowing you to add and multiply numbers and check if they are equal or greater than each other. The values will either be Python integers/booleans or strings representing abstract variables. We can represent our operations as Python functions, because they all take in some number of arguments and return a value:\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \ndef add(a, b):\n    ...\n\ndef mult(a, b):\n    ...\n\ndef equal(a, b):\n    ...\n\ndef gt(a, b):\n    ...\n\n\n    \n\n\n\n\n\n\n\nWe can also represent our constructors as functions:\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \ndef from_int(i):\n    ...\n\ndef from_str(s):\n    ...\n\n\n    \n\n\n\n\n\n\n\nBut what types should these functions return? Our goal here is build up the computation before we decide how to execute it, so each expression is defined by the operation and its arguments:\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \nimport dataclasses\nimport typing\n\n\n@dataclasses.dataclass\nclass Expression:\n    function: typing.Callable\n    arguments: typing.Tuple\n    \n    def __repr__(self):\n        return f\"{self.function.__qualname__}({', '.join(map(repr, self.arguments))})\"\n\n\n    \n\n\n\n\n\n\n\nWe can create an expression that adds the two variables \"a\" and \"b\":\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \na = Expression(from_str, (\"a\",))\nb = Expression(from_str, (\"b\",))\n\nExpression(add, (a, b))\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[4]:\n\n\n\n\n\nadd(from_str('a'), from_str('b'))\n\n\n\n\n\n\n\n\n\n\n\nIt would be more natural to be able to call the functions themselves to build up expressions. So let's rewrite the functions so they return expressions:\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \ndef from_str(s):\n    return Expression(from_str, (s,))\n\ndef add(a, b):\n    return Expression(add, (a, b))\n\n\nadd(from_str(\"a\"), from_str(\"b\"))\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[5]:\n\n\n\n\n\nadd(from_str('a'), from_str('b'))\n\n\n\n\n\n\n\n\n\n\n\nYou might notice that we are actually repeating ourselves a bit here. In each function, we repeat\nthe function name and the argument names.\nInstead of having to do this for each function, which is error prone, tendious, and ugly, we can\ncreate a decorator that does it for us. We create a decorator called expression that\ntakes in the initial function and returns a new one that\ncreates an Expression object with that function and the arguments passed in:\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \nimport functools\n\n\ndef expression(fn):\n\n    @functools.wraps(fn)\n    def expression_inner(*args, fn=fn):\n        return Expression(fn, args)\n\n    return expression_inner\n\n\n    \n\n\n\n\n\n\n\nNow we can rewrite our expression functions with this decorator in a concise way:\n\n\n\n\n\n\nIn\u00a0[\u00a0]:\n\n    \n@expression\ndef add(a, b):\n    ...\n\n@expression\ndef mult(a, b):\n    ...\n\n@expression\ndef equal(a, b):\n    ...\n\n@expression\ndef gt(a, b):\n    ...\n    \n@expression\ndef from_int(i):\n    ...\n\n@expression\ndef from_str(s):\n    ...\n\n\n    \n\n\n\n\n\n\n\nWe now have a concise way of defining operations that has a Pythonic API:\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \nmult(from_str(\"a\"), from_str(\"b\"))\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[7]:\n\n\n\n\n\nmult(from_str('a'), from_str('b'))\n\n\n\n\n\n\n\n\n\n\n\nAdding Typing\u00b6\n\n\n\n\n\n\nNow we can build up these expressions naturally, but there are some expressions that should not be allowed, for example:\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \nsome_bool = equal(from_str(\"a\"), from_str(\"b\"))\n\nmult(some_bool, some_bool)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[8]:\n\n\n\n\n\nmult(equal(from_str('a'), from_str('b')), equal(from_str('a'), from_str('b')))\n\n\n\n\n\n\n\n\n\n\n\nWe don't want to allow multiplying booleans in our language, only numbers. So this brings us to types. Types gives us an explicit and succinct way to restrict all the possible expressions to a subset that we consider meaningful or valid. Simple types can't eliminate all errors (like dividing by zero), but they can give us some guide posts for what is allowed. They also provide us with a mental model of our domain. So how do we add types? Let's subclass Expression for the two types we have defined.\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \nclass Boolean(Expression):\n    pass\n\n\nclass Number(Expression):\n    pass\n\n\n    \n\n\n\n\n\n\n\nNow we can also define our operations as Python's dunder methods, allowing us to use the + and * infix operators instead of requiring functions. However, you might notice that now our expression function is no longer valid. We don't want to return an Expression anymore, but either a Boolean or Number. So we can rewrite our expression function to first take in an expression_type argument,\nthen return a decorator for that expression type:\n\n\n\n\n\n\nIn\u00a0[10]:\n\n    \ndef expression(expression_type):\n\n    def expression_inner(fn, fn=fn, expression_type=expression_type):\n        @functools.wraps(fn)\n        def expression_inner_inner(*args, fn=fn):\n            return expression_type(fn, args)\n\n        return expression_inner_inner\n\n    return expression_inner\n\n\n    \n\n\n\n\n\n\n\nAnd we can add our dunder methods to the types:\n\n\n\n\n\n\nIn\u00a0[13]:\n\n    \nclass Number(Expression):\n    @expression(Number)\n    def __add__(self, other: Number) -> Number:\n        ...\n    \n    @expression(Number)\n    def __mul__(self, other: Number) -> Number:\n        ...\n    \n    @expression(Boolean)\n    def __eq__(self, other: Number) -> Boolean:\n        ...\n\n    @expression(Boolean)\n    def __gt__(self, other: Number) -> Boolean:\n        ...\n    \n    @staticmethod\n    @expression(Number)\n    def from_int(i: int) -> Number:\n        ...\n\n    @staticmethod\n    @expression(Number)\n    def from_str(s: str) -> Number:\n        ...\n\n\n    \n\n\n\n\n\n\nIn\u00a0[14]:\n\n    \n(Number.from_int(10) + Number.from_str(\"A\")) == Number.from_int(10)\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[14]:\n\n\n\n\n\nNumber.__eq__(Number.from_int(10), Number.__add__(Number.from_int(10), Number.from_str('A')))\n\n\n\n\n\n\n\n\n\n\n\nNext Steps\u00b6\n\n\n\n\n\n\nSo now we have created a lightweight way to represent this DSL in Python that supports static type analysis by MyPy (or other tools). The Expression class we have defined here is conceptual core of metadsl. On top of this, metadsl provides:\n\nA type safe way to define replacements on this graph and a system to apply replacements repeatedly to allow execution/compilation. This would allow us to actually evaluate this DSL in some way or optimize it.\nA way to create \"wrapper\" classes that also build up expression graphs, but can take in regular Python objects. This would allow us to add an Number to an int, instead of first having to call from_int on it.\n\nWe are working towards representing the full NumPy API faithfully to support translating it to other APIs, like that of Torch or Tensorflow, and also optimize it with the Mathematics of Arrays formalism. We are actively looking for other projects that this effort would be useful for and welcome any collaboration. Feel free to raise an issue on our Github repo, or reach our to me directly at saul@quansight.com to set up a time to chat more in depth. Nothing here is set in stone. It is just a couple of ideas that we have found useful as we explore this space. While it is by no means a complete solution we hope it can be a meeting place to grow this concept to suit the needs of the Python community.",
      "tags": "",
      "url": "https://labs.quansight.org/blog/2019/05/metadsl-dsl-framework/"
    },
    {
      "title": "Community-driven open source and funded development",
      "text": "Quansight Labs is an experiment for us in a way. One of our main aims is to\nchannel more resources into community-driven PyData projects, to keep them\nhealthy and accelerate their development. And do so in a way that projects\nthemselves stay in charge.\nThis post explains one method we're starting to use for this. I'm writing it\nto be transparent with projects, the wider community and potential funders\nabout what we're starting to do. As well as to explicitly solicit feedback on\nthis method.\nCommunity work orders\nIf you talk to someone about supporting an open source project, in particular a\nwell-known one that they rely on (e.g. NumPy, Jupyter, Pandas), they're often\nwilling to listen and help. What you quickly learn though is that they want\nto know in some detail what will be done with the funds provided. This is\ntrue not only for companies, but also for individuals. In addition, companies\nwill likely want a written agreement and some form of reporting about the\nprogress of the work. To meet this need we came up with community work\norders (CWOs) - agreements that outline what work will be done on a project\n(implementing new features, release management, improving documentation,\netc.) and outlining a reporting mechanism. What makes a CWO different from a\nconsulting contract? Key differences are:\n\nIt must be work that is done on the open source project itself (and not e.g.\n   on a plugin for it, or a customization for the client).\nThe developers must have a reasonable amount of freedom to decide what to\n   work on and what the technical approach will be, within the broad scope of\n   the agreement.\nDeliverables cannot be guaranteed to end up in a project; instead the funder\n   gets the promise of a best effort of implementation and working with the\n   community.\n\nRespecting community processes\nPoint 3 above is particularly important: we must respect how open source\nprojects make decisions. If the project maintainers decide that they don't\nwant to include a particular change or new feature, that's their decision to\nmake. Any code change proposed as part of work on a CWO has to go through the\nsame review process as any other change, and be accepted on its merits. The\nargument \"but someone paid for this\" isn't particularly strong, nor is one\nthat reviewers should have to care about. Now of course we don't expect it to\nbe common for work to be rejected. An important part of the Quansight value\nproposition is that because we understand how open source works and many of\nour developers are maintainers and contributors of the open source projects\nalready, we propose work that the community already has interest in and we\nopen the discussion about any major code change early to avoid issues.\n\n\nThere may be other community processes or preferences to respect. For example,\nJupyterLab has a Corporate Engagement and Contribution Guide\nwhich formulates guidelines for how corporations should engage with and\ncontribute to the project.\nThe principle is simple here: be considerate to what the community wants, and\noperate transparently so if we make honest mistakes we get feedback and can\ntake corrective action quickly.\nEngaging companies to fund work on projects\nThere's an impedance mismatch between companies and open source projects. For\nstarters, it may not be clear who speaks for the project - a \"steering\ncouncil\", one or more maintainers, an entity like NumFOCUS, or ...? Then,\nwhen you do talk to someone involved in the project, chances are that he or she\nwill not be in a position to do anything. I've personally experiences this\nmultiple times in the past. As one of the more visible maintainers of SciPy and\nNumPy I received offers of financial support or paid development work\noccasionally. The answer was always easy: even if I were interested, I have a\njob and my employment contract does not allow me to take on any other paid\nwork.  So I cannot help you. Referring the inquirer to, e.g., NumFOCUS doesn't\nhelp - unless there's a developer who is in the position to accept a contract,\nnot much can be done.\nQuansight Labs changes this dynamic. There's a pool of people at Quansight\nthat we can draw on already. The chicken and egg problem (how to start a\ncareer working on open source before you have contracts in place) is much\nmore tractable now, because we can hire developers to work on a mix of open\nsource and consulting, and ramp up the time spent on open source as funding\nallows.\nThat brings us to the next step: how to bring in this funding. With Matt\nHarward and Jim Martino we have a couple of very good sales people, who aim\nto find companies to engage in open source activities and to build long-term\nrelationships with. In their contacts with potential clients, they can\nexplore whether a CWO is of interest to the client and pull in a more\ntechnical person familiar with the project(s) of interest to the client as\nthe conversation progresses. To get started though, they need conversation\nstarters - things the open source project wants or needs. Telling a client\n\"good to hear that you're interested in NumPy, they need some general\nmaintenance\" isn't going to go very far. Which brings us to: roadmaps.\nOf roadmaps and road blocks\n\nA roadmap is a strategic plan that defines a goal or desired outcome, [...].\nIt also serves as a communication tool, a high-level document that helps\narticulate the strategic thinking \u2014 the why \u2014 behind both goal and the plan\nfor getting there.\n\nAdapted from here. Note that\n\"plan\" does not imply a timeline (that's usually not possible to specify for\nopen source projects).\nA few years ago the number of projects in the PyData ecosystem that had a\nroadmap was at or very close to zero. That's slowly starting to change. At\nlast years' NumFOCUS Summit, Brian Granger and I led a session on roadmaps,\nto share experiences and best practices in writing roadmaps. In preparation\nfor that session I surveyed the roadmaps of all NumFOCUS projects. About half\nthe projects had a roadmap, and of those roadmap again about half was\noutdated or very incomplete. So eight months ago only 25% of projects had a\ngood roadmap, today it's probably a little higher. That's not a lot if we\nwant to find roadmap items as conversation starters for all projects we're\ninterested in. Luckily we can talk to project maintainers and get a few big\nticket items from them (in most cases) that we can use instead.\nHere's the idea: we look at a project roadmap, take a couple of ideas that we\nthink are most likely to be of interest to a company, put those on a\nbrochure, and let our sales team take it from there to use (to support a \nconversation, generate some initial interest, provide an overview of the\nbreadth of our interests and capabilities at Quansight Labs, etc.). Here's what\nthat currently looks like:\n\nWe aim for wide coverage, even if we don't employ a project maintainer (yet).\nFor example, Matplotlib is an important part of the PyData ecosystem, so we\nwant to include it. If there's interest, we talk to the Matplotlib core team\nand hire or contract a developer that's a good fit. Sometimes the egg has to\ncome before the chicken.\nThis process mostly seems to work. The exception is when a project does not\nwant to provide any roadmap items. That can have different reasons. Some\ndevelopers simply don't like the word \"roadmap\", because they are of the\nopinion that it implies a promise or a timeline. That's mostly semantics\nthough - if \"wish list\" works better, simply use that instead. For the\npurpose of listing some high priority items, the exact name you use doesn't\nreally matter. And if you really can't think of any high priority wants or\nneeds, then perhaps your project is done (is it, really? you don't want\nfunding? nor volunteer contributors?).\nOne of the things we (Anthony Scopatz and I) will continue doing is working\nwith projects to improve their roadmaps. If you're interested in this, please\nreach out.\nRelationship with NumFOCUS\nNow I hear you asking: why can't NumFOCUS do all this? The answer is: it\ndepends. If any company wants to provide funds to support a project with no\nstrings attached, doing that via NumFOCUS is likely the best option and we\nwill advise that company to go that route. The nonprofit status of NumFOCUS\nensures that the funding is tax-deductible (at least in the US), which is\nlikely to be appealing. It's important to understand though that that for\nprovided funding to be deductible, it has to be unrestricted. It cannot be\n\"work for hire\". A concrete example: giving $100k to support Jupyter\ndevelopment is tax-deductible, giving the same amount to implement a better\ntable widget in Jupyter (or any other concrete feature) is not.\nAnother important difference is that NumFOCUS does not have developers on\nstaff. NumFOCUS provides services to open source projects, therefore its\nstaff is mainly skilled in things like financial administration, event\nplanning, and marketing. When a project wants to use the funds administered\nfor them by NumFOCUS, it needs to subcontract to an individual contractor or\nanother company. Therefore Quansight Labs being a subcontractor to NumFOCUS\nfor work an open source project wants done is perfectly feasible. Now of\ncourse, if funding is not tax-deductible because it's for specific features\nand it needs to be executed by a developer or team employed by Quansight\nLabs, going via NumFOCUS can be an extra layer of indirection/overhead. At\nthat point, a CWO may be the better option.\nSo in summary:\n\nIf funding is unrestricted and in support of an open source project, go\n  to NumFOCUS.\nIf the funding is for a particular purpose and requires a statement of work,\n  consider a community work order with Quansight Labs.\n\nWhat to pay for?\nThere's a set of important questions that I haven't touched upon yet about\nintroducing money into a (mostly) volunteer-driven project. This will be the\ntopic of a follow-up post, as well as a talk at SciPy'19 which evalutes the\nstate of NumPy after a first year of paid development by two full-time\nengineers. One thing is clear: introducing funding into a community-driven\nproject has to be given careful thought. We should aim to pay for things that\nincrease the health of the project and make the lives of volunteer contributors\neasier rather than harder.\nGive us feedback\nWe want to hear from you. Whether you have critical feedback, a particular\nconcern, an idea or opportunity that could help, or anything else that's\nrelevant - please tell us about it. We are trying to do something here that\nwe know is hard. Working with a large and diverse community is challenging.\nWe feel that it will be ultimately very effective and rewarding though.\nYou can reach out via email (rgommers@quansight.com), Twitter\n(@quansightai), or in person at one of the many conferences we visit. I will\nalso actively be reaching out to maintainers of the projects we're including in\nour sales and marketing conversations.",
      "tags": "community,funding,Labs",
      "url": "https://labs.quansight.org/blog/2019/05/community-driven-opensource-funded-development/"
    },
    {
      "title": "Measuring API usage for popular numerical and scientific libraries",
      "text": "Developers of open source software often have a difficult time\nunderstanding how others utilize their libraries. Having better data of\nwhen and how functions are being used has many benefits. Some of these\nare:\n\nbetter API design\ndetermining whether or not a feature can be deprecated or removed.\nmore instructive tutorials\nunderstanding the adoption of new features\n\nPython Namespace Inspection\nWe wrote a general tool\npython-api-inspect\nto analyze any function/attribute call within a given set of\nnamespaces in a repository. This work was heavily inspired by a blog\npost on inspecting method usage with\nGoogle BigQuery\nfor pandas,\nNumPy, and\nSciPy. The previously mentioned work used\nregular expressions to search for method usage. The primary issue with\nthis approach is that it cannot handle import numpy.random as rand;\nrand.random(...) unless additional regular expressions are\nconstructed for each case and will result in false\npositives. Additionally,\nBigQuery is not a free resource.\nThus, this approach is not general enough and does not scale well with\nthe number of libraries that we would like to inspect function and\nattribute usage.\nA more robust approach is to inspect the Python abstract syntax tree\n(AST). Python comes with a performant method from the ast\nmodule ast.parse(...)\nfor constructing a Python AST from source code. A node\nvisitor\nis used to traverse the AST and record import statements, and\nfunction/attribute calls. This allows us to catch any absolute\nnamespace reference. The following are cases that\npython-api-inspect\ncatches:\n\n\nimport numpy\nimport numpy as np\nimport numpy.random as rnd\nfrom numpy import random as rand\n\nnumpy.array([1, 2, 3])\nnumpy.random.random((2, 3))\nnp.array([1, 2, 3])\nrnd.random((2, 3))\nrand.random((2, 3))\n\n\n\nThere are limitations to this approach since Python is a heavily\nduck-typed language. To understand this see the following two\nexamples.\ndef foobar(array):\n    return array.transpose()\n\na = numpy.array(...)\n\na.transpose()\nfoobar(a)\n\n\n\nHow is one supposed to infer that a.transpose() is a numpy\nnumpy.ndarray method or foobar is a function that takes a\nnumpy.ndarray as input? These are open questions that would allow\nfor further inspection of how libraries use given functions and\nattributes. It should be noted that dynamically typed languages in\ngeneral have this\nproblem. Now\nthat the internals of the tool have been discussed, the usage is quite\nsimple. The repository\nQuansight-Labs/python-api-inspect\ncomes with two command line tools (Python scripts). The important tool\ninspect_api.py has heavy caching of downloaded repositories and\nsource files that have been analyzed. Inspecting a file the second\ntime is a sqlite3 lookup. Currently, this repository inspects 17\nlibraries/namespaces and around 10,000 repositories (100 GB\ncompressed). It has been designed to have no other dependencies than\nthe Python stdlib and easily run\nfrom the command line. Below is the command that is run when\ninspecting all the libraries that depend on numpy.\npython inspect_api.py data/numpy-whitelist.ini \\\n  --exclude-dirs test,tests,site-packages \\\n  --extensions ipynb,py \\\n  --output data/inspect.sqlite\n\n\n\nThe command comes with several options that can be useful for\nfiltering the results. --exclude-dirs is used to exclude directories\nfrom counts (e.g. tests directory or site-packages directory)\nwithin a repository. This option reveals the use of a given namespace\nin tests as opposed to within the library. --extensions is by\ndefault all Python files *.py but can also include Jupyter notebooks\n*.ipynb showing us how users use a namespace in an interactive\ncontext. Unsurprisingly this work found that many Jupyter notebooks in\nrepositories have syntax errors.\nWhile not the focus of this post, an additional script is provided in\nthe repository dependant-packages.py. This script is used to\npopulate the data/numpy-whitelist.ini file with repositories that\ndepend on numpy. This would not be possible without the libraries.io\nAPI. It is a remarkable project which\ndeserves more attention.\nResults\nThe table below summarizes the findings of namespace usage within all\n*.py files, all *.py in only test directories, all *.py files\nexcluding ones within test directories (tests, test), and only\nJupyter notebook *.ipynb files. All of the results are provided as\ncsv files. It is important to note that the inspect_api.py script\ngets much more detail than is included in the csv files and there is\nplenty of additional work that could be done with this tool for\ngeneral Python ast analysis.\n\n\n  Library\n  Whitelist\n  Summary only `.py`\n  Summary only `.py` tests\n  Summary only `.py` without tests\n  Summary only `.ipynb`\n\n\n  astropy\n  ini\n  csv\n  csv\n  csv\n  csv\n\n\n  dask\n  ini\n  csv\n  csv\n  csv\n  csv\n\n\n  ipython\n  ini\n  csv\n  csv\n  csv\n  csv\n\n\n  ipywidgets\n  ini\n  csv\n  csv\n  csv\n  csv\n\n\n  matplotlib\n  ini\n  csv\n  csv\n  csv\n  csv\n\n\n  numpy\n  ini\n  csv\n  csv\n  csv\n  csv\n\n\n  pandas\n  ini\n  csv\n  csv\n  csv\n  csv\n\n\n  pyarrow\n  ini\n  csv\n  csv\n  csv\n  csv\n\n\n  pymapd\n  ini\n  csv\n  csv\n  csv\n  csv\n\n\n  pymc3\n  ini\n  csv\n  csv\n  csv\n  csv\n\n\n  pytorch\n  ini\n  csv\n  csv\n  csv\n  csv\n\n\n  requests\n  ini\n  csv\n  csv\n  csv\n  csv\n\n\n  scikit-image\n  ini\n  csv\n  csv\n  csv\n  csv\n\n\n  scikit-learn\n  ini\n  csv\n  csv\n  csv\n  csv\n\n\n  scipy\n  ini\n  csv\n  csv\n  csv\n  csv\n\n\n  statsmodels\n  ini\n  csv\n  csv\n  csv\n  csv\n\n\n  sympy\n  ini\n  csv\n  csv\n  csv\n  csv\n\n\n  tensorflow\n  ini\n  csv\n  csv\n  csv\n  csv\n\n\n\nSince many namespaces were checked we will highlight only some of the\nresults. First for NumPy the\nunsurprising function calls: numpy.array, numpy.zeros,\nnumpy.asarray, numpy.arange, numpy.sqrt, numpy.sum, and\nnumpy.dot. There are\nplans\nto deprecate numpy.matrix and this seem possible since it\nnumpy.matrix\nis not in the top 150 functions calls. Numpy testing functions were\nthe expected testing.assert_allclose, testing.assert_almost_equal,\nand testing.assert_equal.\nSciPy acts as a glue for many algorithms\nneeded for scientific and numerical work. The usage of scipy is\nsurprising and also possibly the most accurate results of the\nfollowing analysis. This is due to the fact that scipy tends to be\nfunction wrappers over lower level routines and less class instance\nmethods which are harder to detect as discussed above. The sparse\nmethods are heavily used along with several high level wrappers for\nscipy.interpolate.interp1d and scipy.optimize.minimize. I was\nsurprised to find out one of my favorite SciPy methods,\nscipy.signal.find_peaks, is rarely used! Only a small fraction of the\nscipy.signal functions are used and these include:\nscipy.signal.lfilter, scipy.signal.fftconvolve,\nscipy.signal.convolve2d, scipy.signal.lti, and\nscipy.signal.savgol_filter.\nscikit-learn is a popular library\nfor data analysis and offers some of the traditional machine learning\nalgorithms. Interestingly here we order the most used models.\n\nsklearn.linear_model.LogisticRegression\nsklearn.decomposition.PCA\nsklearn.ensemble.RandomForestClassifier\nsklearn.cluster.KMeans\nsklearn.svm.SVC\n\npandas is another popular data analysis\nlibrary for tabular data that helped drive the popularity of\nPython. One of the huge benefits of pandas is that it allows reading\nmany file formats to a single in memory pandas.DataFrame\nobject. Unsurprisingly the most popular pandas functions are\npandas.DataFrame and pandas.Series. Here we rank the most popular\npandas.read_* functions.\n\npandas.read_csv\npandas.read_table\npandas.read_sql_query\npandas.read_json\npandas.read_pickle\n\nrequests makes working\nwith HTTP requests easier to work with than the stdlib\nurllib.request\nand is one of the most downloaded\npackages. Looking at the\ndata for usage of requests, three functions are primarily used\n(everything else is used 3-5x less): requests.get, requests.post,\nand requests.Session with headers being the most common argument.\nOverall it is clear that libraries are being used differently within\neither a package, tests, or notebooks. Notebooks tend to prefer high\nlevel routines such as scipy.optimize.minimize, numpy.linspace,\nmatplotlib.pyplot.plot which can be used for demos. Additionally\nnotebook function usage would be a good metric for material that is\nworthwhile to include in introduction and quick-start\ndocumentation. The same goes for testing and development documentation\nthat is equally informed as to what functions are used in tests and in\npackages. Further work is necessary to generalize this tool as it\ncould be useful for the Python ecosystem to better understand through\nanalytics how the language is being used.",
      "tags": "Labs",
      "url": "https://labs.quansight.org/blog/2019/05/python-package-function-usage/"
    },
    {
      "title": "Spyder 4.0 takes a big step closer with the release of Beta 2!",
      "text": "It has been almost two months since I joined Quansight in April, to start \nworking on Spyder maintenance and \ndevelopment. So far, it has been a very exciting and rewarding journey under \nthe guidance of long time Spyder maintainer\nCarlos C\u00f3rdoba.\nThis is the first of a series of blog posts we will be writing to showcase \nupdates on the development of Spyder, new planned features and news on the \nroad to Spyder 4.0 and beyond.\nFirst off, I would like to give a warm welcome to\nEdgar Margffoy,\nwho recently joined Quansight and will be working with the Spyder team to\ntake its development even further. Edgar has been a core Spyder developer \nfor more than two years now, and we are very excited to have his (almost)\nfull-time commitment to the project.\nSpyder 4.0 Beta 2 released!\nSince August 2018, when the first beta of the 4.x series was released, the\nSpyder development team has been working hard on our next release.\nOver the past year, we've implemented the long awaited full-interface dark theme;\noverhauled our entire code completion and linting architecture to use the\nLanguage Server Protocol,\nopening the door to supporting many other languages in the future;\nadded a new Plots pane to view and manage the figures generated by your code;\nand numerous other feature enhancements, bug fixes and internal improvements.\nDark theme\nA full-interface dark theme has been a\nlong awaited feature,\nand is enabled by default in Spyder 4. You can still select the\nlight theme under  Preferences > Appearance by either choosing a light-background\nsyntax-highlighting scheme, or changing Interface theme to Light.\n\nPretty, right :-) ?\n\n\nThis enhancement was made possible by the work of\nColin Duquesnoy,\nDaniel Pizzeta\nand their QDarkStyle\npackage. The Spyder team is now actively collaborating with Colin and Daniel\nto pursue the release of QDarkStyle 3.x, which will be using Spyder's\nQtSASS\npackage to harness the power of SASS/SCSS and allow users to fully customize\nthe theme dynamically.\nLanguage Server Protocol architecture\nThe Language Server Protocol (LSP) was created by Microsoft for Visual Studio Code to standardize how\ndevelopment tools (e.g. editors and IDEs) communicate with\nservers that provide code completion, linting and related facilities for\ndifferent programming languages. With LSP, as\nthey describe it:\n\nA single Language Server can be reused in multiple development tools,\nwhich in turn can support multiple languages with minimal effort.\nLSP is a win for both language providers and tooling vendors!\n\nAs of Spyder 4 Beta 2, Spyder is now one of those tools!\nWe developed our own client to communicate with any server that implements\nLSP v3.0 through a transport layer that uses ZeroMQ sockets. Code completion,\nhelp generation, calltips, and real-time code/style analysis were rewritten to take advantage of this\narchitecture, and hover hints and docstring style analysis were added.\nFurther LSP features, such as workspace functionality and on the fly completion, will come in future betas.\n\nOur current support is geared towards Python, using the great\nPython-Language-Server\npackage. This has allowed us to provide fine-grained configurability for\nPycodestyle and\nPydocstyle options, and in future\nbetas we\u2019ll also add the ability to use and configure code formatters like\nYAPF and autopep8.\n\nWe support configuring LSP servers for additional, non-Python programming languages.\nIn the future, we hope to include out-of-the-box LSP integration for\nsome of the most popular languages in the scientific computing space,\nincluding Fortran, Julia and C/C++.\n\nPlots pane\nSimilar to RStudio and other data science IDEs,\nSpyder now includes a Plots pane, allowing you to browse all figures created during a\nsession. Beyond just viewing plots, you can also zoom, save/export, copy and remove\nthem.\n\nBut wait, there's more!\nThere are numerous additional features we've added in the previous 10\nmonths! These include:\n\nAutosave and File Recovery in the editor, so Spyder can restore your\nunsaved files in case it crashes or something else goes wrong.\nDedicated Sympy, Cython and Pylab consoles, making it\nvery simple to quickly explore and create code for these libraries.\nOS level window pane undocking, allowing you to easily\norganize panes across different monitors.\nSupport for multi-indexes\nin our Dataframe viewer, making working with complex datasets much easier.\n\nWe will describe all of these additional enhancements in greater detail in\nfuture blog posts.\nYou can help!\nIf would like to help us test this beta release and try out the new features it offers,\nyou can! You can install it with conda (if using Anaconda/Miniconda, as we recommend),\nor with pip; we suggest doing so in a new Conda env or virtualenv/venv\nso you can easily switch between your existing Spyder install and the Spyder 4 beta.\nFor example, to do so with conda, enter the following at the Terminal/Anaconda prompt:\n$ conda create --channel spyder-ide --name spyder-beta spyder=4.0.0b2\n$ conda activate spyder-beta\n$ spyder\n\n\n\nThis is a safe process because Spyder now uses a different configuration\ndirectory for its development versions, so you can easily switch between our stable\nand beta releases without worrying about one affecting the other.\nIf you find any bugs, you can report them on our\nissue tracker;\nmake sure to search for your error message or behavior before making a new one.\nClosing remarks\nI would like to thank Quansight for the opportunity of working in open source\ndevelopment on an awesome project like Spyder. I am also grateful to our\nusers, contributors\nand core developers for helping\nmaking Spyder amazing!",
      "tags": "Labs,Spyder",
      "url": "https://labs.quansight.org/blog/2019/05/spyder-4-beta2-release/"
    },
    {
      "title": "Labs update and April highlights",
      "text": "It has been an exciting first month for me at Quansight Labs. It's a good time\nfor a summary of what we worked on in April and what is coming next.\nProgress on array computing libraries\nOur first bucket of activities I'd call \"innovation\". The most prominent\nprojects in this bucket are XND,\nuarray,\nmetadsl,\npython-moa,\nRemote Backend Compiler and\narrayviews.\nXND is an umbrella name for a set of related array\ncomputing libraries: xnd, ndtypes, gumath, and xndtools.\nHameer Abbasi made some major steps forward with uarray: the backend and\ncoercion semantics are now largely worked out, there is\ngood documentation, and the\nunumpy package (which currently has numpy, XND and PyTorch backends)\nis progressing well. This blog post\ngives a good overview of the motivation for uarray and its main concepts.\nSaul Shanabrook and Chris Ostrouchov worked out how best to put metadsl\nand python-moa together: metadsl can be used to create the API for\npython-moa to simplify the code base of the latter a lot. Chris \nalso wrote an interesting blog post\nexplaining the MoA principles.\n\n\nThe work on XND over the last month consisted mostly of \"under the hood\"\nimprovements and fixes in xnd and ndtypes by Stefan Krah. We did create\na new xnd-benchmarks repository\nand had some interesting discussions on performance. One thing I learned is that\nXND has automatic multithreading and has very similar performance to NumPy + MKL\nfor basic arithmetic operations (at least for array sizes above ~1e4 elements, the\noverhead for small arrays is larger). The xnd.array interface, which is a higher\nlevel interface than xnd.xnd and can be used similarly to numpy, is taking\nshape as well. One user-visible new feature worth mentioning is that xnd containers\ncan now be serialized and pickled.\nWork on PyData core projects\nMost people in the team are maintainers of or contributors to one or more core\nprojects in the PyData or SciPy stacks. Helping maintain and evolve those\nprojects is our second bucket of activities.\nAaron Meurer did a lot of work on SymPy, both\nmaintenance on the SymPy internals and managing the SymPy 1.4 release. He\nwrote a nice blog post on the highlights in that release\nhere.\nGonzalo Pena-Castellanos is working full-time on Spyder,\nwith guidance from Carlos Cordoba. Together they have been working very hard to get\nthe first beta of Spyder 4 ready. Some exciting new features are also in the\nworks, however Gonzalo will be blogging about those soon so I won't steal his\nthunder.\nIvan Ogasawara is spending some time each week on maintenance of\nIbis. If you're a Pandas or scikit-learn user\nand need to interact with SQL databases or HDFS/Spark, Ibis is worth looking into.\nI myself have enjoyed having a little more bandwidth for NumPy and SciPy.\nOn the technical front, this allowed me to contribute to the design discussion\nabout an addition\nto NEP 18 (the __array_function__ override mechanism),\ndo the numpydoc 0.9 release, deal\nwith several build issues, and review a number of PRs\n(the one\nallowing to specify BLAS and LAPACK link order\nwas particularly nice). On the organizational front, I fixed the description\nof how donations are handled on numpy.org, finalized the\nTidelift agreement for NumPy (see the\nannouncement\nfor details), helped NumPy and SciPy get accepted for the\nGoogle Season of Docs program,\nand did everything needed to finalize the fiscal sponsorship agreement between\nSciPy and NumFOCUS.\nJupyter and JupyterLab improvements\nJupyter is a key part of the PyData ecosystem. It extends well beyond that though, so I'm\ngiving it its own bucket here. At Quansight we have a number of Jupyter core developers\nand contributors. Ian Rose, Saul Shanabrook, Grant Nestor and others have been very busy\nwith both maintenance tasks and adding new features to Jupyter and JupyterLab.\nJupyterLab is about to get support for printing (not inside the notebook, but the old-fashioned\nCtrl-P variant). This pull request\nby Saul has nice screenshots showing the feature in action for whole notebooks,\n images, the JSON viewer and the inspector.\nIan worked on the third alpha release of JupyterLab 1.0, on testing and CI infrastructure,\nand other general maintenance tasks. He also improved PDF preview in JupyterLab, so it\nnow works as expected in Firefox\nand Chrome (at least).\nSaul added support for the nteract Data Explorer to the JupyterLab data registry as a plugin.\nThis pull request shows it\nin action on a pandas DataFrame.\nOther interesting features are in progress and will make their way into the main\nrepositories soon.\nStarting to shape Labs\nThere is a lot of work to do to figure out for ourselves exactly what Labs\nwill be, and then to communicate that clearly to the outside world. We have\na rough idea (see my first blog post\nand Travis' blog post), but there's a long way\nto go from there to having an compelling elevator pitch, a website that tells\nour story well, people and projects organized, a funding stream, and more.\nOne of the first things we did do is start this blog, to start communicating\nabout the technical work we're doing. We're also going through the roadmaps\npublished at quansight.com/projects,\nto ensure they're up-to-date and to make clear that those are for community\ndriven projects that Quansight is aiming to obtain industry support for.\nFunding\nWe reached about 20% of our funding goal for 2019 so far, primarily with contributions\nfrom DE Shaw, OmniSci and\nTDK.\nBoth DE Shaw and OmniSci are supporting a significant amount of work on\nJupyterLab, which highlights how important Jupyter and JupyterLab have become\nin the data science ecosystem. DE Shaw is also supporting work on projects\nlike Dask, Numba and XND that is starting at the moment. OmniSci supports work\non Ibis and Remote Backend Compiler. Finally, Quansight is working with Cal Poly\n(one of the Jupyter lead institutes,\ntogether with UC Berkeley) to execute on the Project Jupyter roadmap for JupyterLab.\nTDK is sponsoring the Spyder work I talked about above. Supporting both general\nmaintenance for the Spyder 4 release and some interesting new features is an\nimportant contribution that helps the many engineers and scientists that use\nSpyder as their main development and data science interface.\nThe above is direct funding from companies for work on open source projects.\nQuansight also offers open-source support and consulting, as well as training\naround the PyData stack. Those activities also yield funds that we then use to\nfund the efforts of Quansight Labs. To learn more about those offerings,\ncontact Travis (travis@quansight.com), myself (rgommers@quansight.com) or\nsales@quansight.com.\nBesides funding from companies, we are also applying for grants. So far we have\nsubmitted two proposals to the NSF and three to NASA, on topics ranging from\nJupyterLab extensions for high performance computing to improving Xarray's array\nbackend system. For most of these proposals we expect the verdict in the next\n1-2 months. In April we got a rejection from the NSF for a\nproposal\ntitled \"Accelerated Development of the Scientific Python Ecosystem\", which we\nwrote together with NumFOCUS and Columbia, with the latter as lead\ninstitute (thanks goes especially to Andreas Mueller and Andy Terrel for a lot\nof the hard work on that proposal). The discussions triggered by that\nrejection have been very useful and generated a number of new ideas and\ncontacts to follow up on in the coming months.\nOne idea that came up more than once is to clearly express the needs of these\nprojects in public, ideally in fundable chunks and with an effort estimate attached,\nand then approaching both funding bodies and companies with that. This is likely\nto be more effective than responding to solicitations that may not be a perfect\nmatch. Quansight Labs is positioned well to either participate in or help lead such\na process, and to work with companies that rely on the PyData stack in particular.\nHowever we look for funding, it will be important to be clear in our messaging\nand transparent with the community about the ways we look for funding. I will be\nactively soliciting feedback on this as well, both via blog posts like these\n(please email me at rgommers@quansight.com if you have ideas, questions or\nconcerns!) and in person.\nFinally, we are finalizing and signing a preferred partnership with NumFOCUS,\nwhere 5% of Quansight Labs funds or projects referred from NumFOCUS will be\nprovided to NumFOCUS to sustain their efforts. NumFOCUS is an important fundament\nof the PyData ecosystem, and we would like to contribute to keeping it on a sound\nfinancial footing and growing NumFOCUS further.",
      "tags": "Labs",
      "url": "https://labs.quansight.org/blog/2019/05/labs-update-april-highlights/"
    },
    {
      "title": "What's New in SymPy 1.4",
      "text": "As of November, 2018, I have been working at\nQuansight, under the heading of Quansight\nLabs. Quansight Labs is a public-benefit\ndivision of Quansight. It provides a home for a \"PyData Core Team\" which\nconsists of developers, community managers, designers, and documentation\nwriters who build open-source technology and grow open-source communities\naround all aspects of the AI and Data Science workflow. As a part of this, I\nam able to spend a fraction of my time working on SymPy.\nSymPy, for those who do not know, is a\nsymbolic mathematics library written in pure Python. I am the lead maintainer\nof SymPy.\nSymPy 1.4 was released on April 9, 2019. In this post, I'd like to go over\nsome of the highlights for this release. The full release notes for the\nrelease can be found on the SymPy\nwiki.\nTo update to SymPy 1.4, use\nconda install sympy\n\n\n\nor if you prefer to use pip\npip install -U sympy\n\n\n\nThe SymPy 1.4 release contains over 500 changes from 38 different\nsubmodules,\nso I will not be going over every change, but only a few of the main\nhighlights. A total of 104\npeople\ncontributed to this release, of whom 66 contributed for the first time for\nthis release.\nWhile I did not personally work on any of the changes listed below (my work\nfor this release tended to be more invisible, behind the scenes fixes), I did\ndo the release itself.\n\n\nAutomatic LaTeX rendering in the Jupyter notebook\nPrior to SymPy 1.4, SymPy expressions in the notebook rendered by default with their\n string representation. To get LaTeX output, you had to call init_printing():\n\nIn SymPy 1.4, SymPy expressions now automatically render as LaTeX in the notebook:\n\nHowever, this only applies automatically if the type of an object is a SymPy\nexpression. For built-in types such as lists or ints, init_printing() is\nstill required to get LaTeX printing. For example, solve() returns a list,\nso does not render as LaTeX unless init_printing() is called:\n\ninit_printing() is also still needed if you want to change any of the\nprinting settings, for instance, passing flags to the latex() printer or\nselecting a different printer.\nIf you want the string form of an expression for copy-pasting, you can use\nprint.\nImproved simplification of relational expressions\nSimplification of relational and piecewise expressions has been improved:\n>>> x, y, z, w = symbols('x y z w')\n>>> init_printing()\n>>> expr = And(Eq(x,y), x >= y, w < y, y >= z, z < y)\n>>> expr\nx = y \u2227 x \u2265 y \u2227 y \u2265 z \u2227 w < y \u2227 z < y\n>>> simplify(expr)\nx = y \u2227 y > Max(w, z)\n\n\n\n>>> expr = Piecewise((x*y, And(x >= y, Eq(y, 0))), (x - 1, Eq(x, 1)), (0, True))\n>>> expr\n\u23a7 x\u22c5y   for y = 0 \u2227 x \u2265 y\n\u23aa\n\u23a8x - 1      for x = 1\n\u23aa\n\u23a9  0        otherwise\n>>> simplify(expr)\n0\n\n\n\nImproved MathML printing\nThe MathML presentation printer has been greatly improved, putting it on par\nwith the existing Unicode and LaTeX pretty printers.\n>>> mathml(Integral(exp(-x**2), (x, -oo, oo)), 'presentation')\n<mrow><msubsup><mo>&#x222B;</mo><mrow><mo>-</mo><mi>&#x221E;</mi></mrow><mi>&#x221E;</mi></msubsup><msup><mi>&ExponentialE;</mi><mrow><mo>-</mo><msup><mi>x</mi><mn>2</mn></msup></mrow></msup><mo>&dd;</mo><mi>x</mi></mrow>\n\n\n\nIf your browser supports MathML (at the\ntime of writing, only Firefox and Safari), you should see the above\npresentation form for Integral(exp(-x**2), (x, -oo, oo)) below:\n\u222b-\u221e\u221e&ExponentialE;-x2&dd;x\n\nImprovements to solvers\nSeveral improvements have been made to the solvers.\n>>> eq = Eq((x**2 - 7*x + 11)**(x**2 - 13*x + 42), 1)\n>>> eq\n                2\n               x  - 13\u22c5x + 42\n\u239b 2           \u239e\n\u239dx  - 7\u22c5x + 11\u23a0               = 1\n>>> solve(eq, x) # In SymPy 1.3, this only gave the partial solution [2, 5, 6, 7]\n[2, 3, 4, 5, 6, 7]\n\n\n\nThe ODE solver, dsolve, has also seen some improvements. Two new hints have\nbeen added.\n'nth_algebraic' solves ODEs using solve by inverting the derivatives\nalgebraically:\n>>> f = Function('f')\n>>> eq = Eq(f(x) * (f(x).diff(x)**2 - 1), 0)\n>>> eq\n\u239b          2    \u239e\n\u239c\u239bd       \u239e     \u239f\n\u239c\u239c\u2500\u2500(f(x))\u239f  - 1\u239f\u22c5f(x) = 0\n\u239d\u239ddx      \u23a0     \u23a0\n>>> dsolve(eq, f(x)) # In SymPy 1.3, this only gave the solution f(x) = C1 - x\n[f(x) = 0, f(x) = C\u2081 - x, f(x) = C\u2081 + x]\n\n\n\n'nth_order_reducible' solves ODEs that only involve derivatives of f(x),\nvia the substitution $g(x)=f{(n)}(x)$.\n>>> eq = Eq(Derivative(f(x), (x, 2)) + x*Derivative(f(x), x), x)\n>>> eq\n               2\n  d           d\nx\u22c5\u2500\u2500(f(x)) + \u2500\u2500\u2500(f(x)) = x\n  dx           2\n             dx\n>>> dsolve(eq, f(x))\n                  \u239b\u221a2\u22c5x\u239e\nf(x) = C\u2081 + C\u2082\u22c5erf\u239c\u2500\u2500\u2500\u2500\u239f + x\n                  \u239d 2  \u23a0\n\n\n\nDropping Python 3.4 support\nThis is the last release of SymPy to support Python 3.4. SymPy 1.4 supports\nPython 2.7, 3.4, 3.5, 3.6, 3.7, and PyPy. What's perhaps more exciting is that\nthe next release of SymPy, 1.5, which will be released later this year, will\nbe the last version to support Python 2.7.\nOur\npolicy is\nto drop support for major Python versions when they reach their End of\nLife. In other words,\nthey receive no further support from the core Python team. Python 3.4 reached\nits end of life on May 19 of this year, and Python 2.7 will reach its end of\nlife on January 1, 2020.\nI have blogged in the\npast on why I\nbelieve it is important for library authors to be proactive in dropping Python\n2 support, and since then a large number of Python\nlibraries have either dropped support or\nannounced their plans to by 2020.\nHaving Python 2 support removed will not only allow us to remove a large\namount of compatibility\ncruft\nfrom our codebase, it will also allow us to use some Python 3-only features\nthat will clean up our API, such as keyword-only\narguments,\ntype\nhints,\nand Unicode variable\nnames.\nIt will also enable several internal\nchanges\nthat will not be visible to end-users, but which will result in a much cleaner\nand more maintainable codebase.\nIf you are still using Python 2, I strongly recommend switching to Python 3,\nas otherwise the entire ecosystem of Python libraries is soon going to stop\nimproving for you. Python 3 is already highly recommended for SymPy usage due\nto several key improvements. In particular, in Python 3, division of two\nPython ints like 1/2 produces the float 0.5. In Python 2, it does\ninteger division (producing 1/2 == 0). The Python 2 integer division\nbehavior can lead to very surprising results when using SymPy (imagine writing\nx**2 + 1/2*x + 2 and having the x term \"disappear\"). When using SymPy, we\nrecommend\nusing rational numbers (like Rational(1, 2)) and avoiding int/int, but the\nPython 3 behavior will at least maintain a mathematically correct result if\nyou do not do this. SymPy is also already faster in Python\n3\ndue to things like math.gcd and functools.lru_cache being written in C,\nand general performance improvements in the interpreter itself.\nAnd much more\nThese are only a few of the highlights of the hundreds of changes in this\nrelease. The full release notes can be found on our\nwiki. The wiki\nalso has the in progress changes for our next release, SymPy\n1.5, which will be\nreleased later this year. Our bot\nautomatically collects release notes from every pull request, meaning SymPy\nreleases have very comprehensive and readable release notes pages. If you see\nany mistakes on either page, feel free to edit the wiki and fix them.",
      "tags": "Labs,sympy",
      "url": "https://labs.quansight.org/blog/2019/04/whats-new-in-sympy-14/"
    },
    {
      "title": "uarray: A Generic Override Framework for Methods",
      "text": "uarray: A Generic Override Framework for Methods\u00b6uarray is an override framework for methods in Python. In the scientific Python ecosystem, and in other similar places, there has been one recurring problem: That similar tools to do a job have existed, but don't conform to a single, well-defined API. uarray tries to solve this problem in general, but also for the scientific Python ecosystem in particular, by defining APIs independent of their implementations.\nArray Libraries in the Scientific Python Ecosystem\u00b6When SciPy was created, and Numeric and Numarray unified into NumPy, it jump-started Python's data science community. The ecosystem grew quickly: Academics started moving to SciPy, and the Scikits that popped up made the transition all the more smooth.\nHowever, the scientific Python community also shifted during that time: GPUs and distributed computing emerged. Also, there were old ideas that couldn't really be used with NumPy's API, such as sparse arrays. To solve these problems, various libraries emerged:\n\nDask, for distributed NumPy\nCuPy, for NumPy on Nvidia-branded GPUs.\nPyData/Sparse, a project started to make sparse arrays conform to the NumPy API\nXnd, which extends the type system and the universal function concept found in NumPy\n\n\n\nThere were yet other libraries that emerged: PyTorch, which mimics NumPy to a certain degree; TensorFlow, which defines its own API; and MXNet, which is another deep learning framework that mimics NumPy.\nThe Problem\u00b6The problem is, stated simply: How do we use all of these libraries in tandem, moving seamlessly from one to the other, without actually changing the API, or even the imports? How do we take functions written for one library and allow it to be used by another, without, as Travis Oliphant so eloquently puts it, \"re-writing the world\"?\nIn my mind, the goals are (stated abstractly):\n\nMethods that are not tied to a specific implementation.\nFor example np.arange\n\n\nBackends that implement these methods.\nNumPy, Dask, PyTorch are all examples of this.\n\n\nCoercion of objects to other forms to move between backends.\nThis means converting a NumPy array to a Dask array, and vice versa.\n\n\n\nIn addition, we wanted to be able to do this for arbitrary objects. So dtypes, ufuncs etc. should also be dispatchable and coercible.\nThe Solution?\u00b6With that said, let's dive into uarray. If you're not interested in the gory details, you can jump down to this section.\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \nimport uarray as ua\n\n# Let's ignore this for now\ndef myfunc_rd(a, kw, d):\n    return a, kw\n\n# We define a multimethod\n@ua.create_multimethod(myfunc_rd)\ndef myfunc():\n    return () # Let's also ignore this for now\n\n\n# Now let's define two backends!\nbe1 = ua.Backend()\nbe2 = ua.Backend()\n\n# And register their implementations for the method!\n@ua.register_implementation(myfunc, backend=be1)\ndef myfunc_be1(): # Note that it has exactly the same signature\n    return \"Potato\"\n\n@ua.register_implementation(myfunc, backend=be2)\ndef myfunc_be2(): # Note that it has exactly the same signature\n    return \"Strawberry\"\n\n\n    \n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \nwith ua.set_backend(be1):\n    print(myfunc())\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nPotato\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \nwith ua.set_backend(be2):\n    print(myfunc())\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nStrawberry\n\n\n\n\n\n\n\n\n\n\n\nAs we can clearly see: We have already provided a way to do (1) and (2) above. But then we run across the problem: How do we decide between these backends? How do we move between them? Let's go ahead and register both of these backends for permanent use. And see what happens when we want to implement both of their methods!\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \nua.register_backend(be1)\nua.register_backend(be2)\n\n\n    \n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \nprint(myfunc())\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nPotato\n\n\n\n\n\n\n\n\n\n\n\nAs we see, we get only the first backend's answer. In general, it's indeterminate what backend will be selected. But, this is a special case: We're not passing arguments in! What if we change one of these to return NotImplemented?\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \n# We redefine the multimethod so it's new again\n@ua.create_multimethod(myfunc_rd)\ndef myfunc():\n    return ()\n\n\n# Now let's redefine the two backends!\nbe1 = ua.Backend()\nbe2 = ua.Backend()\n\n# And register their implementations for the method!\n@ua.register_implementation(myfunc, backend=be1)\ndef myfunc_be1(): # Note that it has exactly the same signature\n    return NotImplemented\n\n@ua.register_implementation(myfunc, backend=be2)\ndef myfunc_be2(): # Note that it has exactly the same signature\n    return \"Strawberry\"\n\nua.register_backend(be1)\nua.register_backend(be2)\n\n\n    \n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \nwith ua.set_backend(be1):\n    print(myfunc())\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\nStrawberry\n\n\n\n\n\n\n\n\n\n\n\nWait... What? Didn't we just set the first Backend? Ahh, but, you see... It's signalling that it has no implementation for myfunc. The same would happen if you simply didn't register one. To force a Backend, we must use only=True or coerce=True, the difference will be explained in just a moment.\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \nwith ua.set_backend(be1, only=True):\n    print(myfunc())\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n---------------------------------------------------------------------------\nBackendNotImplementedError                Traceback (most recent call last)\n<ipython-input-8-ec856cf7c88b> in <module>\n      1 with ua.set_backend(be1, only=True):\n----> 2     print(myfunc())\n\n~/Quansight/uarray/uarray/backend.py in __call__(self, *args, **kwargs)\n    108 \n    109         if result is NotImplemented:\n--> 110             raise BackendNotImplementedError('No selected backends had an implementation for this method.')\n    111 \n    112         return result\n\nBackendNotImplementedError: No selected backends had an implementation for this method.\n\n\n\n\n\n\n\n\n\n\nNow we are told that no backends had an implementation for this function (which is nice, good error messages are nice!)\n\n\n\n\n\n\n\nCoercion and passing between backends\u00b6Let's say we had two Backends. Let's choose the completely useless example of one storing a number as an int and one as a float.\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \nclass Number(ua.DispatchableInstance):\n    pass\n\ndef myfunc_rd(args, kwargs, dispatchable_args):\n    # Here, we're \"replacing\" the dispatchable args with the ones supplied.\n    # In general, this may be more complex, like inserting them in between\n    # other args and kwargs.\n    return dispatchable_args, kwargs\n\n@ua.create_multimethod(myfunc_rd)\ndef myfunc(a):\n    # Here, we're marking a as a Number, and saying that \"we want to dispatch/convert over this\"\n    # We return as a tuple as there may be more dispatchable arguments\n    return (Number(a),)\n\n\nNumber.register_convertor(be1, lambda x: int(x))\nNumber.register_convertor(be2, lambda x: str(x))\n\n\n    \n\n\n\n\n\n\n\nLet's also define a \"catch-all\" method. This catches all implementations of methods not already registered.\n\n\n\n\n\n\nIn\u00a0[10]:\n\n    \n# This can be arbitrarily complex\ndef gen_impl1(method, args, kwargs, dispatchable_args):\n    if not all(isinstance(a, Number) and isinstance(a.value, int) for a in dispatchable_args):\n        return NotImplemented\n    \n    return args[0]\n\n# This can be arbitrarily complex\ndef gen_impl2(method, args, kwargs, dispatchable_args):\n    if not all(isinstance(a, Number) and isinstance(a.value, str) for a in dispatchable_args):\n        return NotImplemented\n    \n    return args[0]\n\nbe1.register_implementation(None, gen_impl1)\nbe2.register_implementation(None, gen_impl2)\n\n\n    \n\n\n\n\n\n\nIn\u00a0[11]:\n\n    \nmyfunc('1') # This calls the second implementation\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[11]:\n\n\n\n\n\n'1'\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[12]:\n\n    \nmyfunc(1) # This calls the first implementation\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[12]:\n\n\n\n\n\n1\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[13]:\n\n    \nmyfunc(1.0) # This fails\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n---------------------------------------------------------------------------\nBackendNotImplementedError                Traceback (most recent call last)\n<ipython-input-13-8431c1275db5> in <module>\n----> 1 myfunc(1.0) # This fails\n\n~/Quansight/uarray/uarray/backend.py in __call__(self, *args, **kwargs)\n    108 \n    109         if result is NotImplemented:\n--> 110             raise BackendNotImplementedError('No selected backends had an implementation for this method.')\n    111 \n    112         return result\n\nBackendNotImplementedError: No selected backends had an implementation for this method.\n\n\n\n\n\n\n\n\n\nIn\u00a0[14]:\n\n    \n# But works if we do this:\n\nwith ua.set_backend(be1, coerce=True):\n    print(type(myfunc(1.0)))\n\nwith ua.set_backend(be2, coerce=True):\n    print(type(myfunc(1.0)))\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n<class 'int'>\n<class 'str'>\n\n\n\n\n\n\n\n\n\n\n\nThis may seem like too much work, but remember that it's broken down into a lot of small steps:\n\nExtract the dispatchable arguments.\nRealise the types of the dispatchable arguments.\nConvert them.\nPlace them back into args/kwargs\nCall the right function.\n\nNote that only=True does not coerce, just enforces the backend strictly.\n\n\n\n\n\n\n\nWith this, we have solved problem (3). Now remains the grunt-work of actually retrofitting the NumPy API into unumpy and extracting the right values from it.\n\n\n\n\n\n\n\nHow To Use It Today\u00b6unumpy is a set of NumPy-related multimethods built on top of uarray. You can use them as follows:\n\n\n\n\n\n\nIn\u00a0[15]:\n\n    \nimport unumpy as np # Note the changed import statement\nfrom unumpy.xnd_backend import XndBackend\n\nwith ua.set_backend(XndBackend):\n    print(type(np.arange(0, 100, 1)))\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n<class 'xnd.array'>\n\n\n\n\n\n\n\n\n\n\n\nAnd, as you can see, we get back an Xnd array when using a NumPy-like API. Currently, there are three back-ends: NumPy, Xnd and PyTorch. The NumPy and Xnd backends have feature parity, while the PyTorch backend is still being worked on.\nWe are also working on supporting more of the NumPy API, and dispatching over dtypes.\nFeel free to browse the source and open issues at: https://github.com/Quansight-Labs/uarray or shoot me an email at habbasi@quansight.com if you want to contact me directly. You can also find the full documentation at https://uarray.readthedocs.io/en/latest/.",
      "tags": "",
      "url": "https://labs.quansight.org/blog/2019/04/uarray-intro/"
    },
    {
      "title": "MOA: a theory for composable and verifiable tensor computations",
      "text": "Python-moa (mathematics of arrays) is an approach to a high level tensor\ncompiler that is based on the work of Lenore\nMullin and her\ndissertation.\nA high level compiler is necessary because there are many optimizations\nthat a low level compiler such as gcc will miss. It is trying to solve\nmany of the same problems as other technologies such as the taco\ncompiler and the xla\ncompiler. However, it takes a much\ndifferent approach than others guided by the following principles.\n\nWhat is the shape? Everything has a shape. scalars, vectors, arrays,\noperations, and functions.\nWhat are the given indicies and operations required to produce a\ngiven index in the result?\n\nHaving a compiler that is guided upon these principles allows for high\nlevel reductions that other compilers will miss and allows for\noptimization of algorithms as a whole. Keep in mind that MOA is NOT\na compiler. It is a theory that guides compiler development. Since\npython-moa is based on\ntheory we get unique properties that other compilers cannot guarantee:\n\n\n\nNo out of bounds array accesses\nA computation is determined to be either valid or invalid at\ncompile time\nThe computation will always reduce to a deterministic minimal form\n(dnf) (see\nchurch-rosser\nproperty)\nAll MOA operations are composable (including black box functions\nand\ngufuncs)\nArbitrary high level operations will compile down to a minimal\nbackend instruction set. If the shape and indexing of a given\noperation is known it can be added to python-moa.\n\nFrontend\u00b6\n\n\n\n\n\n\nLenore Mullin originally developed a moa\ncompiler in the 90s\nwith programs that used a symbolic syntax heavily inspired by\nAPL)\n(example\nprogram).\nThis work was carried into python-moa initially with a lex/yacc compiler\nwith an example program below.\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \nfrom moa.frontend import parse\n\ncontext = parse('<0> psi (tran (A  <n m> + B  <k l>))')\n\n\n    \n\n\n\n\n\n\n\nUpon pursuing this approach it became apparent that MOA should not\nrequire that a new syntax be developed since it is only a theory! So a\npythonic interface to MOA was developed that expressed the same ideas\nwhich look much like the current numeric python libraries. Ideally MOA\nis hidden from the user. The python-moa compiler is broken into several\npieces each which their own responsibilities: shape, DNF, and ONF.\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \nfrom moa.frontend import LazyArray\n\nA = LazyArray(shape=('n', 'm'), name='A')\nB = LazyArray(shape=('k', 'l'), name='B')\n\nexpression = (A + B).T.reduce('+')[0]\nexpression\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[2]:\n\n\n\n\n\n\n\n\n\n\n%3\n\n\n\n0\n\npsi(\u03a8)\n\n\n\n1\n\n\nArray _a6\n\n<1>\n\n(0)\n\n\n\n0->1\n\n\n\n\n\n2\n\nreduce (+)\n\n\n\n0->2\n\n\n\n\n\n3\n\ntranspose(\u00d8)\n\n\n\n2->3\n\n\n\n\n\n4\n\n+\n\n\n\n3->4\n\n\n\n\n\n5\n\n\nArray A\n\n<n m>\n\n\n\n4->5\n\n\n\n\n\n6\n\n\nArray B\n\n<k l>\n\n\n\n4->6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShape Calculation\u00b6The shape calculation is responsible for calculating the shape at every\nstep of the computation. This means that operations have a shape. Note\nthat the compiler handles symbolic shapes thus the exact shape does not\nneed to be known, only the dimension. After the shape calculation step\nwe can guarantee that an algorithm is a valid program and there will be\nno out of bound memory accesses. Making MOA extremely compelling for\nFPGAs and\ncompute units with a minimal OS. If an algorithm makes it past this\nstage and fails then it is an issue with the compiler not the algorithm.\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \nexpression.visualize(stage='shape')\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[3]:\n\n\n\n\n\n\n\n\n\n\n%3\n\n\n\n0\n\n\ncondition\n\n<>\n\n((0 <= n) and ((m == l) and (n == k)))\n\n\n\n1\n\n\npsi(\u03a8)\n\n<>\n\n\n\n0->1\n\n\n\n\n\n2\n\n\nArray _a6\n\n<1>\n\n(0)\n\n\n\n1->2\n\n\n\n\n\n3\n\n\nreduce (+)\n\n<n>\n\n\n\n1->3\n\n\n\n\n\n4\n\n\ntranspose(\u00d8)\n\n<m n>\n\n\n\n3->4\n\n\n\n\n\n5\n\n\n+\n\n<n m>\n\n\n\n4->5\n\n\n\n\n\n6\n\n\nArray A\n\n<n m>\n\n\n\n5->6\n\n\n\n\n\n7\n\n\nArray B\n\n<k l>\n\n\n\n5->7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDenotational Normal Form (DNF)\u00b6The DNF\\'s responsibility is to reduce the high level MOA expression to\nthe minimal and optimal machine independent computation. This graph has\nall of the indexing patterns of the computation and resulting shapes.\nNotice that several operations disappear in this stage such a transpose\nbecause transpose is simply index manipulation.\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \nexpression.visualize(stage='dnf')\n\n\n    \n\n\n\n\n\n\n\n\n\n    Out[4]:\n\n\n\n\n\n\n\n\n\n\n%3\n\n\n\n0\n\n\ncondition\n\n<>\n\n((0 <= n) and ((m == l) and (n == k)))\n\n\n\n1\n\n\nreduce (+)\n\n<>\n\n_i10\n\n\n\n0->1\n\n\n\n\n\n2\n\n\n+\n\n<>\n\n\n\n1->2\n\n\n\n\n\n3\n\n\npsi(\u03a8)\n\n<>\n\n\n\n2->3\n\n\n\n\n\n6\n\n\npsi(\u03a8)\n\n<>\n\n\n\n2->6\n\n\n\n\n\n4\n\n\nArray _a12\n\n<2>\n\n(0 _i10)\n\n\n\n3->4\n\n\n\n\n\n5\n\n\nArray A\n\n<n m>\n\n\n\n3->5\n\n\n\n\n\n7\n\n\nArray _a12\n\n<2>\n\n(0 _i10)\n\n\n\n6->7\n\n\n\n\n\n8\n\n\nArray B\n\n<k l>\n\n\n\n6->8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperational Normal Form (ONF)\u00b6The ONF is the stage of the compiler that will have to be the most\nflexible. At its current stage the ONF is a naive compiler that does not\nperform many important optimizations such as PSI\nreduction\nwhich reduces the number of loops in the calculation, loop ordering, and\nminimize the number of accumulators. MOA has ideas of dimension lifting\nwhich make parallization and optimizing for cache sizes much easier.\nAdditionally algorithms must be implemented differently for sparse,\ncolumn major, row major. The ONF stage is responsible for any\n\\\"optimal\\\" machine dependent implementation. \\\"optimal\\\" will vary from\nuser to user and thus will have to allow for multiple programs: optimal\nsingle core, optimal parallel, optimal gpu, optimal low memory, etc.\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \nprint(expression.compile(use_numba=True, include_conditions=False))\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n\n\n@numba.jit\ndef f(A, B):\n    \n    \n    n = A.shape[0]\n    \n    m = A.shape[1]\n    \n    k = B.shape[0]\n    \n    l = B.shape[1]\n    \n    _a21 = numpy.zeros(())\n    \n    _a19 = numpy.zeros(())\n    \n    _a21 = 0\n    \n    for _i10 in range(0, m, 1):\n        \n        _a21 = (_a21 + (A[(0, _i10)] + B[(0, _i10)]))\n    \n    _a19[()] = _a21\n    return _a19\n\n\n\n\n\n\n\n\n\n\n\nPerformance\u00b6MOA excels at performing reductions and reducing the amount of actual\nwork done. You will see that the following algorithm only requires the\nfirst index of the computation. Making the naive implementation 1000x\nmore expensive for 1000x1000 shaped array. The following benchmarks\nhave been performed on my laptop with an intel i5-4200U. However, more\nbenchmarks are always available on the Travis\nCI (these benchmarks\ntest python-moa\\'s weaknesses). You will see with the benchmarks that if\nany indexing is required MOA will be significantly faster unless you\nhand optimize the numerical computations.\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \nimport numpy\nimport numba\n\nn, m = 1000, 1000\n\nexec(expression.compile(backend='python', use_numba=True, include_conditions=False))\n\nA = numpy.random.random((n, m))\nB = numpy.random.random((n, m))\n\n\n    \n\n\n\n\n\n\n\nHere we execute the MOA optimized code with the help of\nnumba which is a JIT LLVM compiler for\npython.\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \n%%timeit\n\nf(A=A, B=B)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n2.14 \u00b5s \u00b1 6.76 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\n\n\n\n\n\n\n\n\n\n\nThe following numpy computation is obviously the worst case expression\nthat you could write but this brings up the point that often times the\nalgorithm is expressed differently than the implementation. This is one\nof the problems that MOA hopes to solve.\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \n%%timeit\n\n(A + B).T.sum(axis=0)[0]\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n2.74 ms \u00b1 29.2 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n\n\n\n\n\n\n\n\n\n\nWe notice that even with the optimized version MOA is faster. This is\nmostly due to the transpose operation the numpy performs that we have no\nway around.\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \n%%timeit\n\n(A[0] + B[0]).T.sum(axis=0)\n\n\n    \n\n\n\n\n\n\n\n\n\n    \n\n\n\n6.67 \u00b5s \u00b1 91.2 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\n\n\n\n\n\n\n\n\n\n\nConclusions\u00b6I hope that this walk through has shown the promising results that the\nMOA theory can bring to tensor computations and the python ecosystem as\na whole. Please feel free to try out the project at Quansight\nLabs/python-moa. I hope\nthat this work can allow for the analysis and optimization of algorithms\nin a mathematically rigorous way which allows users to express their\nalgorithms in an implementation independent manner.",
      "tags": "",
      "url": "https://labs.quansight.org/blog/2019/04/python-moa-tensor-compiler/"
    },
    {
      "title": "Thoughts on joining Quansight Labs",
      "text": "In his blog post welcoming me, Travis set out his vision for pushing forward the Python ecosystem for scientific computing and data science, and how to fund it. In this post I'll add my own perspectives to that. Given that Quansight Labs' purpose, it seems fitting to start with how I see things as a community member and organizer.\nA community perspective\nThe SciPy and PyData ecosystems have experienced massive growth over the past years, and this is likely to continue in the near future. As a maintainer, that feels very gratifying. At the same time it brings up worries. Core projects struggle to keep up with the growth in number of users. Funded development can help with this, if done right. Some of the things I would like to see from companies that participate in the ecosystem:\n\nExplain innovations they're working on to the community and solicit input, at an early stage. Developing something away from the spotlight and then unveiling it as the \"next big thing\" once it's done usually leads to either corporate-driven projects (if users adopt it) or a short life span.\nParticipate in a sustainable way. This means for example to contribute in a way that lowers, or at least doesn't increase, the overall effort required for maintenance. Only sending pull requests with new features doesn't achieve that. Solving maintenance pain points or helping with code review does.\nOperate transparently. Develop in the open, plan in the open, be clear about directions and motivations.\n\n\n\nWhen I started working on NumPy and SciPy the world was simple: projects with Subversion repositories, mailing lists, and volunteers, and a couple of yearly conferences. Today the landscape looks a lot more complex. This rough sketch attempts to capture a part (all doesn't fit on a 2-D canvas) of how I think it looks from a community or project perspective:\n\nActing effectively within that landscape as a project has become important maintenance work in itself, and working on governance structures or legal agreements is less fun (and thus harder to find volunteers for) than writing code. Quansight Labs will add to this complexity. It also provides an opportunity to help manage that same complexity though, as a home for core developers of many of the projects in the ecosystem. \nA Quansight Labs perspective\nTo date, the work at Quansight Labs has mainly focused on innovation and highly experimental prototypes.\nTravis already provided some context on XND (a clean, modular implementation of generalized NumPy concepts) and uarray (a Python array interface). XND is already fairly mature while uarray is still very young; however both are at the stage where we can think about adoption pathways. Experimental prototypes with a lot of potential include python-moa (a Python implementation of Mathematics of Arrays), metadsl (a framework for creating domain specific language libraries in Python), and mtypes (\"memory types for Python\", bridging the Python type system to and from C).\nThe balance of work at Quansight Labs will gradually shift towards sustaining core projects in the PyData ecosystem.\nWhich projects we work on will be driven by both availability of funding and personal interests of people. As well as by input from the community - we want to push forward core projects, so we need to act on priorities that core development teams have. Clearly articulating those priorities isn't always simple. Half a year ago at the NumFOCUS Summit I took stock of the roadmaps for all NumFOCUS projects: 50% of the projects didn't have a roadmap, and of the 50% that did again half said that their roadmaps were not up to date. This is also the kind of thing Quansight Labs can help with. For example, Anthony Scopatz has done a great job in producing roadmap brochures for each of the projects that have been covered in the Open Source Directions webinars: https://www.quansight.com/projects. In that process he helped improve those project roadmaps tremendously.\nQuansight itself is only a year and a bit old; Quansight Labs is still being shaped as a separate entity. We do know though that our landscape picture will look complex as well. We have and will build out close relationships with many projects and ecosystems (PyData, Jupyter, SciPy), with like-minded organizations (NumFOCUS, Ursa Labs, BIDS), with companies interested in open source, and with funders.\nA personal perspective\nWhy did I start my journey in open source scientific Python in the first place? I was a happy user for a couple of years, and answered a request to the community to help out with documenting NumPy. Then I stayed because I could see that those first contributions had impact. Also because I was learning things and because of the friendly community, but impact was and still is an important driver. I see the potential that Quansight Labs has, and the need for it (and other organizations like it) to be successful.\nBesides working with talented, humble people and being able to spend more time on the projects I care about, what particularly interests me is how to bridge the gap between innovation and maintenance. In just one week I've seen an impressive amount of innovation - now how do we go about making that benefit the whole user community?\nThis was the view from my window on my first day: a ship with the letters \"APL\" on it:\n\nGiven NumPy's heritage and the work on Mathematics of Arrays at Quansight Labs (both connected to APL), it felt symbolic.\nNext steps\nIt's early days, however I do have two things in mind that will be priorities over the next period:\n\nTogether with the people working on XND and uarray, figure out how to engage the community in a constructive way and get a clear user adoption story in place (where the users may well include projects like NumPy and Pandas).\n\nInitiate more contributions from Quansight Labs to core projects. Here are some initial ideas I'm thinking about:\n\n\nIn NumPy, help with finalizing and road testing NEP 18 (__array_function__)\n\nIn SciPy, fix the use of deprecated NumPy APIs\nFinish the backwards compatibility NumPy Enhancement Proposal\nIn SciPy, complete one of the spatial.distance reimplementations.\n\nThey are focused on NumPy and SciPy for a number of reasons: I understand the needs of those projects well; improvements in those projects benefit the whole stack; they need more maintenance; and a lot of the people at Quansight work on things that require in-depth knowledge of the NumPy and Python C APIs. We will quickly branch out though. Last week I have already talked to core developers of Dask, JupyterLab and Spyder. I have also recently co-written a proposal that included significant work on Matplotlib, Pandas and scikit-learn. And I plan, together with others at Quansight, to actively engage with the wider community.\nI strongly believe in a community-governed open source ecosystem, so am very interested in open conversations about project and ecosystem priorities.\nFeedback and ideas for what we should (or should not) be focusing on at Quansight Labs are very much welcome!",
      "tags": "community,Labs",
      "url": "https://labs.quansight.org/blog/2019/04/joining-labs/"
    },
    {
      "title": "Search",
      "text": "Search results appear here.",
      "tags": "",
      "url": "https://labs.quansight.org/search/"
    }
  ]
};